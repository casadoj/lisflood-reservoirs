{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f67b57-3009-41dc-b060-60c177fa584d",
   "metadata": {},
   "source": [
    "# Download DAHITI\n",
    "***\n",
    "\n",
    "***Author:** Chus Casado Rodr√≠guez*<br>\n",
    "***Date:** 17-02-2025*<br>\n",
    "\n",
    "**Description**<br>\n",
    "\n",
    "This code downloads data from the [DAHITI](https://dahiti.dgfi.tum.de/en/) dataset using its API version 2. \n",
    "\n",
    "It requires two inputs:\n",
    "1. A TXT file (_api_key.txt_) with the API key associated to your DAHITI user.\n",
    "2. A YAML file with the configuration: URL, type of point of interest (river, reservoir), countries of interest, variables of interest, and output path.\n",
    "\n",
    "It searches for available time series of water level, surface area and volume variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37123b88-fcbc-4442-88ff-92bfdad92110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "import geopandas as gpd\n",
    "from shapely import Point\n",
    "import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284640b8-0c76-430f-b877-9272802e649a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6f66a2-cebd-445f-9fde-c91ddd4ae44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "with open('config.yml', 'r') as file:\n",
    "    cfg = yaml.safe_load(file)\n",
    "API_URL = cfg.get('api_url', 'https://dahiti.dgfi.tum.de/api/v2/')\n",
    "TYPE = cfg.get('type', None)\n",
    "COUNTRY = cfg.get('country', None)\n",
    "VARS = cfg.get('variables', 'water-level')\n",
    "PATH_DAHITI = Path(cfg.get('output_path', './'))\n",
    "\n",
    "# personal API key\n",
    "with open('api_key.txt', 'r') as txt:\n",
    "    api_key = txt.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47eabc3d-d547-42eb-8daa-cf76c44d44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_VAR_NAMES = {\n",
    "    'target_name': 'name',\n",
    "    'water_level_altimetry': 'level',\n",
    "    'surface_area': 'area',\n",
    "    'water_occurrence_mask': 'occurrence',\n",
    "    'land_water_mask': 'land_water',\n",
    "    'volume_variation': 'volume',\n",
    "    'water_level_hypsometry': 'level_hyps'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe02a0-0a84-4270-bfd3-c9d5b8302c35",
   "metadata": {},
   "source": [
    "## Targets\n",
    "\n",
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e679d3c7-61d3-4608-a6ec-52e5707ddbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load targets\n",
    "input_file = PATH_DAHITI / TYPE / 'targets' / f'DAHITI_{TYPE}.shp'\n",
    "targets = gpd.read_file(input_file).set_index('dahiti_id')\n",
    "print(f'Loaded input file: {input_file}')\n",
    "print('DAHITI contains {0} targets of type {1}'.format(len(targets), TYPE))\n",
    "\n",
    "# filter targets by country\n",
    "if COUNTRY is not None:\n",
    "    mask_country = targets.country.isin(COUNTRY)\n",
    "    targets_sel = targets[mask_country].copy()\n",
    "    print('DAHITI contains {0} targets of type {1} in {2}.'.format(len(targets_sel), TYPE, ', '.join(COUNTRY)))\n",
    "\n",
    "# # load shapefile of selected targets\n",
    "# targets_sel = gpd.read_file(PATH_DAHITI / TYPE / 'targets' / 'DAHITI_reservoir_krishna.shp').set_index('dahiti_id', drop=True)\n",
    "# targets_sel.index = targets_sel.index.astype(int)\n",
    "# if 'GRAND_ID' in targets_sel.columns:\n",
    "#     targets_sel.GRAND_ID = targets_sel.GRAND_ID.astype('Int64')\n",
    "\n",
    "# rename columns\n",
    "map_columns = {new: old.replace('_', '-') for old, new in MAP_VAR_NAMES.items() if 'name' not in old}\n",
    "targets_sel.rename(columns=map_columns, inplace=True)\n",
    "cols = list(map_columns.values())\n",
    "targets_sel[cols] = targets_sel[cols].astype(int)\n",
    "targets_sel.rename(columns={'water-level-altimetry': 'water-level'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7838d-8122-414c-8b78-043391769bfb",
   "metadata": {},
   "source": [
    "## Time series\n",
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9447f7c4-08e6-4a85-9c3c-0e3d099b544e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ad6993d2af4c839c6213c9e6e3628d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timeseries = {}\n",
    "for ID in tqdm(targets_sel.index):\n",
    "    ts_id = pd.DataFrame(dtype=float)\n",
    "    \n",
    "    for var in VARS:\n",
    "        if targets_sel.loc[ID, var] == 0:\n",
    "            continue\n",
    "\n",
    "        # request info to the API\n",
    "        response = requests.post(\n",
    "            f'{API_URL}download-{var}/',\n",
    "            json={\n",
    "                'api_key': api_key,\n",
    "                'format': 'json',\n",
    "                'dahiti_id': ID,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # load as dictionary\n",
    "            data = json.loads(response.text)\n",
    "\n",
    "            # extract timeseries\n",
    "            try:\n",
    "                df = pd.DataFrame.from_dict(data['data'])\n",
    "                index_col = [col for col in df.columns if col.startswith('date')][0]\n",
    "                df.set_index(index_col, drop=True, inplace=True)\n",
    "                df.index = pd.to_datetime(df.index).date\n",
    "                df.rename(columns={'error': f'{var}_error'}, inplace=True)\n",
    "                df.index.name = 'date'\n",
    "                # concatenate to the timeseries of other variables\n",
    "                ts_id = pd.concat((ts_id, df), axis=1)\n",
    "                del df\n",
    "            except Exception as e:\n",
    "                print(f'Time series from ID {ID} could not be retrieved:\\n{e}', flush=True)\n",
    "            \n",
    "            del data\n",
    "        else:\n",
    "            error = json.loads(response.text)\n",
    "            print('Error while downloading {0} for target {1}:\\n{2}:\\t{3}'.format(var, ID, error['code'], error['message']))\n",
    "            continue\n",
    "            \n",
    "    if len(ts_id) > 0:\n",
    "        ts_id.sort_index(inplace=True)\n",
    "        timeseries[ID] = ts_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93f114-35e8-482a-9dcd-b807dd5b6268",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e412b172-1b57-457f-b9ef-611372726ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\41627.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\14940.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\41625.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\7031.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\41626.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13024.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\2256.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\2257.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13028.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\39439.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13075.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13073.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\22604.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\9929.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\41554.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13067.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\17805.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\40381.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\38450.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13066.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13021.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13052.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13019.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\41433.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\8974.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\17806.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13037.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\39566.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\19858.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\23849.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\41062.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\11255.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\238.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13039.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\13069.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\10479.csv\n",
      "File saved: Z:\\nahaUsers\\casadje\\datasets\\DAHITI\\reservoir\\time_series\\38449.csv\n"
     ]
    }
   ],
   "source": [
    "# export timeseries\n",
    "PATH_OUT = PATH_DAHITI / TYPE / 'time_series'\n",
    "PATH_OUT.mkdir(parents=True, exist_ok=True)\n",
    "for ID, ts in timeseries.items():\n",
    "    output_file = PATH_OUT / f'{ID}.csv'\n",
    "    ts.to_csv(PATH_OUT / f'{ID}.csv')\n",
    "    print(f'File saved: {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
