{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bfb620-0515-4bbe-adde-71b978d69700",
   "metadata": {},
   "source": [
    "# Hydrological Data Colection Center\n",
    "***\n",
    "\n",
    "**_Autor:_** Chus Casado Rodr√≠guez<br>\n",
    "**_Fecha:_** 14-03-2025<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code downloads the stations and time series available in the HYDRO data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf46c52f-2c37-40b1-9ed6-010b9715c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import yaml\n",
    "import unicodedata\n",
    "\n",
    "from lisfloodreservoirs.utils import DatasetConfig, APIConfig\n",
    "\n",
    "def remove_accents(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFKD', text) if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8d181-fc8f-4b5b-87dd-46f1b68d2830",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25409bab-6422-4c84-af54-b306600e8dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station filters\n",
    "TYPE = 'river'\n",
    "COUNTRY_ID = 'ES' # 'HR'\n",
    "PROVIDER_ID = None\n",
    "\n",
    "# # dataset cofiguration\n",
    "# cfg = DatasetConfig(f'./ResOps{COUNTRY_ID}/config_dataset.yml')\n",
    "\n",
    "# HDMS API configuration\n",
    "api = APIConfig('./HDMS_API.yml')\n",
    "\n",
    "# study period\n",
    "START = datetime(1990, 1, 1)\n",
    "END = datetime.now().date()\n",
    "strftime = '%Y-%m-%dT%H:%M:%S'\n",
    "\n",
    "# variables of interest\n",
    "VARIABLES = {\n",
    "    'river': {\n",
    "        # 'W': 'water_level',\n",
    "        'D': 'discharge',\n",
    "    },\n",
    "    'reservoir': {\n",
    "        'I': 'inflow',\n",
    "        'O': 'outflow',\n",
    "        'V': 'volume',\n",
    "        'R': 'level',\n",
    "    }\n",
    "}\n",
    "\n",
    "# directory of the HDMS dataset\n",
    "PATH_HDMS = Path(f'Z:/nahaUsers/casadje/datasets/hDMS/{TYPE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd555531-7479-47be-8da7-69b89bc182c8",
   "metadata": {},
   "source": [
    "## Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03362d18-e120-4235-b12f-91e4d4b354ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = 'stationsmdv2'\n",
    "\n",
    "url = f'{api.URL}/{service}/json/' \n",
    "if PROVIDER_ID:\n",
    "    url = f'{url}provider/{PROVIDER_ID}/'\n",
    "response = requests.get(url, auth=requests.auth.HTTPBasicAuth(api.USERNAME, api.PASSWORD))\n",
    "if response.status_code == 200:\n",
    "    # convert to pandas\n",
    "    stations = pd.DataFrame(response.json())\n",
    "    stations.columns = stations.columns.str.upper()\n",
    "    stations.set_index('EFAS_ID', drop=True, inplace=True)\n",
    "\n",
    "    # keep only stations of the specific type\n",
    "    if TYPE:\n",
    "        mask_type = stations.TYPE == TYPE.upper()\n",
    "    else:\n",
    "        mask_type = True\n",
    "    # keep only stations in the specific country\n",
    "    if COUNTRY_ID:\n",
    "        mask_country = stations['COUNTRY-CODE'] == COUNTRY_ID\n",
    "    else:\n",
    "        mask_country = True\n",
    "    # apply filters\n",
    "    stations = stations[mask_type & mask_country]\n",
    "\n",
    "    # organize fields\n",
    "    stations.dropna(axis=1, how='all', inplace=True)\n",
    "    stations.drop(['LATITUDE_GEODESIC', 'LONGITUDE_GEODESIC', 'GEODESIC_REFERENCE_SYSTEM', 'VARIABLES', 'CATCHMENT_AREA_UNITS', 'HEIGHT_UNITS', 'TYPE'],\n",
    "                    axis=1,\n",
    "                    inplace=True,\n",
    "                    errors='ignore')\n",
    "    stations.rename(columns={\n",
    "         'HAS_RTDATA': 'DATA_RT',\n",
    "         'HAS_HISTORICAL_DATA': 'DATA_HIST',\n",
    "         'NAME': 'RES_NAME',\n",
    "         'NATIONAL_STATION_IDENTIFIER': 'LOCAL_ID',\n",
    "         'PROVIDER_ID': 'PROV_ID',\n",
    "         'COUNTRY-CODE': 'COUNTRY_ID',\n",
    "         'BASIN_ENGLISH': 'BASIN_EN',\n",
    "         'BASIN_LOCAL': 'BASIN_LOC',\n",
    "         'RIVERNAME_LOCAL': 'RIVER_LOC',\n",
    "         'RIVERNAME_ENGLISH': 'RIVER_EN',\n",
    "         'CATCHMENT_AREA': 'CATCH_SKM',\n",
    "         'LATITUDE_WGS84': 'LAT',\n",
    "         'LONGITUDE_WGS84': 'LON',\n",
    "         'COORDINATES_CHECKED': 'COORD_TEST',\n",
    "         'HEIGHT': 'DAM_HGT_M',\n",
    "         'HEIGHT_REFERENCE_SYSTEM': 'HEIGHT_RS',\n",
    "         'LOCAL_REFERENCE_SYSTEM': 'LOCAL_Rs',\n",
    "         'DATE_OF_STARTING_MEASUREMENT': 'START',\n",
    "         'DATE_OF_ENDING_MEASUREMENT': 'END',\n",
    "         'DATE_OF_REGISTRATION': 'REGISTERED',\n",
    "         'LAST_CHANGE_COMMENT': 'COMMENT_',\n",
    "         'X-COORDINATE': 'X',\n",
    "         'Y-COORDINATE': 'Y',\n",
    "         'CALIBRATION_ID': 'CALIB_ID',\n",
    "        'DELIVERY_POLICY': 'DELIVERY',\n",
    "        'INTERNAL_NATIONALSTATIONIDENTIFIER': 'INT_ID',\n",
    "        'LOCAL_PROJECTION_INFO': 'LOC_PROJ',\n",
    "        'LOCATION_ON_RIVER_KM': 'RIVER_KM',\n",
    "        'VERTICAL_DATUM': 'VERT_DATUM'\n",
    "    }, inplace=True)\n",
    "    stations.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    # convert to geopandas\n",
    "    stations = gpd.GeoDataFrame(\n",
    "        stations,\n",
    "        geometry=[Point(xy) for xy in zip(stations.LON, stations.LAT)],\n",
    "        crs='epsg:4326'\n",
    "    )\n",
    "\n",
    "# fix country names\n",
    "stations.COUNTRY_ID = stations.COUNTRY_ID.str.upper()\n",
    "stations.COUNTRY = stations.COUNTRY.str.capitalize()\n",
    "map_countries = {}\n",
    "for ID in stations.COUNTRY_ID.unique():\n",
    "    try:\n",
    "        map_countries[ID] = stations[stations.COUNTRY_ID == ID].COUNTRY.value_counts().index[0]\n",
    "    except:\n",
    "        print(ID, 'has no country name associated')\n",
    "stations.COUNTRY = stations.COUNTRY_ID.map(map_countries)\n",
    "\n",
    "# treat string columns\n",
    "col_names = ['RES_NAME', 'BASIN_LOC', 'BASIN_EN', 'RIVER_LOC', 'RIVER_EN']\n",
    "stations[col_names] = stations[col_names].astype(str).replace('nan', '')\n",
    "for col in col_names:\n",
    "    stations[col] = stations[col].str.lower().apply(remove_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5058c0-17ca-4d95-abf7-2608ea0b69f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m', edgecolor='face', facecolor='lightgray'),\n",
    "               alpha=.5,\n",
    "               zorder=0)\n",
    "stations.plot(markersize=5, ax=ax)\n",
    "ax.set_title(f'{len(stations)} {TYPE} stations')\n",
    "# ax.set_extent([-10, 40, 35.5, 70])\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba366ac2-6b66-46d4-a0ed-eabc1cb4953f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "PATH_GIS = PATH_HDMS / 'GIS'\n",
    "PATH_GIS.mkdir(parents=True, exist_ok=True)\n",
    "if PROVIDER_ID:\n",
    "    shp_file = PATH_GIS / '{0}_HDMS_{1}_{2}.shp'.format(TYPE, PROVIDER_ID, datetime.now().strftime('%Y%m%d'))\n",
    "elif COUNTRY_ID:\n",
    "    shp_file = PATH_GIS / '{0}_HDMS_{1}_{2}.shp'.format(TYPE, COUNTRY_ID, datetime.now().strftime('%Y%m%d'))\n",
    "else:\n",
    "    shp_file = PATH_GIS / '{0}_HDMS_{1}.shp'.format(TYPE, datetime.now().strftime('%Y%m%d'))\n",
    "stations.to_file(shp_file)\n",
    "print(f'Shapefile of stations saved in {shp_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b48624-c12c-4781-85fc-73644afb29c5",
   "metadata": {},
   "source": [
    "**Duplicated by distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12537c8-b509-463e-a078-66cfdb4a6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    provider_col: str = 'PROV_ID',\n",
    "    distance_thr: float = 1500\n",
    "):\n",
    "    \"\"\"Finds duplicates in the input GeoDataFrame based on distance (points closer than the threshold) and provider (if they have different provider)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf: geopandas.GeoDataFrame\n",
    "        table of reservoirs/stations in the database\n",
    "    provider_col: str\n",
    "        column in \"gdf\" that defines the provider. Duplicates must have a different provider\n",
    "    distance_thr: float\n",
    "        distance below which duplicates can exist. Points further apart than this distance will not be spotted as duplicates. The values depend on the reference coordinate system in \"gdf\"\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    A list of lists with groups of duplicates. The values are the index in \"gdf\"\n",
    "    \"\"\"\n",
    "    \n",
    "    duplicates = []\n",
    "    for ID, point in gdf.geometry.items():\n",
    "        if any(ID in sublist for sublist in duplicates):\n",
    "            continue\n",
    "        \n",
    "        prov_id = gdf.loc[ID, provider_col]\n",
    "\n",
    "        # distance to the other reservoirs\n",
    "        others = gdf[gdf.index != ID]\n",
    "        distance = others.geometry.distance(point)\n",
    "\n",
    "        # find close reservoirs\n",
    "        if distance.min() < distance_thr:\n",
    "            ids = distance[distance < distance_thr].index.tolist()\n",
    "            ids = [id for id in ids if gdf.loc[id, provider_col] != prov_id]\n",
    "            if len(ids) > 0:\n",
    "                duplicates.append([ID] + ids)\n",
    "                \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2958ddf-2a23-4a82-aae7-5d4d4e767bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates based on distance and provider\n",
    "duplicates = find_duplicates(stations, provider_col='PROV_ID', distance_thr=.01667)\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f52d2-9b9d-4b27-b779-7cbae5c7ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "with open(f\"duplicated_{TYPE}.txt\", \"w\") as file:\n",
    "    for sublist in duplicates:\n",
    "        file.write(','.join(map(str, sublist)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d6ef5-102e-4807-9eab-8297bf7c067c",
   "metadata": {},
   "source": [
    "**Duplicated reservoir names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482428-9c2a-4831-b9ff-b0961dfbac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping local and English river names\n",
    "# river_loc_en = {}\n",
    "# for river_loc in reservoirs.RIVER_LOC.unique():\n",
    "#     if river_loc == '':\n",
    "#         continue\n",
    "#     mask = reservoirs.RIVER_LOC == river_loc\n",
    "#     river_en = reservoirs[mask].RIVER_EN.unique().tolist()\n",
    "#     if '' in river_en:\n",
    "#         river_en.remove('')\n",
    "#     if len(river_en) == 0:\n",
    "#         print(f'No correspondece for river {river_loc}')\n",
    "#     elif len(river_en) == 1:\n",
    "#         river_loc_en[river_loc] = river_en[0]\n",
    "#     else:\n",
    "#         print(river_loc, river_en)\n",
    "\n",
    "# # fill in the English river names\n",
    "# for river_loc, river_en in river_loc_en.items():\n",
    "#     mask = reservoirs.RIVER_LOC == river_loc\n",
    "#     reservoirs.loc[mask, 'RIVER_EN'] = river_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378e7e8-502d-4b77-a367-6004ffc0285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates\n",
    "res_name_counts = reservoirs.RES_NAME.value_counts()\n",
    "duplicated_res_name = res_name_counts[res_name_counts > 1].index.tolist()\n",
    "print(f'{len(duplicated_res_name)} reservoir names are duplicated')\n",
    "\n",
    "# dictionary that connects duplicated local IDs and EFAS IDs\n",
    "duplicate_names = {}\n",
    "for res_name in duplicated_res_name:\n",
    "    duplicate_names[res_name] = reservoirs.loc[reservoirs.RES_NAME == res_name].index.tolist()\n",
    "\n",
    "# export\n",
    "with open('duplicated_res_name.yml', 'w') as file:\n",
    "    yaml.dump(duplicate_names, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f9a27-67c3-4b38-9291-972012bf9b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # summarize data availability\n",
    "# data_cols = reservoirs.columns[hdcc.columns.str.contains('[O|V|R]_Has_')].tolist()\n",
    "# reservoirs[data_cols].astype(bool).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cb414-145b-496a-8f39-9c0d8a5c7cf0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98985199-1a97-4a95-8dc4-5d2735bc3f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs.COUNTRY_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0ccae-eed1-42cb-a701-99ca2621622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs.COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed931c8c-e957-4d73-91e3-2097d51d2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in reservoirs.COUNTRY_ID.unique():\n",
    "    mask = reservoirs.COUNTRY_ID == country\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165e71e-198b-4000-ac54-60b1bfb90b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs_es = reservoirs[reservoirs.COUNTRY_ID == 'ES']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ddb1a-7b27-4ac7-905e-dc83f20f07a8",
   "metadata": {},
   "source": [
    "**Duplicated local ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bfad0-6f5a-4b45-85c8-59d0bc95baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates\n",
    "local_id_counts = reservoirs_es.LOCAL_ID.value_counts()\n",
    "duplicated_local_id = local_id_counts[local_id_counts > 1].index.tolist()\n",
    "print(f'{len(duplicated_local_id)} local IDs are duplicated')\n",
    "\n",
    "# dictionary that connects duplicated local IDs and EFAS IDs\n",
    "duplicates_id = {}\n",
    "for local_id in duplicated_local_id:\n",
    "    duplicates_id[local_id] = reservoirs_es.loc[reservoirs.LOCAL_ID == local_id].index.tolist()\n",
    "\n",
    "with open('duplicated_local_id.yml', 'w') as file:\n",
    "    yaml.dump(duplicates_id, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7563b-8b55-4400-aaec-82e7784ae06e",
   "metadata": {},
   "source": [
    "**Duplicated by name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2ee93-2fb5-4f3e-b847-39c812fa72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs[mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79996b4f-f8a0-4ac4-9b23-637d13dffc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29afbc-60f7-4eea-8e25-32ade7a2bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f3e1d-2578-41a3-b8d7-0a630688fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs.PROV_ID.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a373f-4873-4c54-a2e6-c791dc63299e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac74e5-f926-42c1-9152-d86dd58c88a1",
   "metadata": {},
   "source": [
    "## Timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb65aad-9d5c-410f-8706-abacdaf352e8",
   "metadata": {},
   "source": [
    "### Data range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b2d56-de89-4564-8f52-d5037885f50d",
   "metadata": {},
   "source": [
    "```Python\n",
    "service = 'hdatarange'\n",
    "url = f'{api.URL}/{service}/'\n",
    "response = requests.get(url + '9142', auth=requests.auth.HTTPBasicAuth(api.USERNAME, api.PASSWORD))\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Do something with the data\n",
    "else:\n",
    "    print(\"Failed to retrieve data from the API\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824922a-21b0-46a6-8d7c-bd685f02d9fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Operational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "466414dc-57f8-40b6-83aa-3f2c35f74385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stations in Guadiana\n",
    "# stations = gpd.read_file('Z:/nahaUsers/casadje/Guadiana/GIS/river_HDMS_20250314.shp').set_index('EFAS_ID', drop=True)\n",
    "stations = gpd.read_file('Z:/nahaUsers/casadje/datasets/hDMS/river/GIS/river_HDMS_20250314.shp').set_index('EFAS_ID', drop=True)\n",
    "stations = stations[stations.BASIN_EN == 'guadalquivir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc1af343-d90c-48f6-a8be-fe498bfa23a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75636f6e8bc54244b1a710a98add8df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "station:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\955.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\966.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\967.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\968.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\969.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\971.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\972.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\973.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\975.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\976.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\977.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\978.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\979.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\981.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\982.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\984.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\985.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\987.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\988.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\994.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\997.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\1000.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\1016.nc\n",
      "No data was found for station with EFAS_ID 2120\n",
      "No data was found for station with EFAS_ID 2121\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2831.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2832.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2833.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2834.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2835.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2836.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2837.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2838.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2839.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2840.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2841.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2842.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2843.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2844.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2845.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2846.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2847.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2848.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2987.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2991.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\2992.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3026.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3027.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3028.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3029.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3030.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3031.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3032.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3033.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3034.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3035.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3036.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3037.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3038.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3039.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3040.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3041.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3042.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3043.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3044.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3045.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3046.nc\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3047.nc\n",
      "No data was found for station with EFAS_ID 3048\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3049.nc\n",
      "No data was found for station with EFAS_ID 3050\n",
      "No data was found for station with EFAS_ID 3051\n",
      "No data was found for station with EFAS_ID 3052\n",
      "No data was found for station with EFAS_ID 3053\n",
      "No data was found for station with EFAS_ID 3054\n",
      "No data was found for station with EFAS_ID 3055\n",
      "No data was found for station with EFAS_ID 3056\n",
      "No data was found for station with EFAS_ID 3057\n",
      "No data was found for station with EFAS_ID 3058\n",
      "No data was found for station with EFAS_ID 3059\n",
      "No data was found for station with EFAS_ID 3060\n",
      "No data was found for station with EFAS_ID 3061\n",
      "No data was found for station with EFAS_ID 3062\n",
      "No data was found for station with EFAS_ID 3063\n",
      "No data was found for station with EFAS_ID 3064\n",
      "No data was found for station with EFAS_ID 3065\n",
      "No data was found for station with EFAS_ID 3066\n",
      "No data was found for station with EFAS_ID 3067\n",
      "No data was found for station with EFAS_ID 3068\n",
      "No data was found for station with EFAS_ID 3069\n",
      "No data was found for station with EFAS_ID 3070\n",
      "No data was found for station with EFAS_ID 3071\n",
      "No data was found for station with EFAS_ID 3072\n",
      "No data was found for station with EFAS_ID 3073\n",
      "No data was found for station with EFAS_ID 3074\n",
      "No data was found for station with EFAS_ID 3075\n",
      "No data was found for station with EFAS_ID 3076\n",
      "No data was found for station with EFAS_ID 3077\n",
      "Saved Z:\\nahaUsers\\casadje\\datasets\\hDMS\\river\\nhoperational24hw\\3092.nc\n",
      "No data was found for station with EFAS_ID 4547\n"
     ]
    }
   ],
   "source": [
    "# service = 'noperational24h' # 24 hours NRT operational data\n",
    "service = 'nhoperational24hw' # 24 hours historic weighted operational table\n",
    "\n",
    "# data must be downloaded in 4 batches due to server limitations\n",
    "dates = [date.date() for date in pd.date_range(START, END, periods=4)]\n",
    "\n",
    "# path where the data will be saved\n",
    "path_out = PATH_HDMS / service\n",
    "path_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pbar = tqdm(stations.index, desc='station', total=stations.shape[0])\n",
    "for ID in pbar:\n",
    "    \n",
    "    file_out = path_out / f'{ID}.nc'\n",
    "    if file_out.exists():\n",
    "        continue\n",
    "\n",
    "    # download data\n",
    "    data = {}\n",
    "    for var, variable in VARIABLES[TYPE].items(): # for each variable\n",
    "        if 'serie' in locals():\n",
    "            del serie\n",
    "        serie = pd.DataFrame(columns=[ID], dtype=float)\n",
    "        for i, (st, en) in enumerate(zip(dates[:-1], dates[1:])): # for each batch\n",
    "            if i > 0:\n",
    "                st += timedelta(days=1)           \n",
    "            url = f'{api.URL}/{service}/{st.strftime(strftime)}/{en.strftime(strftime)}/{ID}/{var}' #'/'\n",
    "            response = requests.get(url, auth=requests.auth.HTTPBasicAuth(api.USERNAME, api.PASSWORD))\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                if 'message' in response.json():\n",
    "                    continue\n",
    "                serie_i = pd.DataFrame(response.json())\n",
    "                if serie_i.shape[0] > 0:\n",
    "                    serie_i = serie_i[['Timestamp', 'AvgValue']].set_index('Timestamp', drop=True)\n",
    "                    serie_i.index = pd.to_datetime(serie_i.index)\n",
    "                    serie_i.columns = [ID]\n",
    "                    serie = pd.concat((serie, serie_i), axis=0)\n",
    "        if serie.shape[0] > 0:\n",
    "            data[variable] = serie.sort_index().copy()\n",
    "        \n",
    "    if len(data) > 0:\n",
    "        \n",
    "        # convert to xarray.Dataset\n",
    "        data = xr.Dataset({var: xr.DataArray(serie, dims=['date', 'ID']) for var, serie in data.items()})\n",
    "        \n",
    "        # export as NetCDF\n",
    "        data.to_netcdf(file_out)\n",
    "        print(f'Saved {file_out}')\n",
    "    else:\n",
    "        print(f'No data was found for station with EFAS_ID {ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd92ff5-a81d-479f-950d-362d644b2da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
