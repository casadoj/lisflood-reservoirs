{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1160f7-c385-49ac-ac01-fad2044faf0b",
   "metadata": {},
   "source": [
    "# ResOpsMX: download data\n",
    "***\n",
    "\n",
    "**Author:** Chus Casado Rodríguez<br>\n",
    "**Date:** 01-08-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "\n",
    "This notebook downloads the reservoir data available in the [Conagua](https://sih.conagua.gob.mx/basedatos/Presas/) website (Mexico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a8b9f7-746a-4cbc-996e-9002368991c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226443e-f266-4cc0-996a-8cf692867df1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f15619-c63b-4915-9acf-096d47329d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config_dataset.yml', 'r', encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "VERSION = cfg['version']\n",
    "\n",
    "URL = cfg['url']\n",
    "\n",
    "PATH_DATASET = Path(cfg['paths']['dataset']['root'])\n",
    "PATH_ATTR = PATH_DATASET / 'raw' / 'attributes'\n",
    "PATH_ATTR.mkdir(parents=True, exist_ok=True)\n",
    "PATH_TS = PATH_DATASET/ 'raw' / 'time_series'\n",
    "PATH_TS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70034036-e070-4faf-af83-734769efbe6e",
   "metadata": {},
   "source": [
    "## Catalogue of dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e76b03c-ff12-4f56-9f3a-c26aad161ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# download catalogue of dams\n",
    "response = requests.get(URL + '0_Catalogo_de_presas.csv')\n",
    "\n",
    "# if correct response\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    # format table of attributes\n",
    "    data_str = response.content.decode('latin-1')\n",
    "    data_io = StringIO(data_str)\n",
    "    dams = pd.read_csv(data_io)\n",
    "\n",
    "    # rename columns\n",
    "    rename_cols = {'Número': 'dam_ID',\n",
    "                   'Clave ': 'key',\n",
    "                   'Nombre de la presa': 'name',\n",
    "                   'Latitud': 'lat',\n",
    "                   'Longitud': 'lon',\n",
    "                   'Altitud': 'Z',\n",
    "                   'Estado': 'state',\n",
    "                   'Municipio': 'city',\n",
    "                   'Identificador de la \\ncuenca de disponibilidad': 'cat_ID',\n",
    "                   'Cuenca de disponibilidad': 'catchment',\n",
    "                   'Número de la \\nregión hidrológica': 'reg_ID',\n",
    "                   'Región hidrológica': 'region'}\n",
    "    dams.rename(columns=rename_cols, inplace=True)\n",
    "    dams.set_index('dam_ID', inplace=True, drop=True)\n",
    "    \n",
    "    # convert into GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(dams.lon, dams.lat)]\n",
    "    dams = gpd.GeoDataFrame(dams, geometry=geometry, crs='epsg:4326')\n",
    "\n",
    "    # export with columns renamed\n",
    "    dams.drop('geometry', axis=1).to_csv(PATH_ATTR / 'dams.csv', encoding='utf-8')\n",
    "    dams.to_file(PATH_ATTR / 'dams.shp', encoding='utf-8')\n",
    "    print(\"File downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43f718-b458-4709-87e0-1e2b2df80c76",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1869b-9c78-4954-bd4a-831a8811e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(dams.key.sort_values()):\n",
    "    \n",
    "    # download data\n",
    "    data = pd.read_csv(URL + f'{key}.csv', skiprows=7)\n",
    "    data.columns = [col.strip() for col in data.columns]\n",
    "\n",
    "    # rename columns\n",
    "    rename_cols = {'Fecha': 'date',\n",
    "                   'Elevación(msnm)': 'Z_MASL',\n",
    "                   'Almacenamiento(hm³)': 'V_MCM',\n",
    "                   'Area(ha)': 'A_HA',\n",
    "                   'Extracciones por Obra de Toma(m³/s)': 'SLUICE_CMS',\n",
    "                   'Extracciones por Vertedor(m³/s)': 'SPILL_CMS',\n",
    "                   'Evaporación(mm)': 'EVAP_MM',\n",
    "                   'Precipitación(mm)': 'PRECIP_MM'}\n",
    "    data.rename(columns=rename_cols, inplace=True)\n",
    "\n",
    "    # set date as index\n",
    "    data.date = pd.to_datetime(data.date, format='%Y/%m/%d')\n",
    "    data.set_index('date', drop=True, inplace=True)\n",
    "\n",
    "    # convert data to float\n",
    "    data.replace('-', np.nan, inplace=True)\n",
    "    data = data.astype(float)\n",
    "\n",
    "    # export\n",
    "    data.to_csv(PATH_TS / f'{key}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
