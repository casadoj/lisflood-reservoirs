{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455af9d0-e8c9-4935-a28a-53e7320ecba8",
   "metadata": {},
   "source": [
    "# Static maps\n",
    "***\n",
    "\n",
    "**_Autor:_** Chus Casado Rodríguez<br>\n",
    "**_Fecha:_** 02-04-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This notebook creates the files that make up the ResOpsES dataset.\n",
    "\n",
    "**To do:**<br>\n",
    "* [ ] Fix `point_polygon_statistics` to work with masks rasters instead of polygons, so that we can compute the attributes of reservoirs and lakes.\n",
    "* [ ] Would it be possible to use the new `lisflood-utilities.catchstats.catchment_statistics` function instead of `mask_statistics`?\n",
    "* [ ] In the [LISFLOOD parameters](#LISFLOOD-parameters), it could be interesting to extract the reservoir parameters for each of the reservoirs represented in EFAS.\n",
    "* [ ] There's 1 day lag between EFAS5 and EMO1. EMO1 starts January 2nd 1990, whereas EFAS5 January 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56aae7f-34f2-4639-b294-e79fcbe20668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import rioxarray\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.features import shapes\n",
    "from typing import Union, List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime\n",
    "\n",
    "from funciones import mask_statistics, read_static_map, plot_attributes#, dict2da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29654c04-471a-4957-98ad-206bc79abef1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56421aea-cd93-417c-a19c-b7a0154f0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the dataset ResOpsES\n",
    "path_datasets = Path('Z:/nahaUsers/casadje/datasets')\n",
    "path_ResOpsES = path_datasets / 'reservoirs' / 'ResOpsES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3fd05-ebcb-4fcf-b288-0f8ce05c1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to EFASv5\n",
    "path_EFAS = Path('E:/casadje/Documents/EFASv5/iberia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59bad1-8224-4d34-834d-86eb2eac2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crs = 4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2f208-f278-4195-8fa6-d453f0ccba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beginning of the study period: start of the EFAS5 long run\n",
    "start = datetime(1990, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3affc9-7c12-4217-8da4-8a7b22fc6c1e",
   "metadata": {},
   "source": [
    "## Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a655b9-840c-4ca0-b3de-bb1a7f78f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reservoir points\n",
    "reservoirs = gpd.read_file(path_ResOpsES / 'GIS' / 'reservoirs_ResOpsES.shp')\n",
    "reservoirs.set_index('SNCZI', inplace=True)\n",
    "reservoirs.index = reservoirs.index.astype(int)\n",
    "reservoirs.CAP_MCM = reservoirs.CAP_MCM.astype(float)\n",
    "reservoirs.CATCH_SKM = reservoirs.CATCH_SKM.astype(float)\n",
    "\n",
    "print('{0} reservoirs and {1} attributes'.format(*reservoirs.shape))\n",
    "\n",
    "# plot reservoir locations\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection=ccrs.PlateCarree()))\n",
    "ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m', edgecolor='face', facecolor='lightgray'),\n",
    "               alpha=.5,\n",
    "               zorder=0)\n",
    "reservoirs.plot(markersize=reservoirs.CAP_MCM**.5, cmap='viridis', c=reservoirs.CATCH_SKM, alpha=.5, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ecdab-005a-459a-b5f7-73caf0b1b327",
   "metadata": {},
   "source": [
    "## Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ba6bb-ee5d-4fb2-ae09-ff1dee7b4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path_ResOpsES / 'ancillary' / 'cutmaps'\n",
    "mask_map = 'upArea_01min.nc'\n",
    "\n",
    "masks = {}\n",
    "directories = [dir for dir in path.iterdir() if dir.is_dir()]\n",
    "for directory in tqdm(directories, desc='loading masks'):\n",
    "    try:\n",
    "        # reservoir ID\n",
    "        ID = int(directory.stem)\n",
    "        \n",
    "        # load upstream area map\n",
    "        mask = xr.open_dataset(directory / mask_map)['Band1']\n",
    "        mask.name = str(ID)\n",
    "        \n",
    "        # create and save a mask out of the previous map\n",
    "        mask = xr.where(mask.notnull(), 1, mask)\n",
    "        masks[ID] = mask\n",
    "    except:\n",
    "        print(directory)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f0384-6eed-4960-aeb1-408714c8f3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export masks as NetCDF with ID as filename to be used with `catchstats`\n",
    "path_masks = path_ResOpsES / 'ancillary' / 'cutmaps' / 'masks'\n",
    "path_masks.mkdir(parents=True, exist_ok=True)\n",
    "for ID, mask in masks.items():\n",
    "    mask.to_netcdf(path_masks / f'{ID:04}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11282855-8225-4992-bce8-73a0f45ead08",
   "metadata": {},
   "source": [
    "## Static maps\n",
    "\n",
    "In this section I will compute catchment statistics of the LISFLOOD static maps that will be in the end exported as _attributes_EFAS_static_maps.csv_. As ancillary maps, I will load first the pixel area and upstream area maps, that will be needed in the subsequent calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694c01f-5a6a-4d1b-bd72-6cc2984a419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pixel area map\n",
    "pixarea = xr.open_mfdataset(f'{path_EFAS}/maps/pixarea*.nc')['Band1'].compute()\n",
    "\n",
    "# load the upstream area map\n",
    "upArea = xr.open_mfdataset(f'{path_EFAS}/maps/upArea*.nc')['Band1'].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e1a089-2fab-4ba5-80de-797626d9eaa3",
   "metadata": {},
   "source": [
    "### Geomorphology\n",
    "\n",
    "Here I will compute catchment statistics for geomorphological attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac6967-e377-4b17-ab2c-a604547a609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = {'elv': ['mean', 'std', 'min', 'max'],\n",
    "        'gradient': ['mean', 'std'],\n",
    "        'upArea': ['max'],\n",
    "        # 'pixarea': ['sum']\n",
    "       }\n",
    "\n",
    "# cargar mapas\n",
    "geomorphology = xr.Dataset({var: xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')['Band1'].compute() for var in func})\n",
    "\n",
    "# calcular estadísticos\n",
    "attr_geomorphology = mask_statistics(geomorphology, masks, func, weight=pixarea).to_pandas()\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_geomorphology, reservoirs.geometry.x, reservoirs.geometry.y, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e498546-d4ce-40d0-8766-875c4f3d24c7",
   "metadata": {},
   "source": [
    "### Land use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb039434-17b0-4462-a684-bee8747caf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['fracforest', 'fracirrigated', 'fracother', 'fracrice', 'fracwater', 'fracsealed']\n",
    "variables.sort()\n",
    "\n",
    "# load maps\n",
    "land_use = xr.Dataset({var: xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')['Band1'].compute() for var in variables})\n",
    "land_use = land_use.rename({var: var[4:] for var in list(land_use)})\n",
    "\n",
    "# compute statistics\n",
    "attr_landuse = mask_statistics(land_use, masks, func='mean', weight=pixarea).to_pandas()\n",
    "attr_landuse.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# compute main land use\n",
    "lu_classes = {col: i for i, col in enumerate(attr_landuse.columns, start=1)}\n",
    "attr_landuse['land_use_main'] = attr_landuse.idxmax(axis=1).map(lu_classes)\n",
    "\n",
    "# rename attributes\n",
    "attr_landuse.rename(columns={col: col.split('_')[0] if 'mean' in col else col for col in attr_landuse}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_landuse, reservoirs.geometry.x, reservoirs.geometry.y, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee843-5b84-423b-ae66-d8085f2005da",
   "metadata": {},
   "source": [
    "### Crop coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da331e-0433-4760-a51d-c065fe912b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping land use acronym and name\n",
    "mapping_landuse = {'f': 'forest', 'i': 'irrigated', 'o': 'other'}\n",
    "variables = ['cropcoef_f', 'cropcoef_i', 'cropcoef_o']\n",
    "\n",
    "# load maps\n",
    "crops = xr.Dataset({var: xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')['Band1'].compute() for var in variables})\n",
    "crops = crops.rename({var: mapping_landuse[var.split('_')[1]] for var in list(crops)})\n",
    "\n",
    "# mean weighted by the fraction of pixel covered by each land use\n",
    "crops = crops.to_array('land_use').weighted(land_use.to_array('land_use').fillna(0)).sum('land_use', skipna=True) \n",
    "crops = crops.where(~upArea.isnull())\n",
    "crops.name = 'cropcoef'\n",
    "\n",
    "# compute statistics\n",
    "attr_crops = mask_statistics(crops, masks, func=['mean', 'std']).to_pandas()\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_crops, reservoirs.geometry.x, reservoirs.geometry.y, ncols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78a2cb-6bec-48aa-839d-31d351c4be25",
   "metadata": {},
   "source": [
    "### Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bff2e-2926-490a-9f05-985702e68f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = {'chanbnkf': ['mean'],\n",
    "        'chanbw': ['mean'],\n",
    "        'changrad': ['mean'],\n",
    "        'chanlength': ['sum'],\n",
    "        'chanman': ['mean']}\n",
    "\n",
    "# load maps\n",
    "streams = {var: xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')['Band1'].compute() for var in func}\n",
    "streams = {var: da.rename(var) for var, da in streams.items()}\n",
    "streams = {var : da.drop([coord for coord in list(da.coords) if coord not in ['lon', 'lat']]) for var, da in streams.items()}\n",
    "streams = xr.Dataset({var: xr.DataArray(da.data, coords=upArea.coords, name=var) for var, da in streams.items()})\n",
    "\n",
    "# mask streams (pixels with depth larger than 1 m)\n",
    "rivers = streams['chanbnkf'] > 1\n",
    "# rivers.plot(cmap='Blues')\n",
    "streams = streams.where(rivers)\n",
    "\n",
    "# calcular estadístico\n",
    "attr_streams = mask_statistics(streams, masks, func).to_pandas()\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_streams, reservoirs.geometry.x, reservoirs.geometry.y, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec50dc7-f321-4bd6-a9dd-5d8c117f41ea",
   "metadata": {},
   "source": [
    "### Soil properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8e136-d79c-4362-9cf2-bc887521fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['ksat', 'lambda', 'genua', 'soildepth', 'thetas', 'thetar']\n",
    "layers = [1, 2, 3]\n",
    "maps = [f'{var}{layer}' for var in variables for layer in layers]\n",
    "\n",
    "# load maps\n",
    "soils = {}\n",
    "for var in tqdm(maps, desc='loading maps'):\n",
    "    files = list((path_EFAS / 'maps').glob(f'{var}_*.nc'))\n",
    "    if len(files) > 1:\n",
    "        ds = {}\n",
    "        for file in files:\n",
    "            # type of land use\n",
    "            cover = mapping_landuse[file.stem.split('_')[1]]\n",
    "            # import map\n",
    "            ds[cover] = xr.open_dataset(file)['Band1']#read_static_map(file)\n",
    "        ds = xr.Dataset(ds)\n",
    "        da = ds.to_array('land_use').weighted(land_use.to_array('land_use').fillna(0)).sum('land_use', skipna=True)\n",
    "        soils[var] = da.where(~upArea.isnull())\n",
    "    elif len(files) == 1:\n",
    "        soils[var] = xr.open_dataset(files[0])['Band1'] # read_static_map(files[0])\n",
    "soils = xr.Dataset(soils)\n",
    "\n",
    "# compute statistics\n",
    "attr_soils = mask_statistics(soils, masks, func='mean').to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_soils.rename(columns={col: col.split('_')[0] for col in attr_soils if 'mean' in col}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_soils, reservoirs.geometry.x, reservoirs.geometry.y, ncols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93572e6c-608c-487f-9158-a017931e2d57",
   "metadata": {},
   "source": [
    "### LAI\n",
    "\n",
    "I convert the timeseries of 10-daily timesteps into annual averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92d400-f19a-4f46-acf6-546c221fdbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = ['laif', 'laii', 'laio']\n",
    "\n",
    "# load maps\n",
    "lai = xr.Dataset({var: xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')['Band1'].compute() for var in maps})\n",
    "lai = lai.rename({var: mapping_landuse[var[3]] for var in list(lai)})\n",
    "\n",
    "# mean wheighted by the portion of pixel covered by each land use\n",
    "lai = lai.to_array('land_use').weighted(land_use.to_array('land_use').fillna(0)).sum('land_use', skipna=True) \n",
    "lai = lai.where(~upArea.isnull())\n",
    "lai.name = 'lai'\n",
    "lai['time'] = pd.date_range('2021-01-05', periods=len(lai.time), freq='10D')\n",
    "\n",
    "# monthly resampling\n",
    "lai_m = lai.resample(time='1M').mean()\n",
    "lai_m['time'] = [f'{i:02}' for i in range(1, 13)]\n",
    "lai_agg = xr.Dataset({f'lai{month}': lai_m.sel(time=month).drop('time') for month in lai_m.time.data})\n",
    "\n",
    "# annual statistics\n",
    "lai_agg['laiyrmean'] = lai.mean('time')\n",
    "lai_agg['laiyrmax'] = lai.max('time')\n",
    "lai_agg['laiyrmin'] = lai.min('time')\n",
    "\n",
    "# compute statistics\n",
    "attr_lai = mask_statistics(lai_agg, masks, func=['mean']).to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_lai.rename(columns={col: '_'.join(col.split('_')[:-1]) for col in attr_lai if 'mean' in col}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_lai, reservoirs.geometry.x, reservoirs.geometry.y, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5630c-e57a-4af8-ba77-778b64b9263f",
   "metadata": {},
   "source": [
    "### Water demand\n",
    "\n",
    "The original demand maps are monthly time series (domestic, energy, industry, livestock) in mm/day for the period 1990-2023. \n",
    "\n",
    "I will compute annual and monthly averages and from those I will compute statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29dbd4e-5c34-4697-bf74-ce5f66ef7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = ['dom', 'ene', 'ind', 'liv']\n",
    "\n",
    "# load maps\n",
    "demand = xr.Dataset({var: xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')[var].compute() for var in maps})\n",
    "\n",
    "# monthly means\n",
    "demand_m = demand.groupby('time.month').mean('time')\n",
    "\n",
    "# annual mean\n",
    "demand_y = demand.groupby('time.year').mean('time').mean('year')\n",
    "# demand_y = demand.mean('time')\n",
    "\n",
    "# combine in a single dataset\n",
    "demand_agg = xr.Dataset()\n",
    "for key, da in demand_m.items():\n",
    "    for month in da.month.data:\n",
    "        demand_agg[f'{key}_{month:02}'] = da.sel(month=month).drop('month')\n",
    "    demand_agg[f'{key}_yr'] = demand_y[key]\n",
    "\n",
    "# convert to volume\n",
    "# demand_agg = demand_agg * 1e-3 * pixarea\n",
    "\n",
    "# calcular estadístico\n",
    "attr_demand = mask_statistics(demand_agg, masks, func='sum').to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_demand.rename(columns={col: '_'.join(col.split('_')[:-1]) for col in attr_demand if 'sum' in col}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_demand[['dom_yr', 'ene_yr', 'ind_yr', 'liv_yr']], reservoirs.geometry.x, reservoirs.geometry.y, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f704b-e457-479e-b8f1-b32a0ed452b2",
   "metadata": {},
   "source": [
    "### Reservoirs?\n",
    "\n",
    "It could be interesting to add two attributes that account for the number of reservoirs upstream and the total storage volume of those reservoirs. I can do it with the reservoirs in EFASv5, but those do not include all the reservoirs in ResOpsES (not to mention all the actual reservoirs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676cf738-ab6e-4114-9fb9-37bc834195db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load map\n",
    "# var = 'res'\n",
    "# da = xr.open_mfdataset(f'{path_EFAS}/maps/{var}_*.nc')[var].compute()\n",
    "\n",
    "# # extract reservoir ID\n",
    "# ids = np.unique(da)\n",
    "# ids = ids[~np.isnan(ids)]\n",
    "# print('EFAS represents {0} reservoirs in the study area'.format(len(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b35b13-cab0-4b9a-bebf-48a376062376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cargar metadatos de los embalses\n",
    "# res_efas = pd.read_csv(path_EFAS / 'tables' / 'EFAS_HRES_reservoirs_metadata.csv')\n",
    "# res_efas.set_index('ResID', inplace=True)\n",
    "\n",
    "# # recortar a los IDs que hay en el mapa\n",
    "# res_efas = res_efas.loc[ids, :]\n",
    "\n",
    "# # convertir en geopandas.GeoDataFrame\n",
    "# res_efas = gpd.GeoDataFrame(res_efas, geometry=[Point(xy) for xy in zip(res_efas.LisfloodX, res_efas.LisfloodY)], crs=crs)\n",
    "\n",
    "# # añadir atributos desde las tablas utilizadas LISFLOOD\n",
    "# for file in glob.glob(f'{path_EFAS}/tables/r*.txt'):\n",
    "#     var = file.split('\\\\')[-1].split('.')[0][1:]\n",
    "#     try:\n",
    "#         df = pd.read_csv(file, sep=' ', header=None)\n",
    "#         df.dropna(axis=1, how='all', inplace=True)\n",
    "#         df.columns = ['ResID', var]\n",
    "#         df.set_index('ResID', inplace=True, drop=True)\n",
    "#         res_efas[var] = df\n",
    "#     except:\n",
    "#         print(file)\n",
    "#         continue\n",
    "\n",
    "# # calcular estadístico\n",
    "# attr_reservoirs = point_polygon_statistics(res_efas, cuencas, func={'tstor': ['sum', 'count']})\n",
    "# attr_reservoirs.dropna(axis=1, how='all', inplace=True)\n",
    "# attr_reservoirs.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "# attr_reservoirs.columns = ['reserv_vol', 'reserv_count']\n",
    "\n",
    "# attr_reservoirs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e10c77-773a-4ff2-9019-8af0f00d645f",
   "metadata": {},
   "source": [
    "### Lakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6e481-f887-4305-a717-9492feb8c28c",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1fd237-43e3-4d27-8a5e-69eb2f006082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all attributes\n",
    "attrs = pd.concat((attr_geomorphology,\n",
    "                  attr_landuse,\n",
    "                  attr_crops,\n",
    "                  attr_streams,\n",
    "                  attr_soils,\n",
    "                  attr_lai,\n",
    "                  attr_demand,\n",
    "                  # attr_reservoirs,\n",
    "                  # attr_lakes,\n",
    "                 ), axis=1)\n",
    "attrs.index.name = 'SNCZI'\n",
    "attrs.sort_index(axis=0, inplace=True)\n",
    "\n",
    "print('{0} attributes define the characteristics of {1} catchments'.format(*attrs.shape[::-1]))\n",
    "\n",
    "# export\n",
    "attrs.to_csv(path_ResOpsES / 'attributes' / 'attributes_EFAS_static_maps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a22ca-931a-4eb5-9a04-69e7073265d1",
   "metadata": {},
   "source": [
    "## LISFLOOD parameters\n",
    "\n",
    "In this section I load the maps of LISFLOOD calibrated parameters an compute mean catchment values. The results are exported as a CSV file named _attributes_EFAS_model_parameters.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27994711-9d6c-4e2a-a095-1fc7457d6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of calibrated parameters in LISFLOOD\n",
    "pars = {file.stem: file for file in (path_EFAS / 'parameters').glob('*.nc') if not file.stem.lower().startswith('efas')}\n",
    "print(f'{len(pars)} parameters were calibrated in EFAS')\n",
    "\n",
    "# load parameter maps\n",
    "params = xr.Dataset({par: xr.open_dataset(file)[par].compute() for par, file in pars.items()})\n",
    "\n",
    "# plot reservoir parameters\n",
    "reservoir_parameters = ['adjust_Normal_Flood', 'ReservoirRnormqMult']\n",
    "fig, axes = plt.subplots(ncols=len(reservoir_parameters), figsize=(12, 4), sharex=True, sharey=True)\n",
    "for ax, par in zip(axes, reservoir_parameters):\n",
    "    params[par].plot(ax=ax, add_colorbar=True, cmap='magma', cbar_kwargs={'shrink': 0.66})\n",
    "    ax.scatter(reservoirs.geometry.x, reservoirs.geometry.y, s=2, c='w')\n",
    "    ax.set_title(par)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "# compute statistics\n",
    "parameters = mask_statistics(params, masks, func='mean').to_pandas()\n",
    "parameters.index.name = 'SNCZI'\n",
    "parameters.sort_index(axis=0, inplace=True)\n",
    "parameters.rename(columns={col: '_'.join(col.split('_')[:-1]) for col in parameters if 'mean' in col}, inplace=True)\n",
    "\n",
    "# plot parameters\n",
    "plot_attributes(parameters[reservoir_parameters], reservoirs.geometry.x, reservoirs.geometry.y, ncols=5)\n",
    "print('{0} LISFLOOD calibrated parameters have been averaged for {1} catchments'.format(*parameters.shape[::-1]))\n",
    "\n",
    "# export\n",
    "parameters.to_csv(path_ResOpsES / 'attributes' / 'attributes_EFAS_model_parameters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83f250-03d0-4e03-b3c4-d5dbcd885b40",
   "metadata": {},
   "source": [
    "## Time series\n",
    "\n",
    "In this section I used the EMO1 meteorological time series and the EFAS5 discharge reanalysis to create both hydrometeorological attributes (_attributes_EFAS_hydrometeorology.csv_) and the time series that may be used as input in the deep learning model.\n",
    "\n",
    "### Load time series\n",
    "#### Meteorology: EMO-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7c621-4853-44d4-8dcc-ae6add5bd75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meteorological time series\n",
    "path_meteo = path_ResOpsES / 'ancillary' / 'catchstats' / 'meteo'\n",
    "variables = [x.stem for x in path_meteo.iterdir() if x.is_dir()]\n",
    "meteo = xr.Dataset({f'{var}_emo1': xr.open_mfdataset(f'{path_meteo}/{var}/*.nc')[f'{var}_mean'].compute() for var in variables})\n",
    "meteo['time'] = meteo['time'] - np.timedelta64(6, 'h') # WARNING!! One day lag compared with EFAS5\n",
    "meteo = meteo.rename({'id': 'SNCZI'})\n",
    "\n",
    "# define attributes\n",
    "emo1_units = 'e0_emo1: potential evaporation from open water from EMO1 [mm/d]\\npr_emo1: observed precipitation from EMO1 [mm/d]\\nta_emo1: observed air temperature from EMO1 [°C]\\n'\n",
    "meteo.attrs['Units'] = emo1_units\n",
    "meteo.time.attrs['timezone'] = 'UTC+00'\n",
    "meteo.SNCZI.attrs['Description'] = 'The code that identifies a reservoir in the Spanish Inventory of Dams and Reservoirs (Sistema Nacional de Cartografía de Zonas Inundables)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c5fbc-5f63-4e07-bbd2-be84e7c6cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the time series of a specific catchment to check the areal aggregation\n",
    "# ID = 10\n",
    "# fig, axes = plt.subplots(nrows=3, figsize=(12, 12), sharex=True)\n",
    "# for ax, var in zip(axes, ['pr', 'ta', 'e0']):\n",
    "#     meteo[var].sel(id=ID).plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d7b79-40ea-4ef1-b801-49a0c381b3c1",
   "metadata": {},
   "source": [
    "#### EFAS long run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26d74f-a5fb-45b6-8172-4190fe43d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load discharge time series\n",
    "path_discharge = path_ResOpsES / 'ancillary' / 'ncextract' / 'discharge'\n",
    "dis = xr.open_mfdataset(f'{path_discharge}/*.nc', combine='nested', concat_dim='SNCZI')['dis'].compute()\n",
    "dis = dis.sel(SNCZI=reservoirs.index)\n",
    "\n",
    "# Create a CRS variable and set its attributes\n",
    "crs_attrs = {'epsg_code': 'EPSG:4326',\n",
    "             'semi_major_axis': 6378137.0,  # WGS 84\n",
    "             'inverse_flattening': 298.257223563,  # WGS 84\n",
    "             'grid_mapping_name': 'latitude_longitude'\n",
    "            }\n",
    "dis['crs'] = xr.DataArray(data=0, attrs=crs_attrs)  # CRS variable with its attributes\n",
    "\n",
    "# define attributes\n",
    "dis.attrs['Units'] = 'dis_efas5: discharge reanalysis (mm/d)\\n'\n",
    "dis.time.attrs['timezone'] = 'UTC+00'\n",
    "dis.SNCZI.attrs['Description'] = 'The code that identifies a reservoir in the Spanish Inventory of Dams and Reservoirs (Sistema Nacional de Cartografía de Zonas Inundables)'\n",
    "dis.lat.attrs = {'Units': 'degrees_north',\n",
    "                 'standard_name': 'latitude',\n",
    "                 'grid_mapping': 'crs'}\n",
    "dis.lon.attrs = {'Units': 'degrees_east',\n",
    "                 'standard_name': 'longitude',\n",
    "                 'grid_mapping': 'crs'}\n",
    "\n",
    "# DataArray of catchment area\n",
    "catch_area = attr_geomorphology['upArea_max'].copy()\n",
    "catch_area.index.name = 'SNCZI'\n",
    "catch_area.name = 'CATCH_SM' # in m²\n",
    "catch_area = xr.DataArray(catch_area)\n",
    "\n",
    "# compute specific discharge\n",
    "dis = dis / catch_area * 3600 * 24 * 1000 # mm/d\n",
    "dis = dis.round(2)\n",
    "dis.name = 'dis_efas5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ab984-b02b-4774-89a1-499985b213d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Observation\n",
    "\n",
    "At the model of writing, only the reservoirs from CEDEX are available in the Hydrological Data Management Service (HDMS). The data in HDMS has been quality checked and interpolated to the timezone UTC+00. The data from the Catalan (Agència Catalana del Aigua, ACA) and Andalusian (Hidrosur) agencies are not yet in HDMS, so I will use the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa37feb-ed4b-42ac-991e-1d6027aa10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dis.time[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b1cbb-1eb9-4e8a-877d-143122452098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to the individual data sets\n",
    "path_HDMS = path_datasets / 'hDMS' / 'reservoirs'\n",
    "path_hidrosur = path_datasets / 'Hidrosur' / 'processed' / 'reservoirs'\n",
    "path_aca = path_datasets / 'ACA' / 'processed' / 'reservoirs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d9569-9db1-4e17-97b8-0866bb606fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reservoirs in the Hydrological Data Management Service\n",
    "hdms = gpd.read_file(path_HDMS / 'reservoirs.shp')\n",
    "hdms = hdms[['EFAS_ID', 'Name', 'National_S', 'Prov_ID', 'Country-Co', 'Country']]\n",
    "hdms = hdms.loc[hdms.Prov_ID == '1088']\n",
    "hdms[['EFAS_ID', 'National_S', 'Prov_ID']] = hdms[['EFAS_ID', 'National_S', 'Prov_ID']].astype(int)\n",
    "hdms.rename(columns={'National_S': 'LOCAL_ID'}, inplace=True)\n",
    "hdms.set_index('LOCAL_ID', drop=True, inplace=True)\n",
    "\n",
    "print('There are {0} reservoirs in HDMS'.format(hdms.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d62d5c-0619-44f7-94b8-640067605ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_variables = ['level', 'volume', 'outflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885413c-7cc5-49de-9019-bacfa25fc1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load observed time series\n",
    "obs = {}\n",
    "for ID in tqdm(reservoirs.index, desc='loading observed timeseries'):\n",
    "        \n",
    "    local_ID, provider = reservoirs.loc[ID, ['ID_TS', 'SOURCE_TS']]\n",
    "    \n",
    "    if provider == 'CEDEX':\n",
    "        hdms_ID = hdms.loc[int(local_ID), 'EFAS_ID']\n",
    "        file = path_HDMS / 'nhoperational24hw' / f'{hdms_ID:04}.nc'\n",
    "        if file.exists():\n",
    "            ds = xr.open_dataset(file).sel(ID=hdms_ID)\n",
    "            ds['ID'] = ID\n",
    "        else:\n",
    "            print(f'File {file} does not exist')\n",
    "    elif provider == 'ACA':\n",
    "        file = path_aca / 'timeseries' / f'{local_ID}.csv'\n",
    "        if file.exists():\n",
    "            df = pd.read_csv(file, usecols=['date', 'level', 'volume'])\n",
    "            df.date = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "            df['ID'] = ID\n",
    "            df.set_index(['date', 'ID'], drop=True, inplace=True)\n",
    "            ds = xr.Dataset.from_dataframe(df).sel(ID=ID)\n",
    "        else:\n",
    "            print(f'File {file} does not exist')\n",
    "    elif provider == 'Hidrosur':\n",
    "        file = path_hidrosur / 'timeseries' / f'{int(local_ID):03}.csv'\n",
    "        if file.exists():\n",
    "            df = pd.read_csv(file)\n",
    "            df.datetime = pd.to_datetime(df.datetime)\n",
    "            # select values at the beginning of each day\n",
    "            df.set_index('datetime', drop=True, inplace=True)\n",
    "            datemin, datemax = df.index[df.index.hour == 0][[0, -1]]\n",
    "            idx = pd.date_range(datemin, datemax, freq='D')\n",
    "            df = df.loc[idx]\n",
    "            # convert to xarray.Dataset\n",
    "            df.index.name = 'date'\n",
    "            df['ID'] = ID\n",
    "            df.set_index([df.index, df.ID], inplace=True)\n",
    "            df.drop('ID', axis=1, inplace=True)\n",
    "            ds = xr.Dataset.from_dataframe(df).sel(ID=ID)\n",
    "        else:\n",
    "            print(f'File {file} does not exist')\n",
    "    else:\n",
    "        continue\n",
    "    if ds.date.data[0] < start:\n",
    "        ds = ds.sel(date=slice(start, None))\n",
    "    obs[ID] = ds.rename({'ID': 'SNCZI'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad72dfa-50a3-4022-a71f-f9939c28b23a",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "Here I derive the attributes from the time series.\n",
    "\n",
    "#### EMO-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521e4e2-72cf-48cc-9ded-6ac202af65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly aggregation (as mean daily value)\n",
    "meteo_m = meteo.groupby('time.month').mean()\n",
    "meteo_m = meteo_m.rename({'month': 'time'})\n",
    "\n",
    "# annual aggregation (as mean daily value)\n",
    "meteo_y = meteo.mean('time')\n",
    "meteo_y['time'] = ['year']\n",
    "\n",
    "# concatenate monthly and annual aggregations\n",
    "meteo_ag = xr.concat((meteo_m, meteo_y), dim='time')\n",
    "\n",
    "# organizar en un DataFrame\n",
    "attr_climate = pd.DataFrame(index=meteo_ag.SNCZI.data)\n",
    "for var, da in tqdm(meteo_ag.items()):\n",
    "    # mensual\n",
    "    df = da.to_pandas().transpose()\n",
    "    df.columns = [f'{var}_{i:02}' if i != 'year' else f'{var}_{i}' for i in df.columns]\n",
    "    # concatenar\n",
    "    attr_climate = pd.concat((attr_climate, df), axis=1)\n",
    "attr_climate.index.name = 'SNCZI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234f689-9cd2-4afe-b936-6f4bb7ed649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots of monthly and annual meteorological values\n",
    "ncols = len(meteo)\n",
    "fig, axes = plt.subplots(ncols=ncols, figsize=(5 * ncols, 3.5))\n",
    "\n",
    "for ax, var in zip(axes, list(meteo)):\n",
    "    mask_var = attr_climate.columns.str.startswith(var)\n",
    "    df = attr_climate.loc[:, mask_var]\n",
    "    ax.boxplot(df)\n",
    "    if var in ['pr_emo1', 'e0_emo1']:\n",
    "        ax.set_ylabel('mm/d')\n",
    "    elif var in ['ta_emo1']:\n",
    "        ax.set_ylabel('°C')\n",
    "    ax.set_title(' '.join(var.split('_')))\n",
    "    ax.set_xticklabels([i for i in range(1, 13)] + ['yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478b2ab-bb2d-40eb-afa3-82abff03301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps of annual meteorological averages\n",
    "plot_attributes(attr_climate[['pr_emo1_year', 'ta_emo1_year', 'e0_emo1_year']],\n",
    "                reservoirs.geometry.x,\n",
    "                reservoirs.geometry.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cba0b-0dcc-4fa8-80bb-a6238b3b3f31",
   "metadata": {},
   "source": [
    "#### EFAS long run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6783-fa3a-4a33-901e-db7ac62cba11",
   "metadata": {},
   "source": [
    "<font color='red'>Add standard deviation or coefficient of variation?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8902cce-89ae-4934-9be6-9fa8ac11c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'dis_efas5'\n",
    "\n",
    "# mean monthly discharge inflow\n",
    "dis_m = dis.groupby('time.month').mean('time').to_pandas()\n",
    "dis_m.columns = [f'{var}_{month:02}' for month in dis_m.columns]\n",
    "\n",
    "# annual average on minimum, mean and maximum discharge inflow\n",
    "dis_y = dis.groupby('time.year')\n",
    "dis_y = pd.DataFrame({f'{var}_yrmean': dis_y.mean().mean('year').to_pandas(),\n",
    "                      f'{var}_yrmax': dis_y.max().mean('year').to_pandas(),\n",
    "                      f'{var}_yrmin': dis_y.min().mean('year').to_pandas()})\n",
    "dis_y.index.name = 'SNCZI'\n",
    "\n",
    "# combine in a single DataFrame\n",
    "attr_discharge = pd.concat((dis_m, dis_y), axis=1)\n",
    "\n",
    "# plot\n",
    "plot_attributes(attr_discharge[['dis_efas5_yrmean', 'dis_efas5_yrmax', 'dis_efas5_yrmin']], reservoirs.geometry.x, reservoirs.geometry.y, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670a850-6c66-461e-8498-11236a6393d6",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f40a5-6782-4f06-a46c-22a736482607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate attributes\n",
    "attr_hydrology = pd.concat((attr_climate, attr_discharge), axis=1)\n",
    "\n",
    "print('{0} attributes define the hydrometeorology of {1} catchments'.format(*attr_hydrology.shape[::-1]))\n",
    "\n",
    "# exportar\n",
    "attr_hydrology.to_csv(path_ResOpsES / 'attributes' / 'atributes_EFAS_hydrometeorology.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bbcb3-016a-4bb9-a549-3d170e24b230",
   "metadata": {},
   "source": [
    "### Time series\n",
    "\n",
    "Here I generate the files in the *timeseries* subfolder of the data set that will be the forcing data in the deep learning model.\n",
    "\n",
    "<font color='red'>I'm missing the observed data!!!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f353dce-2bb1-4c54-b6c8-b2bf9feb2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename variables in 'meteo' and 'dis'\n",
    "if 'time' in meteo.coords:\n",
    "    meteo = meteo.rename({'time' : 'date'})\n",
    "if 'time' in dis.coords:\n",
    "    dis = dis.rename({'time' : 'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1a36a-4f1c-4c67-aea6-50d0fc9b87fb",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3879ddd-bd94-4b84-9c04-de81073b4c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output directory\n",
    "path_ts_csv = path_ResOpsES / 'timeseries' / 'csv' \n",
    "path_ts_csv.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for ID in tqdm(reservoirs.index, desc='reservoirs'):\n",
    "\n",
    "    # extract meteorological time series\n",
    "    emo = meteo.sel(SNCZI=ID).drop('SNCZI').to_pandas().round(2)\n",
    "\n",
    "    # extract discharge time series\n",
    "    efas = dis.sel(SNCZI=ID).drop('SNCZI').to_pandas().round(2)\n",
    "    efas.name = 'dis_efas5'\n",
    "    \n",
    "    # observed time series\n",
    "    obs_id = obs[ID].drop('SNCZI').to_pandas().round(3)\n",
    "\n",
    "    # concatenate all time series\n",
    "    df = pd.concat((obs_id, efas, emo), axis=1)\n",
    "    df.index.name = 'date'\n",
    "    \n",
    "    # export\n",
    "    df.to_csv(path_ts_csv / f'{ID:04}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e3a5d-a20a-44aa-b007-c8409940c08a",
   "metadata": {},
   "source": [
    "#### NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882321d7-cac8-42ce-81c9-3238c994bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory\n",
    "path_ts_nc = path_ResOpsES / 'timeseries' / 'netcdf' \n",
    "path_ts_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for ID in tqdm(reservoirs.index, desc='reservoirs'):\n",
    "    \n",
    "    # concatenar series meteorológicas\n",
    "    ds = xr.merge((obs[ID].drop('SNCZI'), dis.sel(SNCZI=ID).drop('SNCZI'), meteo.sel(SNCZI=ID).drop('SNCZI')))\n",
    "    \n",
    "    # export\n",
    "    ds.to_netcdf(path_ts_nc / f'{ID:04}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
