{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# ResOpsBR: time series\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado<br>\n",
    "**Date:** 18-07-2025<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code creates the time series for the reservoirs in ResOpsBR. The time series include records from ANA and simulations from GloFAS. For each reservoir, these time series are exported both in CSV and a NetCDF format.\n",
    "\n",
    "Records are cleaned to avoid errors:\n",
    "    * Outliers in the **storage** time series are filtered by comparison with a moving median (window 7 days). If the relative difference of a given storage value and the moving median exceeds a threshold, the value is removed. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_storage()`\n",
    "    * Outliers in the **inflow** time series are removed using two conditions: one based in the gradient, and the other using an estimated inflow based on the water balance. When both conditions are met, the value is removed. Since inflow time series cannot contain missing values when used in the reservoir simulation, a simple linear interpolation is used to fill in gaps up to 7 days. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_inflow()`.\n",
    "\n",
    "**To do:**<br>\n",
    "* [x] Plot time series\n",
    "* [x] Make sure that there aren't negative values in the time series, nor zeros in storage.\n",
    "* [ ] Check the quality of the data by closing the mass balance when possible. <font color='steelblue'>I've used the mass balance to identify errors in the inflow time series (function `clean_inflow`).</font>.\n",
    "* [ ] Fill in the inflow time series with the mass balance, if possible. <font color='steelblue'>I've filled in gaps in the inflow time series with linear interpolation up to 7-day gaps (function `clean_inflow`).</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f56e74-ae31-4b60-be69-fc127719c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "# from copy import deepcopy\n",
    "\n",
    "from lisfloodreservoirs.utils import DatasetConfig\n",
    "from lisfloodreservoirs import read_attributes\n",
    "from lisfloodreservoirs.utils.plots import plot_resops, reservoir_analysis, compare_flows\n",
    "from lisfloodreservoirs.utils.timeseries import clean_storage, clean_inflow, time_encoding, fit_reservoir_curve, storage_from_elevation, elevation_from_storage\n",
    "from lisfloodreservoirs.utils.timezone import convert_to_utc, reindex_to_00utc\n",
    "\n",
    "from utils_br import plot_timeseries_BR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9034a7b1-74d9-47ea-8457-7f748a3eb4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsBR\\v1.1\\time_series\n"
     ]
    }
   ],
   "source": [
    "cfg = DatasetConfig('config_ResOpsBR_v11.yml')\n",
    "\n",
    "print(f'Time series will be saved in {cfg.PATH_TS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbf891-79d3-4317-9964-b99f1df92649",
   "metadata": {},
   "source": [
    "### Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f65985d-9a72-4cb6-80ce-faf5503d21a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 reservoirs in the attribute tables\n"
     ]
    }
   ],
   "source": [
    "# import all tables of attributes\n",
    "attributes = read_attributes(cfg.PATH_ATTRS, index_col='GDW_ID')\n",
    "map_ana_gdw = {sar_id: gdw_id for gdw_id, sar_id in attributes['SAR_ID'].items()}\n",
    "print(f'{attributes.shape[0]} reservoirs in the attribute tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a66cec-c79a-4862-8595-b0303c7d4f05",
   "metadata": {},
   "source": [
    "### Time series\n",
    "#### Observed: ANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f0aa8f1-c265-4dea-8294-a7ca2224bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff702d70ee91482c985a7aca4aa1e47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsBR\\raw\\time_series\\SIN\\19164.csv doesn't exist\n",
      "142 reservoirs in ResOpsBR time series\n"
     ]
    }
   ],
   "source": [
    "path_plots = cfg.PATH_TS / 'plots'\n",
    "path_plots.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# read time series\n",
    "resops_ts = {}\n",
    "for sar_id, gdw_id in tqdm(map_ana_gdw.items()):\n",
    "\n",
    "    # load timeseries\n",
    "    file = cfg.PATH_OBS_TS / f'{sar_id}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=['date'], index_col='date')\n",
    "        series['volume_mcm'] = series.volume_pct / 100 * attributes.loc[gdw_id, 'CAP_MCM']\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "        continue\n",
    "\n",
    "    # trim to GloFAS long run period\n",
    "    series = series.loc[cfg.START:cfg.END,:]\n",
    "    if series.empty:\n",
    "        print(f'Reservoir {gdw_id} has no observations in the time period from {cfg.START} to {cfg.END}')\n",
    "        continue\n",
    "    # # ensure there aren't gaps in the dates\n",
    "    series = series.asfreq('D')\n",
    "    series.index.name = 'date'\n",
    "\n",
    "    # rename columns\n",
    "    rename_cols = {\n",
    "        'volume_mcm': 'storage',\n",
    "        'level_m': 'elevation',\n",
    "        'inflow_cms': 'inflow',\n",
    "        'outflow_cms': 'outflow',\n",
    "        'outflow_spillway_cms': 'spillway',\n",
    "        'outflow_turbine_cms': 'turbine',\n",
    "        'outflow_natural_cms': 'natural',\n",
    "    }\n",
    "    series.rename(columns=rename_cols, inplace=True)\n",
    "    series = series[rename_cols.values()]\n",
    "\n",
    "    # remove negative values\n",
    "    series[series < 0] = np.nan\n",
    "    # clean storage time series\n",
    "    series.storage = clean_storage(series.storage, w=7, error_thr=.1)\n",
    "    # clean inflow time series\n",
    "    series.inflow = clean_inflow(\n",
    "        series.inflow,\n",
    "        storage=series.storage,\n",
    "        outlfow=series.outflow,\n",
    "        grad_thr=1e4,\n",
    "        balance_thr=5,\n",
    "        int_method='linear'\n",
    "    )\n",
    "    \n",
    "    # # trim time series to period with inflow, storage and outflow\n",
    "    # mask_availability = series[['inflow', 'storage', 'outflow']].notnull().all(axis=1)\n",
    "    # if mask_availability.sum() == 0:\n",
    "    #     continue\n",
    "    # start, end = series[mask_availability].first_valid_index(), series[mask_availability].last_valid_index()\n",
    "    start = series[['storage', 'elevation', 'inflow', 'outflow']].first_valid_index()\n",
    "    end = series[['storage', 'elevation', 'inflow', 'outflow']].last_valid_index()\n",
    "    start, end = max(cfg.START, start), min(cfg.END, end)\n",
    "    attributes.loc[gdw_id, ['TIME_SERIES_START', 'TIME_SERIES_END']] = start, end\n",
    "    series = series.loc[start:end]\n",
    "    \n",
    "    try:\n",
    "        # convert time series to UTC (with offset)\n",
    "        series = convert_to_utc(\n",
    "            lon=attributes.loc[gdw_id, 'LON'], \n",
    "            lat=attributes.loc[gdw_id, 'LAT'], \n",
    "            series=series\n",
    "        )\n",
    "        # interpolate values to 00 UTC\n",
    "        series = reindex_to_00utc(series)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert to UCT the time series for GDW_ID {gdw_id}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # save in dictionary\n",
    "    series.index = pd.DatetimeIndex(series.index.date, name='date')\n",
    "    resops_ts[gdw_id] = series\n",
    "\n",
    "    # plot observed time series\n",
    "    plot_resops(\n",
    "        series.storage,\n",
    "        series.elevation,\n",
    "        series.inflow,\n",
    "        series.outflow,\n",
    "        attributes.loc[gdw_id, ['CAP_MCM', 'SAR_VO_MAX']],\n",
    "        title=gdw_id,\n",
    "        save=path_plots / f'{gdw_id:04}_lineplot.jpg'\n",
    "        )\n",
    "    \n",
    "print(f'{len(resops_ts)} reservoirs in ResOpsBR time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6526d61-6aab-4696-80ee-7fb74548378b",
   "metadata": {},
   "source": [
    "##### **Plot timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cea0f8a-e8a4-4fb7-8bae-82eac2c9890e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7b32714cf946b98ea9fd33ae45d5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PATH_PLOTS = cfg.PATH_TS / 'plots'\n",
    "# PATH_PLOTS.mkdir(exist_ok=True)\n",
    "\n",
    "# for gdw_id, ts in tqdm(resops_ts.items()):\n",
    "#     max_storage = {\n",
    "#         'GDW': attributes.loc[gdw_id, 'CAP_MCM'],\n",
    "#         # 'BR': \n",
    "#     }\n",
    "#     max_elevation = {\n",
    "#         'GDW': attributes.loc[gdw_id, 'ELEV_MASL'],\n",
    "#         # 'BR': \n",
    "#     }\n",
    "#     title = '{0} - {1}'.format(gdw_id, attributes.loc[gdw_id, 'DAM_NAME'])\n",
    "#     plot_timeseries_BR(\n",
    "#         ts.storage,\n",
    "#         ts.elevation,\n",
    "#         ts.outflow,\n",
    "#         ts.inflow,\n",
    "#         max_storage,\n",
    "#         max_elevation,\n",
    "#         # zlim=(attributes.loc[gdw_id, 'NAME_MASL'] - attributes.loc[gdw_id, 'DAM_HGT_M'] * 1.2, None),\n",
    "#         title=title,\n",
    "#         save=PATH_PLOTS / f'{gdw_id}.jpg'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560a3940-2c9d-4550-8e9a-bfb4e4b61038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdw_id = 1363 #1349 #1347 #1333\n",
    "# ts = timeseries[gdw_id]\n",
    "\n",
    "# plot_resops(ts.storage, ts.elevation, outflow=ts.outflow,\n",
    "#             capacity=attributes.loc[gdw_id, ['NAME_MCM', 'NAMO_MCM']],\n",
    "#             level=attributes.loc[gdw_id, ['NAME_MASL', 'NAMO_MASL']])\n",
    "\n",
    "# plot_resops(ts.storage, ts.area, outflow=ts.outflow,\n",
    "#             capacity=attributes.loc[gdw_id, ['NAME_MCM', 'NAMO_MCM']],\n",
    "#             # level=attributes.loc[gdw_id, ['NAME_MASL', 'NAMO_MASL']]\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c53eb1d-d35c-4c92-aedd-79f966374e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to xarray.Dataset\n",
    "xarray_list = []\n",
    "for key, df in resops_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(GDW_ID=key)\n",
    "    xarray_list.append(ds)\n",
    "obs = xr.concat(xarray_list, dim='GDW_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "#### Simulated: GloFAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92a66218-981a-4e73-8c0d-e29bebfe9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import GloFAS simulation\n",
    "# sim = xr.open_dataset(cfg.PATH_SIM_TS / 'dis.nc')\n",
    "# sim = sim.rename({'time': 'date', 'id': 'GDW_ID', 'dis': 'inflow'})\n",
    "\n",
    "# # bias correct\n",
    "# for gdw_id in sim.GDW_ID.data:\n",
    "    \n",
    "#     if gdw_id not in timeseries:\n",
    "#         continue\n",
    "        \n",
    "#     inflow = sim['inflow'].sel(GDW_ID=gdw_id).to_pandas()\n",
    "#     inflow.name = 'inflow'\n",
    "#     ts = timeseries[gdw_id]\n",
    "    \n",
    "#     # compute net inflow\n",
    "#     if ('outflow' in ts.columns) & ('storage' in ts.columns):\n",
    "#         ΔS = ts.storage.diff().values\n",
    "#         net_inflow = ΔS * 1e6 / (24 * 3600) + ts.outflow\n",
    "#         net_inflow[net_inflow < 0] = 0\n",
    "#         net_inflow.name = 'net_inflow'\n",
    "\n",
    "#     # bias correct simulated inflow\n",
    "#     inflow_bc = quantile_mapping(obs=net_inflow,\n",
    "#                                  sim=inflow)\n",
    "#     inflow_bc.name = 'inflow_bc'\n",
    "    \n",
    "#     # # plot raw vs bias-corrected inflow\n",
    "#     # compare_flows(ts.storage, ts.outflow, inflow, inflow_bc)\n",
    "    \n",
    "#     # overwrite bias-corrected inflow\n",
    "#     sim['inflow'].loc[{'GDW_ID': gdw_id}] = inflow_bc.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746404c-c5d9-4e65-8c73-41f478ef4501",
   "metadata": {},
   "source": [
    "#### Meteorology: areal\n",
    "\n",
    "Time series of catchment-average meteorology generated with the LISFLOOD utility `catchstats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2721d204-ec81-4e65-9ee2-22720834d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 reservoirs and 3 variables in the areal meteorological time series\n"
     ]
    }
   ],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_areal = cfg.PATH_RESOPS / 'ancillary' / 'catchstats' / 'meteo'\n",
    "rename_vars = {\n",
    "    'id': 'GDW_ID',\n",
    "    'time': 'date',\n",
    "    'e0': 'evapo_areal',\n",
    "    'tp': 'precip_areal',\n",
    "    'ta': 'temp_areal',\n",
    "}\n",
    "variables = [x.stem for x in path_meteo_areal.iterdir() if x.is_dir() & (x.stem in rename_vars)]\n",
    "meteo_areal = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_areal}/{var}/*.nc')[f'{var}_mean'] for var in variables})\n",
    "\n",
    "# rename variables and coordinates\n",
    "meteo_areal = meteo_areal.rename(rename_vars)\n",
    "\n",
    "# correct and trim time\n",
    "meteo_areal['date'] = meteo_areal['date'] - pd.Timedelta(days=1) # WARNING!! One day lag compared with LISFLOOD\n",
    "meteo_areal = meteo_areal.sel(date=slice(cfg.START, cfg.END))\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_areal.GDW_ID.data))\n",
    "meteo_areal = meteo_areal.sel(GDW_ID=IDs)\n",
    "\n",
    "# compute\n",
    "meteo_areal = meteo_areal.compute()\n",
    "\n",
    "# # define attributes\n",
    "# meteo_units = 'evapo_areal: catchment-average potential evaporation from open water from ERA5 [mm/d]\\n' \\\n",
    "#     'precip_areal: catchment-average precipitation from ERA5 [mm/d]\\n' \\\n",
    "#     'temp_areal: catchment-average air temperature from ERA5 [°C]\\n'\n",
    "# meteo_areal.attrs['Units'] = meteo_units\n",
    "# meteo_areal.time.attrs['timezone'] = 'UTC+00'\n",
    "# meteo_areal.GDW_ID.attrs['Description'] = 'The identifier of the reservor in GRanD (Global Reservoir and Dam database)'\n",
    "\n",
    "print(f'{len(meteo_areal.GDW_ID)} reservoirs and {len(meteo_areal)} variables in the areal meteorological time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf6a18-8346-458e-a73c-73744b0d4249",
   "metadata": {},
   "source": [
    "#### Meteorology: point\n",
    "\n",
    "Time series of reservoir point meteorology extracted with the LISFLOOD utilitiy `ncextract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80c63708-9795-43d8-b30f-12c01bbf981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 reservoirs and 3 variables in the areal meteorological time series\n"
     ]
    }
   ],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_point = cfg.PATH_RESOPS / 'ancillary' / 'ncextract' / 'meteo'\n",
    "rename_vars = {\n",
    "    'id': 'GDW_ID',\n",
    "    'time': 'date',\n",
    "    'e0': 'evapo_point',\n",
    "    'tp': 'precip_point',\n",
    "    'ta': 'temp_point',\n",
    "}\n",
    "variables = [x.stem for x in path_meteo_point.iterdir() if x.is_dir() & (x.stem in rename_vars)]\n",
    "meteo_point = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_point}/{var}/*.nc')[var] for var in variables})\n",
    "\n",
    "# rename variables and coordinates\n",
    "meteo_point = meteo_point.rename(rename_vars)\n",
    "meteo_point = meteo_point.drop_vars(['surface', 'lat', 'latitude', 'lon', 'longitude'], errors='ignore')\n",
    "\n",
    "# correct and trim time\n",
    "meteo_point['date'] = meteo_point['date'] - pd.Timedelta(days=1) # WARNING!! One day lag compared with LISFLOOD\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_point.GDW_ID.data))\n",
    "meteo_point = meteo_point.sel(GDW_ID=IDs)\n",
    "\n",
    "# meteo_point = meteo_point.drop_vars(['lon', 'lat'], errors='ignore')\n",
    "\n",
    "# compute\n",
    "meteo_point = meteo_point.compute()\n",
    "\n",
    "# # define attributes\n",
    "# meteo_units = 'evapo_point: potential evaporation at the reservoir location from open water from ERA5 [mm/d]\\n' \\\n",
    "#     'precip_point: precipitation at the reservoir location from ERA5 [mm/d]\\n' \\\n",
    "#     'temp_point: air temperature  at the reservoir location from ERA5 [°C]\\n'\n",
    "# meteo_point.attrs['Units'] = meteo_units\n",
    "# meteo_point.time.attrs['timezone'] = 'UTC+00'\n",
    "# meteo_point.GDW_ID.attrs['Description'] = 'The identifier of the reservor in GRanD (Global Reservoir and Dam database)'\n",
    "\n",
    "print(f'{len(meteo_point.GDW_ID)} reservoirs and {len(meteo_point)} variables in the areal meteorological time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5c32d-83b3-4747-a327-bdbd20417914",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "### Convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c817905b-edc7-4540-b237-8dc9021c7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.NORMALIZE:\n",
    "\n",
    "    # reservoir attributes used to normalize the dataset\n",
    "    area_sm = xr.DataArray.from_series(attributes.AREA_SKM) * 1e6 # m2\n",
    "    capacity_cm = xr.DataArray.from_series(attributes.CAP_MCM) * 1e6 # m3\n",
    "    catchment_sm = xr.DataArray.from_series(attributes.CATCH_SKM) * 1e6 # m2\n",
    "    \n",
    "    # Observed timeseries\n",
    "    # -------------------\n",
    "    for var, da in obs.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var in ['storage', 'evaporation']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var in ['inflow', 'outflow']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 24 * 3600 / capacity_cm\n",
    "\n",
    "    # # Simulated timeseries\n",
    "    # # -------------------\n",
    "    # for var, da in sim.items():\n",
    "    #     # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "    #     if var.split('_')[0] in ['storage']:\n",
    "    #         sim[f'{var}_norm'] = sim[var] * 1e6 / capacity_cm\n",
    "    #     # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "    #     elif var.split('_')[0] in ['inflow', 'outflow']:\n",
    "    #         sim[f'{var}_norm'] = sim[var] * 24 * 3600 / capacity_cm\n",
    "            \n",
    "    # Catchment meteorology\n",
    "    # ---------------------\n",
    "    # convert areal evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_areal[f'{var}_areal_norm'] = meteo_areal[f'{var}_areal'] * catchment_sm * 1e-3 / capacity_cm\n",
    "\n",
    "    # Point meteorology\n",
    "    # ---------------------\n",
    "    # convert point evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_point[f'{var}_point_norm'] = meteo_point[f'{var}_point'] * catchment_sm * 1e-3 / capacity_cm   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed75f2-62e4-442e-b897-52dee820581e",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ef362ba-a408-490f-be6b-ed74d7cb4ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd57e7bf9e24648936024d4773eddb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting time series:   0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir 40851 does not have observations. Skipping to the next reservoir\n"
     ]
    }
   ],
   "source": [
    "path_csv = cfg.PATH_TS / 'csv'\n",
    "path_csv.mkdir(parents=True, exist_ok=True)\n",
    "path_nc = cfg.PATH_TS / 'netcdf'\n",
    "path_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for gdw_id in tqdm(attributes.index, desc='Exporting time series'):    \n",
    "\n",
    "    # concatenate time series\n",
    "    if gdw_id in obs.GDW_ID.data:\n",
    "        ds = obs.sel(GDW_ID=gdw_id).drop_vars(['GDW_ID'])\n",
    "    else:\n",
    "        print(f'Reservoir {gdw_id} does not have observations. Skipping to the next reservoir')\n",
    "        continue\n",
    "    # if gdw_id in sim.GDW_ID.data:\n",
    "    #     ds = xr.merge((ds, sim.sel(GDW_ID=gdw_id).drop_vars(['GDW_ID'])))\n",
    "    if gdw_id in meteo_areal.GDW_ID.data:\n",
    "        ds = xr.merge((ds, meteo_areal.sel(GDW_ID=gdw_id).drop_vars(['GDW_ID'])))\n",
    "    if gdw_id in meteo_point.GDW_ID.data:\n",
    "        ds = xr.merge((ds, meteo_point.sel(GDW_ID=gdw_id).drop_vars(['GDW_ID'])))\n",
    "        \n",
    "    # delete empty variables\n",
    "    for var in list(ds.data_vars):\n",
    "        if (ds[var].isnull().all()):\n",
    "            del ds[var]\n",
    "\n",
    "    # trim time series to the observed period\n",
    "    start, end = attributes.loc[gdw_id, ['TIME_SERIES_START', 'TIME_SERIES_END']].values\n",
    "    ds = ds.sel(date=slice(start, end))\n",
    "\n",
    "    # create time series of temporal attributes\n",
    "    ds['year'] = ds.date.dt.year\n",
    "    ds['month'] = ds.date.dt.month\n",
    "    ds['month_sin'], ds['month_cos'] = time_encoding(ds['month'], period=12)\n",
    "    ds['weekofyear'] = ds.date.dt.isocalendar().week\n",
    "    ds['woy_sin'], ds['woy_cos'] = time_encoding(ds['weekofyear'], period=52)\n",
    "    ds['dayofyear'] = ds.date.dt.dayofyear\n",
    "    ds['doy_sin'], ds['doy_cos'] = time_encoding(ds['dayofyear'], period=365)\n",
    "    ds['dayofweek'] = ds.date.dt.dayofweek\n",
    "    ds['dow_sin'], ds['dow_cos'] = time_encoding(ds['dayofweek'], period=6)\n",
    "        \n",
    "    # export CSV\n",
    "    # ..........\n",
    "    ds.to_pandas().to_csv(path_csv / f'{gdw_id}.csv')\n",
    "\n",
    "    # export NetCDF\n",
    "    # .............\n",
    "    ds.to_netcdf(path_nc / f'{gdw_id}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
