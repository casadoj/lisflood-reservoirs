{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# ResOpsBR: time series\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado<br>\n",
    "**Date:** 27-11-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code creates the time series for the reservoirs in ResOpsMX. The time series include records from CONAGUA and simulations from GloFAS. The result is a time series that combines the observed data from CONAGUA with the simulation from GloFASv4. For each reservoir, these time series are exported both in CSV and a NetCDF format.\n",
    "\n",
    "Records are cleaned to avoid errors:\n",
    "    * Outliers in the **storage** time series are filtered by comparison with a moving median (window 7 days). If the relative difference of a given storage value and the moving median exceeds a threshold, the value is removed. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_storage()`\n",
    "    * Outliers in the **inflow** time series are removed using two conditions: one based in the gradient, and the other using an estimated inflow based on the water balance. When both conditions are met, the value is removed. Since inflow time series cannot contain missing values when used in the reservoir simulation, a simple linear interpolation is used to fill in gaps up to 7 days. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_inflow()`.\n",
    "\n",
    "**To do:**<br>\n",
    "* [x] Plot time series\n",
    "* [x] Make sure that there aren't negative values in the time series, nor zeros in storage.\n",
    "* [ ] Check the quality of the data by closing the mass balance when possible. <font color='steelblue'>I've used the mass balance to identify errors in the inflow time series (function `clean_inflow`).</font>.\n",
    "* [ ] Fill in the inflow time series with the mass balance, if possible. <font color='steelblue'>I've filled in gaps in the inflow time series with linear interpolation up to 7-day gaps (function `clean_inflow`).</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f56e74-ae31-4b60-be69-fc127719c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from lisfloodreservoirs.utils import DatasetConfig\n",
    "from lisfloodreservoirs import read_attributes\n",
    "from lisfloodreservoirs.utils.plots import plot_resops, reservoir_analysis, compare_flows\n",
    "from lisfloodreservoirs.utils.timeseries import clean_storage, clean_inflow, time_encoding, quantile_mapping\n",
    "\n",
    "from utils_br import plot_timeseries_BR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9034a7b1-74d9-47ea-8457-7f748a3eb4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsBR\\v1.0\\time_series\n"
     ]
    }
   ],
   "source": [
    "cfg = DatasetConfig('config_dataset.yml')\n",
    "\n",
    "print(f'Time series will be saved in {cfg.PATH_TS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbf891-79d3-4317-9964-b99f1df92649",
   "metadata": {},
   "source": [
    "### Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614adf34-bc72-4f25-959b-5dab55da8842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 reservoirs in the attribute tables\n"
     ]
    }
   ],
   "source": [
    "# import all tables of attributes\n",
    "attributes = read_attributes(cfg.PATH_ATTRS)\n",
    "map_ana_grand = {sar_id: grand_id for grand_id, sar_id in attributes['SAR_ID'].iteritems()}\n",
    "print(f'{attributes.shape[0]} reservoirs in the attribute tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228fdc-7588-4e0f-ad5e-25b95df3412a",
   "metadata": {},
   "source": [
    "### Time series\n",
    "#### ResOpsMX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0aa8f1-c265-4dea-8294-a7ca2224bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28739a6d4e904cb6a3f189b6751377cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series were imported for 59 reservoirs\n"
     ]
    }
   ],
   "source": [
    "# read time series\n",
    "timeseries = {}\n",
    "for sar_id, grand_id in tqdm(map_ana_grand.items()):\n",
    "    file = cfg.PATH_OBS_TS / f'{sar_id}.csv'\n",
    "    if file.is_file():\n",
    "\n",
    "        ts = pd.read_csv(file, parse_dates=['date'], index_col='date')\n",
    "        ts.volume_pct /= 100\n",
    "        ts['volume_mcm'] = ts.volume_pct / 100 * attributes.loc[grand_id, 'CAP_MCM']\n",
    "        # make sure there aren't gaps in the dates\n",
    "        dates = pd.date_range(ts.index.min(), ts.index.max(), freq='D')\n",
    "        if len(dates) > ts.shape[0]:\n",
    "            ts = ts.reindex(dates)\n",
    "            ts.index.name = 'date'\n",
    "\n",
    "        # rename columns\n",
    "        rename_cols = {\n",
    "            'volume_mcm': 'storage',\n",
    "            'level_m': 'elevation',\n",
    "            'inflow_cms': 'inflow',\n",
    "            'outflow_cms': 'outflow'\n",
    "        }\n",
    "        ts.rename(columns=rename_cols, inplace=True)\n",
    "        \n",
    "        # remove negative values\n",
    "        ts[ts < 0] = np.nan\n",
    "        # clean outliers in storage\n",
    "        clean_storage(ts.storage, w=7, error_thr=.2, inplace=True)\n",
    "        \n",
    "        # trim time series to period with storage and outflow\n",
    "        mask_availability = ts[['inflow', 'storage', 'outflow']].notnull().all(axis=1)\n",
    "        if mask_availability.sum() == 0:\n",
    "            continue\n",
    "        start, end = ts[mask_availability].first_valid_index(), ts[mask_availability].last_valid_index()\n",
    "        start, end = max(cfg.START, start), min(cfg.END, end)\n",
    "        attributes.loc[grand_id, ['TIME_SERIES_START', 'TIME_SERIES_END']] = start, end\n",
    "        ts = ts.loc[start:end]\n",
    "        \n",
    "        # save\n",
    "        timeseries[grand_id] = ts.loc[start:end]\n",
    "    else:\n",
    "        print(f'File not found: {file}')\n",
    "print(f'Time series were imported for {len(timeseries)} reservoirs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6526d61-6aab-4696-80ee-7fb74548378b",
   "metadata": {},
   "source": [
    "##### **Plot timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cea0f8a-e8a4-4fb7-8bae-82eac2c9890e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df39c8c9bed947eba964e2fce1c5ab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH_PLOTS = cfg.PATH_TS / 'plots'\n",
    "PATH_PLOTS.mkdir(exist_ok=True)\n",
    "\n",
    "for grand_id, ts in tqdm(timeseries.items()):\n",
    "    max_storage = {\n",
    "        'GRanD': attributes.loc[grand_id, 'CAP_MCM'],\n",
    "        # 'BR': \n",
    "    }\n",
    "    max_elevation = {\n",
    "        'GRanD': attributes.loc[grand_id, 'ELEV_MASL'],\n",
    "        # 'BR': \n",
    "    }\n",
    "    title = '{0} - {1}'.format(grand_id, attributes.loc[grand_id, 'DAM_NAME'])\n",
    "    plot_timeseries_BR(\n",
    "        ts.storage,\n",
    "        ts.elevation,\n",
    "        ts[['outflow_turbine_cms', 'outflow_spillway_cms']],\n",
    "        max_storage,\n",
    "        max_elevation,\n",
    "        # zlim=(attributes.loc[grand_id, 'NAME_MASL'] - attributes.loc[grand_id, 'DAM_HGT_M'] * 1.2, None),\n",
    "        title=title,\n",
    "        save=PATH_PLOTS / f'{grand_id}.jpg'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560a3940-2c9d-4550-8e9a-bfb4e4b61038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grand_id = 1363 #1349 #1347 #1333\n",
    "# ts = timeseries[grand_id]\n",
    "\n",
    "# plot_resops(ts.storage, ts.elevation, outflow=ts.outflow,\n",
    "#             capacity=attributes.loc[grand_id, ['NAME_MCM', 'NAMO_MCM']],\n",
    "#             level=attributes.loc[grand_id, ['NAME_MASL', 'NAMO_MASL']])\n",
    "\n",
    "# plot_resops(ts.storage, ts.area, outflow=ts.outflow,\n",
    "#             capacity=attributes.loc[grand_id, ['NAME_MCM', 'NAMO_MCM']],\n",
    "#             # level=attributes.loc[grand_id, ['NAME_MASL', 'NAMO_MASL']]\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c53eb1d-d35c-4c92-aedd-79f966374e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to xarray.Dataset\n",
    "xarray_list = []\n",
    "for key, df in timeseries.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(GRAND_ID=key)\n",
    "    xarray_list.append(ds)\n",
    "obs = xr.concat(xarray_list, dim='GRAND_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "#### GloFAS\n",
    "\n",
    "##### Inflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5754f3-0640-45fe-aa76-926e74833d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GloFAS simulation\n",
    "sim = xr.open_dataset(cfg.PATH_SIM_TS / 'inflow.nc')\n",
    "sim = sim.rename({'time': 'date', 'ID': 'GRAND_ID', 'dis': 'inflow'})\n",
    "\n",
    "# bias correct\n",
    "for grand_id in sim.GRAND_ID.data:\n",
    "    \n",
    "    inflow = sim['inflow'].sel(GRAND_ID=grand_id).to_pandas()\n",
    "    inflow.name = 'inflow'\n",
    "    ts = timeseries[grand_id]\n",
    "    \n",
    "    # compute net inflow\n",
    "    if ('outflow' in ts.columns) & ('storage' in ts.columns):\n",
    "        ΔS = ts.storage.diff().values\n",
    "        net_inflow = ΔS * 1e6 / (24 * 3600) + ts.outflow\n",
    "        net_inflow[net_inflow < 0] = 0\n",
    "        net_inflow.name = 'net_inflow'\n",
    "\n",
    "    # bias correct simulated inflow\n",
    "    inflow_bc = quantile_mapping(obs=net_inflow,\n",
    "                                 sim=inflow)\n",
    "    inflow_bc.name = 'inflow_bc'\n",
    "    \n",
    "    # # plot raw vs bias-corrected inflow\n",
    "    # compare_flows(ts.storage, ts.outflow, inflow, inflow_bc)\n",
    "    \n",
    "    # overwrite bias-corrected inflow\n",
    "    sim['inflow'].loc[{'GRAND_ID': grand_id}] = inflow_bc.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a369b5f-6272-40a9-b989-d5e913473850",
   "metadata": {},
   "source": [
    "##### Meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455eee68-6de0-4fd0-b7e9-0df260597ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_areal = cfg.PATH_RESOPS / 'ancillary' / 'catchstats'\n",
    "variables = [x.stem for x in path_meteo_areal.iterdir() if x.is_dir()]\n",
    "meteo_areal = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_areal}/{var}/*.nc')[f'{var}_mean'].compute() for var in variables})\n",
    "meteo_areal['time'] = meteo_areal['time'] - np.timedelta64(24, 'h') # WARNING!! One day lag compared with LISFLOOD\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_areal.id.data))\n",
    "meteo_areal = meteo_areal.sel(id=IDs)\n",
    "\n",
    "# rename 'id' with the GRanD ID\n",
    "meteo_areal = meteo_areal.rename({\n",
    "    'id': 'GRAND_ID',\n",
    "    'time': 'date',\n",
    "    'e0': 'evapo_areal',\n",
    "    'tp': 'precip_areal',\n",
    "    '2t': 'temp_areal'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5c32d-83b3-4747-a327-bdbd20417914",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "### Convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2303dbdc-84b2-4b23-92c5-8a1d674691d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.NORMALIZE:\n",
    "\n",
    "    # reservoir attributes used to normalize the dataset\n",
    "    area_sm = xr.DataArray.from_series(attributes.AREA_SKM) * 1e6 # m2\n",
    "    capacity_cm = xr.DataArray.from_series(attributes.CAP_MCM) * 1e6 # m3\n",
    "    catchment_sm = xr.DataArray.from_series(attributes.CATCH_SKM) * 1e6 # m2\n",
    "    \n",
    "    # Observed timeseries\n",
    "    # -------------------\n",
    "    for var, da in obs.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var in ['storage', 'evaporation']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var in ['inflow', 'outflow']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 24 * 3600 / capacity_cm\n",
    "\n",
    "    # Simulated timeseries\n",
    "    # -------------------\n",
    "    for var, da in sim.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var.split('_')[0] in ['storage']:\n",
    "            sim[f'{var}_norm'] = sim[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var.split('_')[0] in ['inflow', 'outflow']:\n",
    "            sim[f'{var}_norm'] = sim[var] * 24 * 3600 / capacity_cm\n",
    "            \n",
    "    # Catchment meteorology\n",
    "    # ---------------------\n",
    "    # convert areal evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_areal[f'{var}_areal_norm'] = meteo_areal[f'{var}_areal'] * catchment_sm * 1e-3 / capacity_cm       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed75f2-62e4-442e-b897-52dee820581e",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be87f7ec-3b05-4d48-978f-a3bb8ad140a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaadd0a2f884b938111d2ce774b8daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting time series:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_csv = cfg.PATH_TS / 'csv'\n",
    "path_csv.mkdir(parents=True, exist_ok=True)\n",
    "path_nc = cfg.PATH_TS / 'netcdf'\n",
    "path_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for grand_id in tqdm(attributes.index, desc='Exporting time series'):\n",
    "\n",
    "    # concatenate time series\n",
    "    ds = obs.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])\n",
    "    if grand_id in sim.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, sim.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])))\n",
    "    if grand_id in meteo_areal.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, meteo_areal.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])))\n",
    "\n",
    "    # # delete empty variables\n",
    "    # for var in list(ds.data_vars):\n",
    "    #     if (ds[var].isnull().all()):\n",
    "    #         del ds[var]\n",
    "        \n",
    "    # trim time series to the observed period\n",
    "    start, end = attributes.loc[grand_id, ['TIME_SERIES_START', 'TIME_SERIES_END']]\n",
    "    ds = ds.sel(date=slice(start, end))\n",
    "\n",
    "    # create time series of temporal attributes\n",
    "    ds['year'] = ds.date.dt.year\n",
    "    ds['month'] = ds.date.dt.month\n",
    "    ds['month_sin'], ds['month_cos'] = time_encoding(ds['month'], period=12)\n",
    "    ds['weekofyear'] = ds.date.dt.isocalendar().week\n",
    "    ds['woy_sin'], ds['woy_cos'] = time_encoding(ds['weekofyear'], period=52)\n",
    "    ds['dayofyear'] = ds.date.dt.dayofyear\n",
    "    ds['doy_sin'], ds['doy_cos'] = time_encoding(ds['dayofyear'], period=365)\n",
    "    ds['dayofweek'] = ds.date.dt.dayofweek\n",
    "    ds['dow_sin'], ds['dow_cos'] = time_encoding(ds['dayofweek'], period=6)\n",
    "\n",
    "    # export CSV\n",
    "    # ..........\n",
    "    ds.to_pandas().to_csv(path_csv / f'{grand_id}.csv')\n",
    "\n",
    "    # export NetCDF\n",
    "    # .............\n",
    "    ds.to_netcdf(path_nc / f'{grand_id}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff77ab1-ae42-4b7f-8520-bdfcc037d90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
