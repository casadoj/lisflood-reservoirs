{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# Create dataset - attributes\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado Rodr√≠guez<br>\n",
    "**Date:** 06-09-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code creates the basic table of static attributes for the reservoirs in ResOpsUS. The attributes will be taken from GRanD, but some information will be added from both ResOpsUS and GloFAS.\n",
    "\n",
    "The resulting files are three:\n",
    "\n",
    "* *glofas.csv* contains the storage and outflow limits used in the GloFASv4 simulations, and the two calibrated reservoir parameters.\n",
    "* *grand.csv* contains the reservoir and dam characteristics extracted from GRanD, which will be later on used as the main static inputs in the modelling, and used to normalize the time series.\n",
    "* *resops.csv* defines the variables and time span of the observed time series in ResOpsUS.\n",
    "\n",
    "**To do:**<br>\n",
    "* [ ] Correct reservoir storage capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8ce8f7-4128-4051-9ae2-c59d10b8c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely import Point\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import spotpy\n",
    "# from spotpy.objectivefunctions import kge\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from lisfloodreservoirs.utils import DatasetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bd373b-4aaa-4fe9-873a-14564ab672ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute tables will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\v2.0\\attributes\n"
     ]
    }
   ],
   "source": [
    "cfg = DatasetConfig('config_dataset.yml')\n",
    "\n",
    "print(f'Attribute tables will be saved in {cfg.PATH_ATTRS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228fdc-7588-4e0f-ad5e-25b95df3412a",
   "metadata": {},
   "source": [
    "### ResOpsUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1941e6-ad7b-4423-82a0-58d9fa7ed4fa",
   "metadata": {},
   "source": [
    "#### Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4c4868-497f-4e40-a8cc-90dc7094f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ResOpsUS attributes includes 678 reservoirs\n",
      "The ResOpsUS time series inventory includes 677 reservoirs\n"
     ]
    }
   ],
   "source": [
    "# load reservoir attributes\n",
    "attributes = pd.read_csv(cfg.PATH_RESOPS / 'raw' / 'attributes' / 'reservoir_attributes.csv', index_col='DAM_ID')\n",
    "attributes = attributes[~attributes.index.duplicated(keep='first')]\n",
    "attributes.index.name = 'GRAND_ID'\n",
    "print(f'The ResOpsUS attributes includes {attributes.shape[0]} reservoirs')\n",
    "\n",
    "# load time series recorded for each reservoir\n",
    "inventory = pd.read_csv(cfg.PATH_RESOPS / 'raw' / 'attributes' / 'time_series_inventory.csv', index_col='DAM_ID')\n",
    "inventory = inventory[~inventory.index.duplicated(keep='first')]\n",
    "print(f'The ResOpsUS time series inventory includes {inventory.shape[0]} reservoirs')\n",
    "\n",
    "# merge attributes and inventory and convert into geopandas\n",
    "resops = pd.merge(attributes, inventory, left_index=True, right_index=True)\n",
    "resops = gpd.GeoDataFrame(resops,\n",
    "                          geometry=[Point(xy) for xy in zip(resops.LONG, resops.LAT)])\n",
    "resops.crs = 'EPSG:4326'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de8865-36a2-47fc-82e5-8d724c0e8c27",
   "metadata": {},
   "source": [
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1403acc8-32da-4207-af38-7c39acade638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5a02d7c3314005b43c06872038ac18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResOpsUS contains records for 670 reservoirs in the period from 1982-01-01 to 2019-12-30\n"
     ]
    }
   ],
   "source": [
    "variables = ['STORAGE', 'INFLOW', 'OUTFLOW', 'ELEVATION', 'EVAPORATION']\n",
    "\n",
    "resops[variables] = 0\n",
    "resops['CAP_RESOPS'] = np.nan\n",
    "for ID in tqdm(resops.index): # ID refers to GRanD\n",
    "    # load timeseries\n",
    "    file = cfg.PATH_OBS_TS / f'ResOpsUS_{ID}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, index_col='date')\n",
    "        series.columns = series.columns.str.upper()\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "    # trim to GloFAS long run period\n",
    "    series = series.loc[cfg.START:cfg.END]\n",
    "    # remove duplicated index\n",
    "    series = series[~series.index.duplicated(keep='first')]\n",
    "    # remove empty series\n",
    "    series.dropna(axis=1, how='all', inplace=True)\n",
    "    resops.loc[ID, series.columns] = 1\n",
    "    # identify available variables and the beginning and end of the time series\n",
    "    resops.loc[ID, ['TIME_SERIES_START', 'TIME_SERIES_END']] = series.first_valid_index(), series.last_valid_index()\n",
    "    for var in series.columns:\n",
    "        resops.loc[ID, [f'{var}_START', f'{var}_END']] = series[var].first_valid_index(), series[var].last_valid_index()\n",
    "    # maximum recorded storage\n",
    "    if 'STORAGE' in series.columns:\n",
    "        resops.loc[ID, 'CAP_RESOPS'] = series.STORAGE.max()\n",
    "    \n",
    "# remove reservoirs with no records\n",
    "mask = resops[variables].sum(axis=1) > 0\n",
    "resops = resops.loc[mask]\n",
    "\n",
    "print(f'ResOpsUS contains records for {resops.shape[0]} reservoirs in the period from {cfg.START} to {cfg.END}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc53c8d-c94a-41f9-b46b-e7d346112150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simplify column names\n",
    "# resops.rename(columns={'AGENCY_CODE': 'AGENCY_COD',\n",
    "#          'TIME_SERIES_START': 'TS_START',\n",
    "#          'TIME_SERIES_END': 'TS_END',\n",
    "#          'INCONSISTENCIES_NOTED': 'NOTES',\n",
    "#          'STORAGE_START': 'STO_START',\n",
    "#          'STORAGE_END': 'STO_END',\n",
    "#          'DATA_SOURCE': 'STO_SOURCE',\n",
    "#          'INFLOW_START': 'IN_START',\n",
    "#          'INFLOW_END': 'IN_END',\n",
    "#          'DATA_SOURCE.1': 'IN_SOURCE',\n",
    "#          'OUTFLOW_START': 'OUT_START',\n",
    "#          'OUTFLOW_END': 'OUT_END',\n",
    "#          'DATA_SOURCE.2': 'OUT_SOURCE', \n",
    "#          'ELEVATION_START': 'ELE_START', \n",
    "#          'ELEVATION_END': 'ELE_END',\n",
    "#          'DATA_SOURCE.3': 'ELE_SOURCE',\n",
    "#          'EVAPORATION': 'EVAPORA',\n",
    "#          'EVAPORATION_START': 'EVA_START', \n",
    "#          'EVAPORATION_END': 'EVA_END',\n",
    "#          'DATA_SOURCE.4': 'EVA_SOURCE'},\n",
    "#                  inplace=True)\n",
    "\n",
    "# # export as shapefile\n",
    "# path_GIS = cfg.PATH_RESOPS / 'GIS'\n",
    "# if path_GIS.exists() is False:\n",
    "#     path_GIS.mkdir()\n",
    "# resops.to_file(path_GIS / 'reservoirs.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "### GloFAS\n",
    "\n",
    "#### Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea350a07-53f5-44b7-89fb-fe4a30452b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloFASv4 contains 685 reservoirs worldwide\n",
      "655 of those reservoirs have a GRAND_ID assigned\n",
      "121 of those reservoirs are in ResOpsUS\n"
     ]
    }
   ],
   "source": [
    "# load shapefile of GloFAS reservoirs\n",
    "glofas = gpd.read_file(cfg.PATH_LISFLOOD / 'tables' / 'GloFAS_reservoirs.shp')\n",
    "glofas.rename(columns={'stor': 'CAP_GLWD', 'ResID': 'GLOFAS_ID'}, inplace=True)\n",
    "print(f'GloFASv4 contains {glofas.shape[0]} reservoirs worldwide')\n",
    "\n",
    "# remove those without GRAND_ID\n",
    "glofas = glofas.loc[~glofas.GRAND_ID.isnull()]\n",
    "glofas.GRAND_ID = glofas.GRAND_ID.astype(int)\n",
    "glofas.set_index('GRAND_ID', drop=False, inplace=True)\n",
    "glofas.sort_index(axis=0, inplace=True)\n",
    "\n",
    "print(f'{glofas.shape[0]} of those reservoirs have a GRAND_ID assigned')\n",
    "\n",
    "# keep only reservoirs in ResOps\n",
    "mask_resops = glofas.index.intersection(resops.index)\n",
    "glofas = glofas.loc[mask_resops]\n",
    "glofas.index.name = 'GRAND_ID'\n",
    "print(f'{glofas.shape[0]} of those reservoirs are in ResOpsUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d6afd-6e31-47f1-a729-29f032b3c7be",
   "metadata": {},
   "source": [
    "#### Reservoir model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ad0685-0ae2-46df-9ff1-7b1c6a6ff6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c8cbada5cc4b8c96824a8896369f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = ['adjustNormalFlood', 'ReservoirRnormqMult']\n",
    "\n",
    "# load static map of reservoirs\n",
    "glofas_raster = xr.open_mfdataset((cfg.PATH_LISFLOOD / 'static_maps').glob('*reservoirs*.nc'))['res'].compute()\n",
    "\n",
    "# extract an array of reservoir ID\n",
    "ids = glofas.GLOFAS_ID.to_list()\n",
    "ids.sort()\n",
    "\n",
    "# xr.DataArrays of reservoir longitudes and latitudes\n",
    "lon = xr.DataArray(np.nan, dims=['ResID'], coords={'ResID': ids})\n",
    "lat = xr.DataArray(np.nan, dims=['ResID'], coords={'ResID': ids})\n",
    "for ID in tqdm(ids):\n",
    "    cell = glofas_raster.where(glofas_raster == ID, drop=True)\n",
    "    lon.loc[dict(ResID=ID)] = cell.lon.data[0]\n",
    "    lat.loc[dict(ResID=ID)] = cell.lat.data[0]\n",
    "coords = xr.Dataset({'lon': lon, 'lat': lat})\n",
    "\n",
    "# extract parameter values\n",
    "map_glofas_id = {glofas_id: grand_id for grand_id, glofas_id in glofas.GLOFAS_ID.iteritems()}\n",
    "for par in parameters:\n",
    "    # load parameter map\n",
    "    da = xr.open_mfdataset((cfg.PATH_LISFLOOD / 'parameters').glob(f'{par}*.nc'))[par].compute()\n",
    "    da = da.where(da != -9999, np.nan,)\n",
    "    # extract values for each reservoir\n",
    "    df = da.sel(lon=lon, lat=lat, method='nearest').drop(['lon', 'lat']).to_pandas()\n",
    "    df.rename(index=map_glofas_id, inplace=True)\n",
    "    glofas[par] = df\n",
    "\n",
    "# adjust normal limit and outflow with the calibrated parameters\n",
    "glofas['nlim_adj'] = glofas.nlim + glofas.adjustNormalFlood * (glofas.flim - glofas.nlim)\n",
    "glofas['normq_adj'] = pd.concat((glofas.normq * glofas.ReservoirRnormqMult, glofas.minq + .01), axis=1).max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc941d-c9ee-47d8-886c-94e53685ed2f",
   "metadata": {},
   "source": [
    "### GRanD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8eb624-f04c-48f8-8a14-d5c40e48d5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraND contains 7320 reservoirs worldwide\n",
      "670 reservoirs are both in ResOpsUS and GRanD worldwide\n"
     ]
    }
   ],
   "source": [
    "# load GRanD data set\n",
    "grand = gpd.read_file(cfg.PATH_GRAND / 'grand_dams_v1_3.shp')\n",
    "grand.set_index('GRAND_ID', drop=True, inplace=True)\n",
    "grand = grand.replace(-99, np.nan)\n",
    "print(f'GraND contains {grand.shape[0]} reservoirs worldwide')\n",
    "\n",
    "# filter reservoirs present in ResOpsUS\n",
    "grand = grand.loc[resops.index]\n",
    "print(f'{grand.shape[0]} reservoirs are both in ResOpsUS and GRanD worldwide')\n",
    "\n",
    "# define single use reservoirs\n",
    "uses = [col for col in grand.columns if col.startswith('USE')]\n",
    "mask_single_use = (~grand[uses].isnull()).sum(axis=1) == 1\n",
    "grand['SINGLE_USE'] = 0\n",
    "grand.loc[mask_single_use, 'SINGLE_USE'] = 1\n",
    "\n",
    "# convert feature or individual uses into a ordinal encoder\n",
    "grand[uses] = grand[uses].replace({None: 'None',\n",
    "                                   'Maj': 'Major'})\n",
    "grand[uses] = OrdinalEncoder(categories=[['None', 'Sec', 'Major', 'Main']] * len(uses)).fit_transform(grand[uses])\n",
    "\n",
    "# one hot encoder of the main use\n",
    "main_use = pd.DataFrame(grand.pop('MAIN_USE'))\n",
    "main_use.columns = ['MAIN']\n",
    "map_uses = {'Fisheries': 'FISH',\n",
    "            'Flood control': 'FCON',\n",
    "            'Hydroelectricity': 'ELEC',\n",
    "            'Irrigation': 'IRRI',\n",
    "            'Livestock': 'LIVE',\n",
    "            'Navigation': 'NAVI',\n",
    "            'Other': 'OTHR', \n",
    "            'Pollution control': 'PCON',\n",
    "            'Recreation': 'RECR',\n",
    "            'Water supply': 'SUPP',\n",
    "            None: 'NONE'}\n",
    "main_use.replace(map_uses, inplace=True)\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "main_use_ohe = pd.DataFrame(ohe.fit_transform(main_use),\n",
    "                            index=main_use.index,\n",
    "                            columns=ohe.get_feature_names_out(['MAIN']))\n",
    "main_use_ohe.drop('MAIN_NONE', axis=1, inplace=True)\n",
    "grand = pd.concat((grand, main_use_ohe), axis=1)\n",
    "grand.index.name = 'GRAND_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f06c2-69cc-4106-8a06-b313316ac490",
   "metadata": {},
   "source": [
    "## Static attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47920d-e947-4f12-b926-12c1196aebfd",
   "metadata": {},
   "source": [
    "### Correct reservoir capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c0edda-16f2-4246-a7c1-71c8c811fe93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = Path('fraction_fill.xlsx')\n",
    "if file.is_file():\n",
    "    # import DataFrame with the fraction fill and the selected data source\n",
    "    ff = pd.read_excel(file, index_col='GRAND_ID')\n",
    "else:\n",
    "    # create DataFrame with the fraction fill according to each data source\n",
    "    ff_ = pd.DataFrame(columns=['ResID', 'GLOFAS', 'GRAND'], dtype=float)\n",
    "    ff_.index.name = 'GRAND_ID'\n",
    "    for ID in grand.index:\n",
    "        cap_resops = resops.loc[ID, 'CAP_RESOPS']\n",
    "        if np.isnan(cap_resops):\n",
    "            continue\n",
    "        if ID in glofas.index:\n",
    "            ff_.loc[ID, 'ResID'] = glofas.loc[ID, 'GLOFAS_ID']\n",
    "            ff_.loc[ID, 'GLOFAS'] = cap_resops / glofas.loc[ID, 'CAP_GLWD']\n",
    "        ff_.loc[ID, 'GRAND'] = cap_resops / grand.loc[ID, 'CAP_MCM']\n",
    "\n",
    "    # export\n",
    "    ff.to_excel(file, index=True)\n",
    "\n",
    "# define the capacity  ('CAP') as that of the most reliable source\n",
    "glofas['CAP'] = np.nan\n",
    "for ID in ff.index:\n",
    "    if ff.loc[ID, 'selection'] == 'GLOFAS':\n",
    "        glofas.loc[ID, 'CAP'] = glofas.loc[ID, 'CAP_GLWD']\n",
    "    elif ff.loc[ID, 'selection'] == 'GRAND':\n",
    "        glofas.loc[ID, 'CAP'] = grand.loc[ID, 'CAP_MCM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886cfa9-8b0d-4a0a-a2cd-469d1e0e19a3",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f60603-4e84-408c-8a72-0687110f361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535 reservoirs exceed the minimum catchment area of 250 km¬≤\n",
      "669 reservoirs exceed the minimum reservoir storage of 10 hm3\n",
      "660 reservoirs exceed the minimum time series lenght of 4 years\n",
      "528 reservoirs fulfil all the conditions above\n"
     ]
    }
   ],
   "source": [
    "mask_area = grand.CATCH_SKM >= cfg.MIN_AREA\n",
    "mask_vol = grand.CAP_MCM >= cfg.MIN_VOL\n",
    "len_ts = (resops.TIME_SERIES_END - resops.TIME_SERIES_START) / np.timedelta64(1, 'D')\n",
    "mask_ts = len_ts >= cfg.MIN_YEARS * 365\n",
    "mask = mask_area & mask_vol & mask_ts\n",
    "\n",
    "print(f'{mask_area.sum()} reservoirs exceed the minimum catchment area of {cfg.MIN_AREA} km¬≤')\n",
    "print(f'{mask_vol.sum()} reservoirs exceed the minimum reservoir storage of {cfg.MIN_VOL} hm3')\n",
    "print(f'{mask_ts.sum()} reservoirs exceed the minimum time series lenght of {cfg.MIN_YEARS} years')\n",
    "print(f'{mask.sum()} reservoirs fulfil all the conditions above')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f038211b-3709-4fe3-b1cb-ad7282889101",
   "metadata": {},
   "source": [
    "### GRanD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd1a8f1-1fef-4dc7-977d-1509ca07be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_grand = ['RES_NAME', 'DAM_NAME', 'ALT_NAME', 'RIVER', 'ALT_RIVER', 'MAIN_BASIN', 'SUB_BASIN', 'NEAR_CITY', 'ALT_CITY', 'ADMIN_UNIT', 'SEC_ADMIN',\n",
    "               'COUNTRY', 'SEC_CNTRY', 'YEAR', 'ALT_YEAR', 'REM_YEAR', 'DAM_HGT_M', 'ALT_HGT_M', 'DAM_LEN_M', 'ALT_LEN_M', 'AREA_SKM', 'CAP_MCM',\n",
    "               'DEPTH_M', 'DIS_AVG_LS', 'DOR_PC', 'ELEV_MASL', 'CATCH_SKM', 'DATA_INFO', 'USE_IRRI', 'USE_ELEC', 'USE_SUPP', 'USE_FCON', 'USE_RECR',\n",
    "               'USE_NAVI', 'USE_FISH', 'USE_PCON', 'USE_LIVE', 'USE_OTHR', 'SINGLE_USE', 'MAIN_ELEC', 'MAIN_FCON', 'MAIN_FISH', 'MAIN_IRRI',\n",
    "               'MAIN_NAVI', 'MAIN_OTHR', 'MAIN_RECR', 'MAIN_SUPP', 'LAKE_CTRL', 'MULTI_DAMS', 'TIMELINE', 'COMMENTS', 'QUALITY', 'LONG_DD', 'LAT_DD']\n",
    "grand_export = grand.loc[mask, attrs_grand].copy()\n",
    "grand_export.index.name = 'GRAND_ID'\n",
    "grand_export.sort_index(axis=0, inplace=True)\n",
    "grand_export.sort_index(axis=1, inplace=True)\n",
    "grand_export.rename(columns={'LONG_DD': 'LON',\n",
    "                             'LAT_DD': 'LAT'},\n",
    "                    inplace=True)\n",
    "grand_export.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# export\n",
    "grand_export.to_csv(cfg.PATH_ATTRS / 'grand.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ecba4-5481-43c3-8a09-3959bc11105e",
   "metadata": {},
   "source": [
    "### ResOpsUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21be9da6-6351-498a-ba17-e7659d325624",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_resops = ['CAP_RESOPS', 'ELEVATION', 'ELEVATION_END', 'ELEVATION_START',\n",
    "                'EVAPORATION', 'EVAPORATION_END', 'EVAPORATION_START',\n",
    "                'INCONSISTENCIES_NOTED', 'INFLOW', 'INFLOW_END', 'INFLOW_START',\n",
    "                'OUTFLOW', 'OUTFLOW_END', 'OUTFLOW_START', 'STATE',\n",
    "                'STORAGE', 'STORAGE_END', 'STORAGE_START',\n",
    "                'TIME_SERIES_END', 'TIME_SERIES_START']\n",
    "resops_export = resops.loc[mask, attrs_resops].copy()\n",
    "resops_export.index.name = 'GRAND_ID'\n",
    "resops_export.sort_index(axis=0, inplace=True)\n",
    "resops_export.sort_index(axis=1, inplace=True)\n",
    "resops_export.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# export\n",
    "resops_export.to_csv(cfg.PATH_ATTRS / 'resops.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e48bf-295c-4d2b-b784-60bcba51a0eb",
   "metadata": {},
   "source": [
    "### GloFAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b2d04f0-8811-40da-9d55-ce75c01d0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_glofas = ['CAP', 'CAP_GLWD', 'GLOFAS_ID', 'GLWD_ID', 'LisfloodX3', 'LisfloodY3',\n",
    "                'ReservoirRnormqMult', 'adjustNormalFlood', 'clim', 'flim', 'minq',\n",
    "                'ndq', 'nlim', 'nlim_adj', 'normq', 'normq_adj']\n",
    "glofas_export = glofas.loc[mask, attrs_glofas].copy()\n",
    "glofas_export.index.name = 'GRAND_ID'\n",
    "glofas_export.dropna(axis=1, how='all', inplace=True)\n",
    "glofas_export.rename(columns={'LisfloodX3': 'LON_LISFLOOD',\n",
    "                              'LisfloodY3': 'LAT_LISFLOOD',\n",
    "                              'clim': 'Vmin',\n",
    "                              'flim': 'Vf',\n",
    "                              'minq': 'Qmin',\n",
    "                              'ndq': 'Qf',\n",
    "                              'nlim': 'Vn',\n",
    "                              'nlim_adj': 'Vn_adj',\n",
    "                              'normq': 'Qn',\n",
    "                              'normq_adj': 'Qn_adj'},\n",
    "                     inplace=True)\n",
    "glofas_export.sort_index(axis=0, inplace=True)\n",
    "glofas_export.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# export\n",
    "glofas_export.to_csv(cfg.PATH_ATTRS / 'glofas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50117b9-c32b-4eb1-b925-d1fe5e1cb5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
