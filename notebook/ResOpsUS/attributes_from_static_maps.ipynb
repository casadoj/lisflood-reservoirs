{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455af9d0-e8c9-4935-a28a-53e7320ecba8",
   "metadata": {},
   "source": [
    "# GloFAS - Attributes from static maps and model parameters\n",
    "***\n",
    "\n",
    "**_Autor:_** Chus Casado Rodríguez<br>\n",
    "**_Fecha:_** 22-05-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This notebook creates the static attributes for the GloFAS headwater catchments use to train the LISFLOOD-OS surrogate model.\n",
    "\n",
    "**To do:**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a357d889-e277-4375-8b81-ea11583ee88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# import rioxarray\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.features import shapes\n",
    "from typing import Union, List, Dict, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45dd8f5d-d94f-4a33-9b13-67bfdf88c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../lisflood-utilities/src')\n",
    "from lisfloodutilities.catchstats import catchment_statistics\n",
    "sys.path.append('../../../lisflood-reservoirs/notebook/')\n",
    "from ResOpsES.utils import plot_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29654c04-471a-4957-98ad-206bc79abef1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa20b61-aa9c-4921-9c17-fcc21072043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "with open('attributes.yml') as config_file:\n",
    "    cfg = yaml.safe_load(config_file)\n",
    "\n",
    "# path to the dataset\n",
    "# PATH_DATASET = Path('Z:/nahaUsers/casadje/GloFASv4/LisfloodPL')\n",
    "PATH_DATASET = Path(cfg['catchments']['path'])\n",
    "\n",
    "# list of catchments\n",
    "CATCH_FILE = cfg['catchments']['IDs']\n",
    "\n",
    "# name of mask files\n",
    "# MASK_FILE = 'my_mask.nc'\n",
    "MASK_FILE = cfg['catchments']['mask_file']\n",
    "\n",
    "# path to the static maps\n",
    "# PATH_MAPS = Path('Z:/nahaUsers/grimast/GloFAS_3arcminmaps_JRC_catalogue_UPLOAD/v1.1.1/')\n",
    "PATH_MAPS = Path(cfg['static_maps']['path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3affc9-7c12-4217-8da4-8a7b22fc6c1e",
   "metadata": {},
   "source": [
    "## Base information\n",
    "\n",
    "### Catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a657e2-5f8b-4178-a5c9-0cb7d735cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load catchment IDs\n",
    "catchments = pd.read_csv(PATH_DATASET / CATCH_FILE, index_col='ID')#.squeeze()\n",
    "catchments[['lat', 'lon', 'CATCH_SKM']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1ecdab-005a-459a-b5f7-73caf0b1b327",
   "metadata": {},
   "source": [
    "###  Masks\n",
    "\n",
    "The coordinates of the _my_mask.nc_ maps and those of the GloFAS static maps do not match at the nth decimal, so I have to recreate the maps from the _upArea.nc_ static map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338d0dc-1f4e-4d21-9c38-ed7cfe7e622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the upstream area map\n",
    "upArea = xr.open_mfdataset(f'{PATH_MAPS}/Catchments_morphology_and_river_network/upArea*.nc')['Band1'].compute()\n",
    "\n",
    "# load the pixel area map\n",
    "pixarea = xr.open_mfdataset(f'{PATH_MAPS}/Main/pixarea*.nc')['Band1'].compute()\n",
    "\n",
    "# recreate masks and find outlet and catchment area\n",
    "masks = {}\n",
    "directories = [entry for entry in PATH_DATASET.iterdir() if entry.is_dir()]\n",
    "for directory in tqdm(directories, desc='loading masks'):\n",
    "    try:\n",
    "        # ID\n",
    "        ID = int(directory.stem)\n",
    "        if ID not in catchments.index:\n",
    "            print(f'{ID} not in the original list')\n",
    "            continue\n",
    "        \n",
    "        # load original mask\n",
    "        mask = xr.open_dataset(directory / MASK_FILE)['Band1']\n",
    "        # mask.name = str(ID)\n",
    "\n",
    "        # cut upArea map to the mask   \n",
    "        upArea_masked = upArea.sel(lon=mask.lon, lat=mask.lat, method='nearest', tolerance=1e-3)\n",
    "        mask['lon'] = upArea_masked.lon\n",
    "        mask['lat'] = upArea_masked.lat\n",
    "        upArea_masked = upArea_masked.where(mask == 1)        \n",
    "\n",
    "        # find outlet and catchment area\n",
    "        outlet = upArea_masked.isel(upArea_masked.argmax(dim=('lat', 'lon')))\n",
    "        catchments.loc[ID, ['lat', 'lon', 'CATCH_SKM']] = outlet.lat.data, outlet.lon.data, outlet.data\n",
    "\n",
    "        # create and save a mask out of the upArea map\n",
    "        mask = xr.where(upArea_masked.notnull(), 1, upArea_masked)\n",
    "        mask.name = str(ID)\n",
    "        masks[ID] = mask\n",
    "\n",
    "    except Exception as e: \n",
    "        print(directory, e)\n",
    "        continue\n",
    "        \n",
    "# series of catchment area in km²\n",
    "catchments.CATCH_SKM /= 1e6\n",
    "\n",
    "# create point geodataframe\n",
    "catchments = gpd.GeoDataFrame(catchments, geometry=[Point(xy) for xy in zip(catchments.lon, catchments.lat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f361d-ce0d-40c2-bb4b-70bb7aa56467",
   "metadata": {},
   "source": [
    "## LISFLOOD static maps\n",
    "\n",
    "In this section I will compute catchment statistics of the LISFLOOD static maps that will be in the end exported as _attributes_GLoFAS_static_maps.csv_. As ancillary maps, I have loaded first the pixel area and upstream area maps, that will be needed in the subsequent calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be878c30-80a7-4df7-a93c-2d415cdb7ecb",
   "metadata": {},
   "source": [
    "### Geomorphology\n",
    "\n",
    "Here I will compute catchment statistics for geomorphological attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bbbcf-95f0-4cfd-82e6-f1a888c15436",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Catchments_morphology_and_river_network'\n",
    "func = {'elv': ['mean', 'std', 'min', 'max'],\n",
    "        'gradient': ['mean', 'std'],\n",
    "        'upArea': ['max'],\n",
    "        # 'pixarea': ['sum']\n",
    "       }\n",
    "\n",
    "# load maps\n",
    "geomorphology = xr.Dataset({var: xr.open_mfdataset(f'{PATH_MAPS}/{category}/{var}_*.nc')['Band1'].compute() for var in func})\n",
    "\n",
    "# compute statistics\n",
    "statistic = list(np.unique([stat for stats in func.values() for stat in stats]))\n",
    "attr_geomorphology = catchment_statistics(geomorphology, masks, statistic=statistic, weight=pixarea).to_pandas()\n",
    "cols = [f'{var}_{stat}' for var, stats in func.items() for stat in stats]\n",
    "attr_geomorphology = attr_geomorphology[cols]\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_geomorphology,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=4,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_geomorphology.jpg')\n",
    "\n",
    "del geomorphology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911cbc6b-70cf-4121-bf54-6827dd9ff19c",
   "metadata": {},
   "source": [
    "### Land use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378ffe2-6b6f-452a-960f-939944b9b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Land_use'\n",
    "variables = ['fracforest', 'fracirrigated', 'fracother', 'fracrice', 'fracwater', 'fracsealed']\n",
    "variables.sort()\n",
    "\n",
    "# load maps\n",
    "land_use = xr.Dataset({var: xr.open_mfdataset(f'{PATH_MAPS}/{category}/{var}_*.nc')['Band1'].compute() for var in variables})\n",
    "land_use = land_use.rename({var: var[4:] for var in list(land_use)})\n",
    "\n",
    "# compute statistics\n",
    "attr_landuse = catchment_statistics(land_use, masks, statistic=['mean'], weight=pixarea).to_pandas()\n",
    "attr_landuse.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# compute main land use\n",
    "lu_classes = {col: i for i, col in enumerate(attr_landuse.columns, start=1)}\n",
    "attr_landuse['land_use_main'] = attr_landuse.idxmax(axis=1).map(lu_classes)\n",
    "\n",
    "# rename attributes\n",
    "attr_landuse.rename(columns={col: col.split('_')[0] if 'mean' in col else col for col in attr_landuse}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_landuse,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=4,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_land_use.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d73efe-2dbc-4314-b6a4-2206ea21f39d",
   "metadata": {},
   "source": [
    "### Crop coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5771ae7-2cae-47c8-84bf-eaf48ffe5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Vegetation_properties'\n",
    "\n",
    "# mapping land use acronym and name\n",
    "mapping_landuse = {'f': 'forest', 'i': 'irrigated', 'o': 'other'}\n",
    "variables = ['cropcoef_f', 'cropcoef_i', 'cropcoef_o']\n",
    "\n",
    "# load maps\n",
    "crops = xr.Dataset({var: xr.open_mfdataset(f'{PATH_MAPS}/{category}/{var}_*.nc')['Band1'].compute() for var in variables})\n",
    "crops = crops.rename({var: mapping_landuse[var.split('_')[1]] for var in list(crops)})\n",
    "\n",
    "# mean weighted by the fraction of pixel covered by each land use\n",
    "crops = crops.to_array('land_use').weighted(land_use.to_array('land_use').fillna(0)).sum('land_use', skipna=True) \n",
    "crops = crops.where(~upArea.isnull())\n",
    "crops.name = 'cropcoef'\n",
    "\n",
    "# compute statistics\n",
    "attr_crops = catchment_statistics(crops, masks, statistic=['mean', 'std'], weight=pixarea).to_pandas()\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_crops,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=2,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_crop_coefficient.jpg')\n",
    "\n",
    "del crops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20374d3-4761-4cd0-8f11-120803b157c8",
   "metadata": {},
   "source": [
    "### Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fe1a3-18b6-46f0-ae09-31dd1ef48c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Catchments_morphology_and_river_network'\n",
    "func = {'chanbnkf': ['mean'],\n",
    "        'chanbw': ['mean'],\n",
    "        'changrad': ['mean'],\n",
    "        'chanlength': ['sum'],\n",
    "        'chanman': ['mean']}\n",
    "\n",
    "# load maps\n",
    "streams = {var: xr.open_mfdataset(f'{PATH_MAPS}/{category}/{var}_*.nc')['Band1'].compute() for var in func}\n",
    "streams = {var: da.rename(var) for var, da in streams.items()}\n",
    "streams = {var : da.drop([coord for coord in list(da.coords) if coord not in ['lon', 'lat']]) for var, da in streams.items()}\n",
    "streams = xr.Dataset({var: xr.DataArray(da.data, coords=upArea.coords, name=var) for var, da in streams.items()})\n",
    "\n",
    "# mask streams (pixels with depth larger than 1 m)\n",
    "rivers = streams['chanbnkf'] > 1\n",
    "# rivers.plot(cmap='Blues')\n",
    "streams = streams.where(rivers)\n",
    "\n",
    "# calcular estadístico\n",
    "statistic = list(np.unique([stat for stats in func.values() for stat in stats]))\n",
    "attr_streams = catchment_statistics(streams, masks, statistic, weight=pixarea).to_pandas()\n",
    "cols = [f'{var}_{stat}' for var, stats in func.items() for stat in stats]\n",
    "attr_streams = attr_streams[cols]\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_streams,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=5,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_channels.jpg')\n",
    "\n",
    "del streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c116d-87c7-480f-b32e-a0be98344329",
   "metadata": {},
   "source": [
    "### Soil properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462ce6b-d006-4d51-9ffc-fd884a7d7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Soil_properties'\n",
    "variables = ['ksat', 'lambda', 'genua', 'soildepth', 'thetas', 'thetar']\n",
    "layers = [1, 2, 3]\n",
    "maps = [f'{var}{layer}' for var in variables for layer in layers]\n",
    "\n",
    "# load maps\n",
    "soils = {}\n",
    "for var in tqdm(maps, desc='loading maps'):\n",
    "    files = list((PATH_MAPS / category).glob(f'{var}_*.nc'))\n",
    "    if len(files) > 1:\n",
    "        ds = {}\n",
    "        for file in files:\n",
    "            # type of land use\n",
    "            cover = mapping_landuse[file.stem.split('_')[1]]\n",
    "            # import map\n",
    "            ds[cover] = xr.open_dataset(file)['Band1']\n",
    "        ds = xr.Dataset(ds)\n",
    "        da = ds.to_array('land_use').weighted(land_use.to_array('land_use').fillna(0)).sum('land_use', skipna=True)\n",
    "        soils[var] = da.where(~upArea.isnull())\n",
    "    elif len(files) == 1:\n",
    "        soils[var] = xr.open_dataset(files[0])['Band1']\n",
    "soils = xr.Dataset(soils)\n",
    "\n",
    "# compute statistics\n",
    "attr_soils = catchment_statistics(soils, masks, statistic=['mean'], weight=pixarea).to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_soils.rename(columns={col: col.split('_')[0] for col in attr_soils if 'mean' in col}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_soils,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=6,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_soils.jpg')\n",
    "\n",
    "del soils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f8a5a-6ff9-4ce1-b690-7731fe55af55",
   "metadata": {},
   "source": [
    "### LAI\n",
    "\n",
    "I convert the timeseries of 10-daily timesteps into annual and monthly averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91177beb-1f35-4aad-87a2-e9378aa652cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Vegetation_properties'\n",
    "maps = ['laif', 'laii', 'laio']\n",
    "\n",
    "# load maps\n",
    "lai = xr.Dataset({var: xr.open_mfdataset(f'{PATH_MAPS}/{category}/{var}*.nc')['Band1'].compute() for var in maps})\n",
    "lai = lai.rename({var: mapping_landuse[var[3]] for var in list(lai)})\n",
    "\n",
    "# mean wheighted by the portion of pixel covered by each land use\n",
    "lai = lai.to_array('land_use').weighted(land_use.to_array('land_use').fillna(0)).sum('land_use', skipna=True) \n",
    "lai = lai.where(~upArea.isnull())\n",
    "lai.name = 'lai'\n",
    "lai['time'] = pd.date_range('2021-01-05', periods=len(lai.time), freq='10D')\n",
    "\n",
    "# monthly resampling\n",
    "lai_m = lai.resample(time='1M').mean()\n",
    "lai_m['time'] = [f'{i:02}' for i in range(1, 13)]\n",
    "lai_agg = xr.Dataset({f'lai{month}': lai_m.sel(time=month).drop('time') for month in lai_m.time.data})\n",
    "\n",
    "# annual statistics\n",
    "lai_agg['laiyrmean'] = lai.mean('time')\n",
    "lai_agg['laiyrmax'] = lai.max('time')\n",
    "lai_agg['laiyrmin'] = lai.min('time')\n",
    "\n",
    "# compute statistics\n",
    "attr_lai = catchment_statistics(lai_agg, masks, statistic=['mean'], weight=pixarea).to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_lai.rename(columns={col: '_'.join(col.split('_')[:-1]) for col in attr_lai if 'mean' in col}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_lai,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=5,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_lai.jpg')\n",
    "\n",
    "del lai, lai_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178218f1-83e9-4ba4-8500-bfe8c0b6e79a",
   "metadata": {},
   "source": [
    "### Water demand\n",
    "\n",
    "The original demand maps are monthly time series (domestic, energy, industry, livestock) in mm/day for the period 1990-2023. \n",
    "\n",
    "I will compute annual and monthly averages and from those I will compute statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2562e-4b46-4fc2-879e-3035f5bcef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category = 'Water_demand'\n",
    "# maps = ['dom', 'ene', 'ind', 'liv']\n",
    "\n",
    "# # load maps\n",
    "# demand = {}\n",
    "# start, end = pd.to_datetime('1979-01-02'), pd.to_datetime('2023-01-01')\n",
    "# dates = pd.date_range(start, end, freq='D')\n",
    "# for use in tqdm(maps):\n",
    "    \n",
    "#     # load dataset of demand\n",
    "#     da = xr.open_mfdataset(f'{PATH_MAPS}/{category}/{use}*.nc')[use]\n",
    "    \n",
    "#     # break\n",
    "    \n",
    "#     # compute cachtment statistic\n",
    "#     df = catchment_statistics(da, masks, statistic=['mean'], weight=pixarea)[f'{use}_mean'].to_pandas()\n",
    "    \n",
    "#     break\n",
    "    \n",
    "#     # convert dataframe to daily resolution\n",
    "#     daily_df = pd.DataFrame(np.nan, index=dates, columns=df.columns)\n",
    "#     daily_df.index.name = 'time'\n",
    "#     daily_df.loc[df.index] = df\n",
    "\n",
    "#     # fill NaN\n",
    "#     daily_df = daily_df.ffill()\n",
    "#     # daily_df = daily_df.interpolate(method='linear')\n",
    "\n",
    "#     # convert to DataArray\n",
    "#     demand[use] = xr.Dataset.from_dataframe(daily_df).to_array(dim='id', name=use)\n",
    "\n",
    "# # combine all demands in one Dataset\n",
    "# demand = xr.Dataset(demand)\n",
    "\n",
    "# # monthly means\n",
    "# demand_m = demand.groupby('time.month').mean('time')\n",
    "\n",
    "# # annual mean\n",
    "# demand_y = demand.groupby('time.year').mean('time').mean('year')\n",
    "# # demand_y = demand.mean('time')\n",
    "\n",
    "# # combine in a single dataset\n",
    "# demand_agg = xr.Dataset()\n",
    "# for key, da in demand_m.items():\n",
    "#     for month in da.month.data:\n",
    "#         demand_agg[f'{key}_{month:02}'] = da.sel(month=month).drop('month')\n",
    "#     demand_agg[f'{key}_yr'] = demand_y[key]\n",
    "\n",
    "# # convert to volume\n",
    "# # demand_agg = demand_agg * 1e-3 * pixarea\n",
    "\n",
    "# # # compute statistics\n",
    "# # attr_demand = catchment_statistics(demand_agg, masks, statistic=['sum'], weight=pixarea).to_pandas()\n",
    "\n",
    "# # # rename attributes\n",
    "# # attr_demand.rename(columns={col: '_'.join(col.split('_')[:-1]) for col in attr_demand if 'sum' in col}, inplace=True)\n",
    "\n",
    "# attr_demand = demand_agg.to_pandas()\n",
    "\n",
    "# # plot attributes\n",
    "# plot_attributes(attr_demand[['dom_yr', 'ene_yr', 'ind_yr', 'liv_yr']],\n",
    "#                 catchments.geometry.x,\n",
    "#                 catchments.geometry.y,\n",
    "#                 ncols=4,\n",
    "#                 extent=[-180, 180, -90, 90],\n",
    "#                 save='maps_demand.jpg'\n",
    "#                )\n",
    "\n",
    "# del demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8a11f-f24e-4f75-b94d-7a7341f06c77",
   "metadata": {},
   "source": [
    "### Reservoirs\n",
    "\n",
    "It could be interesting to add two attributes that account for the number of reservoirs upstream and the total storage volume of those reservoirs. I can do it with the reservoirs in EFASv5, but those do not include all the reservoirs in ResOpsES (not to mention all the actual reservoirs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fb159-1448-4ae6-8c9a-97775f19d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Lakes_Reservoirs_ID_tables'\n",
    "var = 'res'\n",
    "\n",
    "# load map of reservoir ID\n",
    "res_xr = xr.open_mfdataset(f'{PATH_MAPS}/{category}/*{var}*.nc')[var].compute()\n",
    "\n",
    "# extract reservoir ID\n",
    "ids = [int(id) for id in np.unique(res_xr) if (not np.isnan(id))]\n",
    "ids = [id for id in ids if id >= 0]\n",
    "print('{0} reservoirs in the study area'.format(len(ids)))\n",
    "\n",
    "# load table of total reservoir storage\n",
    "file = list((PATH_MAPS / category).glob('rstor*.txt'))[0]\n",
    "storage_pd = pd.read_csv(file, sep='\\t', header=None, index_col=0).squeeze().astype('int64')\n",
    "storage_pd.index.name = 'ResID'\n",
    "storage_pd.name = 'storage'\n",
    "\n",
    "# create map of reservoir storage\n",
    "storage = res_xr.copy(deep=True)\n",
    "for id in ids:\n",
    "    storage = storage.where(res_xr != id, other=storage_pd.loc[id])\n",
    "# storage /= 1e6\n",
    "storage.name = 'storage'\n",
    "storage.attrs['units'] = 'm3'\n",
    "storage.attrs['standard_name'] = 'capacity'\n",
    "storage.attrs['long_name'] = 'reservoir_storage_capacity'\n",
    "\n",
    "# compute statistics\n",
    "attr_reservoir = catchment_statistics(storage, masks, statistic=['count', 'sum']).to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_reservoir.rename(columns={'storage_count': 'no_reservoirs', 'storage_sum': 'storage_reservoirs'}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_reservoir,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_reservoirs.jpg')\n",
    "\n",
    "del res_xr, storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680935ac-2c9b-47c2-9840-1fbf338e28c5",
   "metadata": {},
   "source": [
    "### Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a76bb-eca2-4c28-95ba-b642611c8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Lakes_Reservoirs_ID_tables'\n",
    "var = 'lakes'\n",
    "\n",
    "# load map or lake ID\n",
    "lakes_xr = xr.open_mfdataset(f'{PATH_MAPS}/{category}/*{var}*.nc')[var].compute()\n",
    "\n",
    "# extract lake ID\n",
    "ids = [int(id) for id in np.unique(lakes_xr) if not np.isnan(id)]\n",
    "ids = [id for id in ids if id >= 0]\n",
    "print('{0} lakes in the study area'.format(len(ids)))\n",
    "\n",
    "# load table of lake area\n",
    "file = list((PATH_MAPS / category).glob('lakearea*.txt'))[0]\n",
    "lakes_pd = pd.read_csv(file, sep='\\t', header=None, index_col=0).squeeze().astype('int64')\n",
    "lakes_pd.index.name = 'LakeID'\n",
    "lakes_pd.name = 'area'\n",
    "\n",
    "# create map of lake area\n",
    "lakearea = lakes_xr.copy(deep=True)\n",
    "for id in ids:\n",
    "    lakearea = lakearea.where(lakes_xr != id, other=lakes_pd.loc[id])\n",
    "lakearea.name = 'area'\n",
    "lakearea.attrs['units'] = 'm2'\n",
    "lakearea.attrs['standard_name'] = 'area'\n",
    "lakearea.attrs['long_name'] = 'lake_surface_area'\n",
    "\n",
    "# compute statistics\n",
    "attr_lake = catchment_statistics(lakearea, masks, statistic=['count', 'sum']).to_pandas()\n",
    "\n",
    "# rename attributes\n",
    "attr_lake.rename(columns={'area_count': 'no_lakes', 'area_sum': 'area_lakes'}, inplace=True)\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_lake, \n",
    "                catchments.geometry.x, \n",
    "                catchments.geometry.y, \n",
    "                extent=[-180, 180, -90, 90], \n",
    "                save='maps_lakes.jpg'\n",
    "               )\n",
    "\n",
    "del lakes_xr, lakearea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debbf762-3b6e-47df-8ec9-c1e59da49f7f",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdac53e-4679-4b9e-afb6-f73915e0a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all attributes\n",
    "attrs = pd.concat((attr_geomorphology,\n",
    "                  attr_landuse,\n",
    "                  attr_crops,\n",
    "                  attr_streams,\n",
    "                  attr_soils,\n",
    "                  attr_lai,\n",
    "                  # attr_demand,\n",
    "                  attr_reservoir,\n",
    "                  attr_lake,\n",
    "                 ), axis=1)\n",
    "attrs.index.name = 'ID'\n",
    "attrs.sort_index(axis=0, inplace=True)\n",
    "\n",
    "print('{0} attributes define the characteristics of {1} catchments'.format(*attrs.shape[::-1]))\n",
    "\n",
    "# export\n",
    "attrs.to_csv('attributes_static_maps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213266f-4f04-4cc5-b3ea-66ade17ba370",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a386a-ccf0-40b2-9309-5bf85865f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'GloFASv4.0_calibrated_parameters'\n",
    "variables = [str(item.name).split('_GloFAS')[0] for item in (PATH_MAPS / category).glob('*.nc')]\n",
    "\n",
    "# load maps\n",
    "parameters = {}\n",
    "for var in variables:\n",
    "    da = xr.open_mfdataset(f'{PATH_MAPS}/{category}/{var}*.nc')[var].compute()\n",
    "    if da.dtype == '<m8[ns]':\n",
    "        da = da.astype(float) / (1e9 * 3600 * 24)\n",
    "    parameters[var] = da\n",
    "parameters = xr.Dataset(parameters)\n",
    "parameters = parameters.where(parameters != -9999, np.nan)\n",
    "\n",
    "# compute statistics\n",
    "attr_parameters = catchment_statistics(parameters, masks, statistic=['mean'], weight=pixarea).to_pandas()\n",
    "attr_parameters.columns = ['_'.join(col.split('_')[:-1]) for col in attr_parameters.columns]\n",
    "\n",
    "# plot attributes\n",
    "plot_attributes(attr_parameters,\n",
    "                catchments.geometry.x,\n",
    "                catchments.geometry.y,\n",
    "                ncols=4,\n",
    "                extent=[-180, 180, -90, 90],\n",
    "                save='maps_parameters.jpg')\n",
    "\n",
    "del parameters\n",
    "\n",
    "attr_parameters.index.name = 'ID'\n",
    "attr_parameters.sort_index(axis=0, inplace=True)\n",
    "\n",
    "print('{0} attributes define the model parameters of {1} catchments'.format(*attr_parameters.shape[::-1]))\n",
    "\n",
    "# export\n",
    "attr_parameters.to_csv('attributes_model_parameters.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
