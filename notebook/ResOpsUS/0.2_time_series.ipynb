{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# Create dataset - time series\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado Rodr√≠guez<br>\n",
    "**Date:** 13-09-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code creates the time series for the reservoirs in ResOpsUS. The time series include records from ResOpsUS and simulations from GloFAS.\n",
    "\n",
    "The result is a time series that combines the observed data from ResOpsUS with the simulation from GloFASv4 (when possible). For each reservoir, these time series are exported both in CSV and a NetCDF format.\n",
    "\n",
    "Records are cleaned to avoid errors:\n",
    "    * Outliers in the **storage** time series are filtered by comparison with the a moving median (window 7 days). If the relative difference of a given storage value and the moving median exceeds a threshold, the value is removed. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_storage()`\n",
    "    * Outliers in the **inflow** time series are removed using two conditions: one based in the gradient, and the other using an estimated inflow based on the water balance. When both conditions are met, the value is removed. Since inflow time series cannot contain missing values when used in the reservoir simulation, a simple linear interpolation is used to fill in gaps up to 7 days. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_inflow()`.\n",
    "\n",
    "**To do:**<br>\n",
    "* [ ] 8 reservoirs that should be in GloFAS don't have time series.\n",
    "* [x] Plot time series\n",
    "* [x] Make sure that there aren't negative values in the time series, nor zeros in storage.\n",
    "* [x] Check the quality of the data by closing the mass balance when possible. <font color='steelblue'>I've used the mass balance to identify errors in the inflow time series (function `clean_inflow`).</font>.\n",
    "* [x] Fill in the inflow time series with the mass balance, if possible. <font color='steelblue'>I've filled in gaps in the inflow time series with linear interpolation up to 7-day gaps (function `clean_inflow`).</font>.\n",
    "\n",
    "<font color='red'> Is the ResOpsUS raw data in the same time zone as GloFAS?</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd31a8cb-3683-4fb1-bc9d-b9d606fe64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from lisfloodreservoirs.utils import DatasetConfig\n",
    "from lisfloodreservoirs import read_attributes\n",
    "from lisfloodreservoirs.utils.plots import plot_resops, reservoir_analysis, compare_flows\n",
    "from lisfloodreservoirs.utils.timeseries import clean_storage, clean_inflow, time_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9102e7db-c302-4e2f-a1f1-4a3820841820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\v2.0\\time_series\n"
     ]
    }
   ],
   "source": [
    "cfg = DatasetConfig('config_dataset.yml')\n",
    "\n",
    "print(f'Time series will be saved in {cfg.PATH_TS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbf891-79d3-4317-9964-b99f1df92649",
   "metadata": {},
   "source": [
    "### Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c3724e-1740-498c-9c74-f1903f53a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528 reservoirs in the attribute tables\n"
     ]
    }
   ],
   "source": [
    "# import all tables of attributes\n",
    "attributes = read_attributes(cfg.PATH_ATTRS)\n",
    "print(f'{attributes.shape[0]} reservoirs in the attribute tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228fdc-7588-4e0f-ad5e-25b95df3412a",
   "metadata": {},
   "source": [
    "### Time series\n",
    "#### ResOpsUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb750f07-f183-4757-ab99-67ac41f6ff03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848756f23e554dfc8bb3d05a72f61cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading observed time series:   0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528 reservoirs in ResOpsUS time series\n"
     ]
    }
   ],
   "source": [
    "path_plots = cfg.PATH_TS / 'plots'\n",
    "path_plots.mkdir(parents=True, exist_ok=True)\n",
    "resops_ts = {}\n",
    "for grand_id in tqdm(attributes.index, desc='Reading observed time series'): # ID refers to GRanD\n",
    "    # load timeseries\n",
    "    file = cfg.PATH_OBS_TS / f'ResOpsUS_{grand_id}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, index_col='date')\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "    # trim to GloFAS long run period\n",
    "    series = series.loc[cfg.START:cfg.END]\n",
    "    # remove duplicated index\n",
    "    series = series[~series.index.duplicated(keep='first')]\n",
    "    # remove negative values\n",
    "    series[series < 0] = np.nan\n",
    "    # clean storage time series\n",
    "    series.storage = clean_storage(series.storage, w=7, error_thr=0.1)\n",
    "    # clean inflow time series\n",
    "    series.inflow = clean_inflow(series.inflow, \n",
    "                                 storage=series.storage if attributes.loc[grand_id, 'STORAGE'] == 1 else None, \n",
    "                                 outlfow=series.outflow if attributes.loc[grand_id, 'OUTFLOW'] == 1 else None, \n",
    "                                 grad_thr=1e4, \n",
    "                                 balance_thr=5, \n",
    "                                 int_method='linear')\n",
    "    # save in dictionary\n",
    "    resops_ts[grand_id] = series\n",
    "\n",
    "    # plot observed time series\n",
    "    plot_resops(series.storage,\n",
    "                series.elevation,\n",
    "                series.inflow,\n",
    "                series.outflow,\n",
    "                attributes.loc[grand_id, ['CAP_MCM', 'CAP_GLWD']].values,\n",
    "                title=grand_id,\n",
    "                save=path_plots / f'{grand_id:04}_lineplot.jpg'\n",
    "               )\n",
    "\n",
    "print(f'{len(resops_ts)} reservoirs in ResOpsUS time series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c53eb1d-d35c-4c92-aedd-79f966374e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to xarray.Dataset\n",
    "xarray_list = []\n",
    "for key, df in resops_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(GRAND_ID=key)\n",
    "    xarray_list.append(ds)\n",
    "obs = xr.concat(xarray_list, dim='GRAND_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "#### GloFAS\n",
    "\n",
    "##### Reservoir variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eae2fa5-e6f0-423a-b3f5-675b7510d530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22cd65c46fc40dbb17f0debd4db5d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading simulated time series:   0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\296.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\179.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\197.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\323.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\068.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\185.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\512.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\465.csv doesn't exist\n",
      "111 reservoirs in GloFAS time series\n"
     ]
    }
   ],
   "source": [
    "# import time series\n",
    "glofas_ts = {}\n",
    "mask = ~attributes.GLOFAS_ID.isnull()\n",
    "for grand_id, glofas_id in tqdm(attributes[mask].GLOFAS_ID.iteritems(), total=mask.sum(), desc='Reading simulated time series'):\n",
    "    file = cfg.PATH_SIM_TS / f'{glofas_id:03.0f}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, dayfirst=False, index_col='date')\n",
    "        series.index -= timedelta(days=1)\n",
    "        series.storage *= attributes.loc[grand_id, 'CAP_GLWD']\n",
    "        series[series < 0] = np.nan\n",
    "        # series.columns = [f'{col.lower()}_glofas' for col in series.columns]\n",
    "        glofas_ts[grand_id] = series\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "        \n",
    "print(f'{len(glofas_ts)} reservoirs in GloFAS time series')\n",
    "\n",
    "# convert to xarray.Dataset\n",
    "new_dim = 'GRAND_ID'\n",
    "xarray_list = []\n",
    "for key, df in glofas_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords({new_dim: key})\n",
    "    xarray_list.append(ds)\n",
    "sim = xr.concat(xarray_list, dim=new_dim)\n",
    "\n",
    "# rename variables in the simulated time series\n",
    "sim = sim.rename_vars({var: f'{var}_glofas' for var in list(sim)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746404c-c5d9-4e65-8c73-41f478ef4501",
   "metadata": {},
   "source": [
    "##### Meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e043838-f825-45d7-a228-650eb683b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_areal = cfg.PATH_RESOPS / 'ancillary' / 'catchstats' / 'meteo'\n",
    "variables = [x.stem for x in path_meteo_areal.iterdir() if x.is_dir()]\n",
    "meteo_areal = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_areal}/{var}/*.nc')[f'{var}_mean'].compute() for var in variables})\n",
    "meteo_areal['time'] = meteo_areal['time'] - np.timedelta64(24, 'h') # WARNING!! One day lag compared with LISFLOOD\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_areal.id.data))\n",
    "meteo_areal = meteo_areal.sel(id=IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d836d62-deff-4a49-901e-fd0d8700c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'id' with the GRanD ID\n",
    "meteo_areal = meteo_areal.rename({\n",
    "    'id': 'GRAND_ID',\n",
    "    'time': 'date'\n",
    "}, )\n",
    "\n",
    "# # define attributes\n",
    "# emo1_units = 'e0_emo1: potential evaporation from open water from EMO1 [mm/d]\\npr_emo1: observed precipitation from EMO1 [mm/d]\\nta_emo1: observed air temperature from EMO1 [¬∞C]\\n'\n",
    "# meteo_areal.attrs['Units'] = emo1_units\n",
    "# meteo_areal.time.attrs['timezone'] = 'UTC+00'\n",
    "# meteo_areal.GRAND_ID.attrs['Description'] = 'The identifier of the reservor in GRanD (Global Reservoir and Dam database)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "145147a1-28f3-43c3-b978-c218dfc1fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_areal = meteo_areal.rename({\n",
    "    'e0': 'evapo_areal',\n",
    "    'tp': 'precip_areal',\n",
    "    '2t': 'temp_areal'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b39fa2-6569-41c5-bf00-8d3725e9eb2a",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "### Convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c817905b-edc7-4540-b237-8dc9021c7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.NORMALIZE:\n",
    "\n",
    "    # reservoir attributes used to normalize the dataset\n",
    "    area_sm = xr.DataArray.from_series(attributes.AREA_SKM) * 1e6 # m2\n",
    "    capacity_cm = xr.DataArray.from_series(attributes.CAP_MCM) * 1e6 # m3\n",
    "    catchment_sm = xr.DataArray.from_series(attributes.CATCH_SKM) * 1e6 # m2\n",
    "    \n",
    "    # Observed timeseries\n",
    "    # -------------------\n",
    "    for var, da in obs.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var in ['storage', 'evaporation']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var in ['inflow', 'outflow']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 24 * 3600 / capacity_cm\n",
    "\n",
    "    # Simulated timeseries\n",
    "    # -------------------\n",
    "    for var, da in sim.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var.split('_')[0] in ['storage']:\n",
    "            sim[f'{var}_norm'] = sim[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var.split('_')[0] in ['inflow', 'outflow']:\n",
    "            sim[f'{var}_norm'] = sim[var] * 24 * 3600 / capacity_cm\n",
    "            \n",
    "    # Catchment meteorology\n",
    "    # ---------------------\n",
    "\n",
    "    # convert areal evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_areal[f'{var}_areal_norm'] = meteo_areal[f'{var}_areal'] * catchment_sm * 1e-3 / capacity_cm                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed75f2-62e4-442e-b897-52dee820581e",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4ded39db-71fb-4209-9734-875971d6f6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f04d56552649dca4036ec138e192bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting time series:   0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_csv = cfg.PATH_TS / 'csv'\n",
    "path_csv.mkdir(parents=True, exist_ok=True)\n",
    "path_nc = cfg.PATH_TS / 'netcdf'\n",
    "path_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for grand_id in tqdm(attributes.index, desc='Exporting time series'):    \n",
    "\n",
    "    # concatenate time series\n",
    "    ds = obs.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])\n",
    "    if grand_id in sim.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, sim.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])))\n",
    "    if grand_id in meteo_areal.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, meteo_areal.sel(GRAND_ID=grand_id).drop(['GRAND_ID'])))\n",
    "        \n",
    "    # # delete empty variables\n",
    "    # for var in list(ds.data_vars):\n",
    "    #     if (ds[var].isnull().all()):\n",
    "    #         del ds[var]\n",
    "\n",
    "    # trim time series to the observed period\n",
    "    start, end = attributes.loc[grand_id, ['TIME_SERIES_START', 'TIME_SERIES_END']].values\n",
    "    ds = ds.sel(date=slice(start, end))\n",
    "        \n",
    "    # create time series of temporal attributes\n",
    "    ds['year'] = ds.date.dt.year\n",
    "    ds['month'] = ds.date.dt.month\n",
    "    ds['month_sin'], ds['month_cos'] = time_encoding(ds['month'], period=12)\n",
    "    ds['weekofyear'] = ds.date.dt.isocalendar().week\n",
    "    ds['woy_sin'], ds['woy_cos'] = time_encoding(ds['weekofyear'], period=52)\n",
    "    ds['dayofyear'] = ds.date.dt.dayofyear\n",
    "    ds['doy_sin'], ds['doy_cos'] = time_encoding(ds['dayofyear'], period=365)\n",
    "    ds['dayofweek'] = ds.date.dt.dayofweek\n",
    "    ds['dow_sin'], ds['dow_cos'] = time_encoding(ds['dayofweek'], period=6)\n",
    "        \n",
    "    # export CSV\n",
    "    # ..........\n",
    "    ds.to_pandas().to_csv(path_csv / f'{grand_id}.csv')\n",
    "\n",
    "    # export NetCDF\n",
    "    # .............\n",
    "    ds.to_netcdf(path_nc / f'{grand_id}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
