{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# Create dataset - time series\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado<br>\n",
    "**Date:** 21-06-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code creates the time series for the reservoirs in ResOpsUS. The time series include records from ResOpsUS and simulations from GloFAS.\n",
    "\n",
    "The result is a time series that combines the observed data from ResOpsUS with the simulation from GloFASv4 (when possible). For each reservoir, these time series are exported both in CSV and a NetCDF format.\n",
    "\n",
    "**To do:**<br>\n",
    "* [ ] 8 reservoirs that should be in GloFAS don't have time series.\n",
    "* [ ] Plot time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd31a8cb-3683-4fb1-bc9d-b9d606fe64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely import Point\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import spotpy\n",
    "# from spotpy.objectivefunctions import kge\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a377d549-4f45-4f2d-9d10-26422992451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\v1.0\\time_series\n"
     ]
    }
   ],
   "source": [
    "with open('config_dataset.yml', 'r', encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "# paths\n",
    "PATH_GLOFAS = Path(cfg['paths']['GloFAS']['root'])\n",
    "PATH_RESOPS = Path(cfg['paths']['ResOpsUS']['root'])\n",
    "PATH_OBS_TS = PATH_RESOPS / cfg['paths']['ResOpsUS']['obs_timeseries']\n",
    "PATH_SIM_TS = PATH_RESOPS / cfg['paths']['ResOpsUS']['sim_timeseries']\n",
    "PATH_GRAND = Path(cfg['paths']['GRanD'])\n",
    "\n",
    "# period\n",
    "START = cfg['period']['start']\n",
    "END = cfg['period']['end']\n",
    "\n",
    "# # conditions\n",
    "# MIN_AREA = cfg['conditions']['min_area'] # km2\n",
    "# MIN_VOL = cfg['conditions']['min_volume'] # hm3\n",
    "# MIN_TS = cfg['conditions']['min_length'] * 365 # days\n",
    "\n",
    "VERSION = cfg['version']\n",
    "PATH_OUT = PATH_RESOPS / VERSION / 'time_series'\n",
    "print(f'Time series will be saved in {PATH_OUT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228fdc-7588-4e0f-ad5e-25b95df3412a",
   "metadata": {},
   "source": [
    "### ResOpsUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1941e6-ad7b-4423-82a0-58d9fa7ed4fa",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f362f5f2-ec5a-4333-b31c-082e8791813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 reservoirs in ResOpsUS attributes\n"
     ]
    }
   ],
   "source": [
    "resops = pd.read_csv(PATH_RESOPS / VERSION / 'attributes' / 'resops_attributes.csv', index_col='GRAND_ID')\n",
    "print(f'{resops.shape[0]} reservoirs in ResOpsUS attributes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de8865-36a2-47fc-82e5-8d724c0e8c27",
   "metadata": {},
   "source": [
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5a5cdb-0ed0-44be-971a-b3d1fa0277dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528c9bc6f17a468dbde43001167c3879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading observed time series:   0%|          | 0/526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 reservoirs in ResOpsUS time series\n"
     ]
    }
   ],
   "source": [
    "resops_ts = {}\n",
    "for ID in tqdm(resops.index, desc='Reading observed time series'): # ID refers to GRanD\n",
    "    # load timeseries\n",
    "    file = PATH_OBS_TS / f'ResOpsUS_{ID}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, index_col='date')\n",
    "        # series.columns = series.columns.str.upper()\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "    # trim to GloFAS long run period\n",
    "    series = series.loc[START:END]\n",
    "    # remove duplicated index\n",
    "    series = series[~series.index.duplicated(keep='first')]\n",
    "    # remove empty series\n",
    "    # series.dropna(axis=1, how='all', inplace=True)\n",
    "    # convert storage from hm3 to m3\n",
    "    if 'STORAGE' in series.columns:\n",
    "        series.STORAGE *= 1e6\n",
    "    # save in dictionary\n",
    "    resops_ts[ID] = series\n",
    "\n",
    "print(f'{len(resops_ts)} reservoirs in ResOpsUS time series')\n",
    "\n",
    "# convert to xarray.Dataset\n",
    "xarray_list = []\n",
    "for key, df in resops_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(GRAND_ID=key)\n",
    "    xarray_list.append(ds)\n",
    "obs = xr.concat(xarray_list, dim='GRAND_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "### GloFAS\n",
    "\n",
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44350df-4c65-40ab-960b-a25a86629c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 reservoirs in GloFAS attributes\n"
     ]
    }
   ],
   "source": [
    "glofas = pd.read_csv(PATH_RESOPS / VERSION / 'attributes' / 'glofas_attributes.csv', index_col='GRAND_ID')\n",
    "print(f'{glofas.shape[0]} reservoirs in GloFAS attributes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f892871-7fc3-4296-b312-13ae41c7db90",
   "metadata": {},
   "source": [
    "#### Time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eae2fa5-e6f0-423a-b3f5-675b7510d530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bedb2af22f4c33bd1c7f0ebbaa78d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading simulated time series:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\296.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\179.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\197.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\323.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\068.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\185.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\512.csv doesn't exist\n",
      "Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\ancillary\\LISFLOOD\\465.csv doesn't exist\n",
      "110 reservoirs in GloFAS time series\n"
     ]
    }
   ],
   "source": [
    "# import time series\n",
    "glofas_ts = {}\n",
    "for grand_id, glofas_id in tqdm(glofas.GLOFAS_ID.iteritems(), total=glofas.shape[0], desc='Reading simulated time series'):\n",
    "    file = PATH_SIM_TS / f'{glofas_id:03}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, dayfirst=False, index_col='date')\n",
    "        series.index -= timedelta(days=1)\n",
    "        series.storage *= glofas.loc[grand_id, 'CAP_GLWD']\n",
    "        # series.columns = [f'{col.lower()}_glofas' for col in series.columns]\n",
    "        glofas_ts[grand_id] = series\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "        \n",
    "print(f'{len(glofas_ts)} reservoirs in GloFAS time series')\n",
    "\n",
    "# convert to xarray.Dataset\n",
    "new_dim = 'GRAND_ID'\n",
    "xarray_list = []\n",
    "for key, df in glofas_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords({new_dim: key})\n",
    "    xarray_list.append(ds)\n",
    "sim = xr.concat(xarray_list, dim=new_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc941d-c9ee-47d8-886c-94e53685ed2f",
   "metadata": {},
   "source": [
    "### GRanD\n",
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8505d2d-0348-47fc-88cb-ceedbb815fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 reservoirs in GRanD attributes\n"
     ]
    }
   ],
   "source": [
    "grand = pd.read_csv(PATH_RESOPS / VERSION / 'attributes' / 'grand_attributes.csv', index_col='GRAND_ID')\n",
    "print(f'{grand.shape[0]} reservoirs in GRanD attributes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47920d-e947-4f12-b926-12c1196aebfd",
   "metadata": {},
   "source": [
    "### Correct reservoir capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c893cb-8a08-44a3-ae80-be2e14c13500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     # import DataFrame with the fraction fill and the selected data source\n",
    "#     ff = pd.read_excel('fraction_fill.xlsx', index_col='ResID')\n",
    "# except:\n",
    "\n",
    "#     # create DataFrame with the fraction fill according to each data source\n",
    "#     ff = pd.DataFrame(columns=['GLOFAS', 'GRAND'], dtype=float)\n",
    "#     ff.index.name = 'ResID'\n",
    "#     for ID in glofas.index:\n",
    "#         cap_glofas = glofas.loc[ID, ['ResID', 'CAP_GLWD']]\n",
    "#         cap_resops = resops.loc[ID, 'CAP_RESOPS']\n",
    "#         cap_grand = grand.loc[ID, 'CAP_RESOPS']\n",
    "#         if np.isnan(cap_resops):\n",
    "#             continue\n",
    "#         ff.loc[ResID, :] = cap_resops / cap_glofas, cap_resops / cap_grand\n",
    "#      # export\n",
    "#     ff.to_excel('fraction_fill.xlsx', index=True)\n",
    "\n",
    "# # define the capacity  ('CAP') as that of the most reliable source\n",
    "# glofas['CAP'] = np.nan\n",
    "# for ID in glofas.index:\n",
    "#     ResID = glofas.loc[ID, 'ResID']\n",
    "#     if ff.loc[ResID, 'selection'] == 'GLOFAS':\n",
    "#         glofas.loc[ID, 'CAP'] = glofas.loc[ID, 'CAP_GLWD']\n",
    "#     elif ff.loc[ResID, 'selection']:\n",
    "#         glofas.loc[ID, 'CAP'] = glofas.loc[ID, 'CAP_GRAND']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f06c2-69cc-4106-8a06-b313316ac490",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ac7236-c180-4615-bca9-dc09c019fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reservoir attributes used to normalize the dataset\n",
    "area_sm = xr.DataArray.from_series(grand.AREA_SKM) * 1e6 # m2\n",
    "capacity_cm = xr.DataArray.from_series(grand.CAP_MCM) * 1e6 # m3\n",
    "catchment_sm = xr.DataArray.from_series(grand.CATCH_SKM) * 1e6 # m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec0f7f-dec6-4159-96ce-678a0a3fae6b",
   "metadata": {},
   "source": [
    "### Observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ccf1b2-5f09-4824-af86-dda41ef01b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_norm = deepcopy(obs)\n",
    "\n",
    "# convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "obs_norm['storage'] *= 1e6 / capacity_cm\n",
    "obs_norm['evaporation'] *= 1e6 / capacity_cm\n",
    "\n",
    "# convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "obs_norm['inflow'] *= 24 * 3600 / capacity_cm\n",
    "obs_norm['outflow'] *= 24 * 3600 / capacity_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4ecf7-f295-4702-b75c-06c124885b68",
   "metadata": {},
   "source": [
    "### Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ee3a4c-80b6-4583-ba1c-16148ee22c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_norm = deepcopy(sim)\n",
    "\n",
    "# convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "sim_norm['storage'] = sim_norm['storage'] * 1e6 / capacity_cm\n",
    "\n",
    "# convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "sim_norm['inflow'] = sim_norm['inflow'] * 24 * 3600 / capacity_cm\n",
    "sim_norm['outflow'] = sim_norm['outflow'] * 24 * 3600 / capacity_cm\n",
    "\n",
    "# rename variables\n",
    "sim_norm = sim_norm.rename_vars({var: f'{var}_glofas' for var in list(sim_norm)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed75f2-62e4-442e-b897-52dee820581e",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ded39db-71fb-4209-9734-875971d6f6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26d9f0250f946659e7b3265a4968a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting time series:   0%|          | 0/526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_csv = PATH_OUT / 'csv'\n",
    "path_csv.mkdir(parents=True, exist_ok=True)\n",
    "path_nc = PATH_OUT / 'netcdf'\n",
    "path_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for ID in tqdm(resops.index, desc='Exporting time series'):    \n",
    "\n",
    "    # concatenate time series\n",
    "    ds = obs_norm.sel(GRAND_ID=ID).drop(['GRAND_ID'])\n",
    "    if ID in sim.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, sim_norm.sel(GRAND_ID=ID).drop(['GRAND_ID'])))\n",
    "\n",
    "    # delete empty variables\n",
    "    for var in list(ds.data_vars):\n",
    "        if (ds[var].isnull().all()):\n",
    "            del ds[var]\n",
    "\n",
    "    # trim time series to the observed period\n",
    "    start, end = resops.loc[ID, ['TIME_SERIES_START', 'TIME_SERIES_END']]\n",
    "    ds = ds.sel(date=slice(start, end))\n",
    "\n",
    "    # create time series of temporal attributes\n",
    "    dates = pd.date_range(start, end, freq='D')\n",
    "    ds['year'] = xr.DataArray(dates.year.values, dims='date', name='year')\n",
    "    ds['month'] = xr.DataArray(dates.month.values, dims='date', name='month')\n",
    "    ds['weekofyear'] = xr.DataArray(dates.isocalendar().week.values, dims='date', name='weekofyear')\n",
    "    ds['dayofyear'] = xr.DataArray(dates.dayofyear.values, dims='date', name='dayofyear')\n",
    "    ds['dayofweek'] = xr.DataArray(dates.dayofweek.values, dims='date', name='dayofweek')\n",
    "\n",
    "    # export CSV\n",
    "    # ..........\n",
    "    ds.to_pandas().to_csv(path_csv / f'{ID}.csv')\n",
    "\n",
    "    # export NetCDF\n",
    "    # .............\n",
    "    ds.to_netcdf(path_nc / f'{ID}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c14a54d-be9c-48a0-a6c1-f86b28e3643f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
