{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casadoj/lisflood-reservoirs/blob/ResOpsUS/notebook/ResOpsUS/dataset/Caravan_part1_reservoirs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxjzJPFoCVO"
      },
      "source": [
        "# Extending Caravan to new basins\n",
        "\n",
        "Author: Frederik Kratzert\n",
        "Contact: kratzert@google.com\n",
        "\n",
        "Last updated: Jan 16, 2025\n",
        "\n",
        "This notebook is part of the [Caravan publication](https://www.nature.com/articles/s41597-023-01975-w) and is the first of two notebooks that can be used to extend the dataset to new basins. This notebook is intended to be run on Google Colab so that the data exports to Earth Engine works as intended.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kratzert/Caravan/blob/main/code/Caravan_part1_Earth_Engine.ipynb)\n",
        "\n",
        "## What is Caravan?\n",
        "\n",
        "Caravan is ...\n",
        "- an open source, large-sample rainfall-runoff dataset.\n",
        "- derived entirely in the cloud (Earth Engine).\n",
        "- a dataset that only uses globally available data products (namely [ECMWF's ERA5-Land](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land?tab=overview) and [HydroATLAS](https://www.hydrosheds.org/page/hydroatlas) by Linke et al.).\n",
        "\n",
        "Why?\n",
        "- to democratize large-sample hydrology: Anyone can extend the dataset to new regions without downloading and preprocessing large amounts of raw data. Instead, we use freely available cloud resources to do all heavy lifting in the cloud and only download the processed results.\n",
        "- to advance research on global rainfall-runoff models: So far, we have seen  different large-sample datasets (e.g. CAMELS variants) that are specific to individual countries. However, due to the use of data products with only local coverage (e.g. meteorological forcing data or static attribute maps), intercomparisons betweens these products are not straightforward.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- You need a Google account to be able to use Earth Engine\n",
        "- You need to add your shapefile as an [Asset](https://developers.google.com/earth-engine/guides/table_upload#upload-a-shapefile) to Google Earth Engine\n",
        "- The shapefile needs to have one field that indicates the basin/gauge ID, which will be used to link the derived data to the individual basin. Make sure to adapt the corresponding variable in the \"General configuration\" section below. It is recommend to name this field `gauge_id`, which will safe you some adaptations across the 2nd notebook.\n",
        "\n",
        "Note: This notebook only derives catchment attributes and meteorological forcing data for each polygon in the shapefile layer. We do not provide additional streamflow data beyond what is already included in Caravan.\n",
        "\n",
        "## Making new data available to the community\n",
        "\n",
        "We envision Caravan as a dynamically growing dataset that can be extended by its users. While making a large-sample dataset available upon the intial publication, the distribution of the basins is still limited to a few regions in the world. Ideally, this will change over time. Anyone who uses Caravan and this code to extend the dataset is invited to also share the streamflow time series data with the community, even if it is just for a single (or a few) basins. By doing so, little by little, we could create a new, globally spanning open source dataset that anyone can use for their own work/research. For the moment, the [discussion forum](https://github.com/kratzert/caravan/discussions) on the Github repository is the place to share information on data that you want to contribute. See also the [wiki](https://github.com/kratzert/caravan/wiki) on Github for further information.\n",
        "\n",
        "##  Structure of this notebook\n",
        "\n",
        "The code consists of two different sections:\n",
        "\n",
        "1. To derive static catchment attributes from HydroATLAS.\n",
        "2. To spatially aggregate ERA5-Land forcing data for each polygon.\n",
        "\n",
        "While the first point takes a couple of seconds up to a few minutes, depending on the number of basins, the second point takes considerably longer. However, within this notebook we only start jobs to process ERA5-Land on Earth Engine and to store the data in Google Drive. The tasks will run in the background and this notebook can be closed after the end of this notebook is reached. For details, see the introduction in each of the two data processing sections.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use the Caravan dataset or this code, please consider referencing the original publication\n",
        "\n",
        "```bib\n",
        "@article{kratzert2023caravan,\n",
        "  title={Caravan-A global community dataset for large-sample hydrology},\n",
        "  author={Kratzert, Frederik and Nearing, Grey and Addor, Nans and Erickson, Tyler and Gauch, Martin and Gilon, Oren and Gudmundsson, Lukas and Hassidim, Avinatan and Klotz, Daniel and Nevo, Sella and others},\n",
        "  journal={Scientific Data},\n",
        "  volume={10},\n",
        "  number={1},\n",
        "  pages={61},\n",
        "  year={2023},\n",
        "  publisher={Nature Publishing Group UK London}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvLi9Bm3eRi"
      },
      "source": [
        "# General Configurations\n",
        "\n",
        "- `ASSET_NAME`: String defining the path of the asset in Google Earth Engine. Usually something like `projects/ee-{username}/assets/{asset_name}`.\n",
        "\n",
        "- `OUTPUT_FOLDER_NAME`: The name of the folder (in Google Drive) that is used to store the resulting data. For example, `OUTPUT_FOLDER_NAME = 'my_caravan_extension'` will create a folder called `my_caravan_extension` in the main directory of your Google Drive.\n",
        "\n",
        "- `BASIN_ID_FIELD`: Name of the attribute field in the shapefile that contains the basin id. It is recommended to name this field `gauge_id`. Make sure that this name does not conflict with any of the field names of HydroATLAS. For example, you could use something like `'gauge_id'` or `'basin_id'` but _not_ `'HYBAS_ID'`, `'PFAF_ID'`, or `'MAIN_BAS'`, which are all existing field names in HydroATLAS/HydroSHEDS.\n",
        "\n",
        "- `BASIN_PREFIX`: A short descriptive string that is prepended to each basin id and that should be unique within the Caravan data space. For example, we use `camels` for basins from the CAMELS (US) dataset and `camelsgb` from the CAMELS-GB dataset. The final name for each basin within the attribute table will be `{BASIN_PREFIX}_{GAUGE_ID}`. Note, if you already included such a prefix in the basin id field of your shapefile, leave this field as an empty string. Please also read the README in the Caravan dataset folder on details about the folder structure of the dataset.\n",
        "\n",
        "- `AREA_MAX_THRESHOLD` (in km2): Defines the upper bound for the basin area. Larger basins in the shapefile are ignored when deriving the attributes and forcings. If `None`, all basins are considered. Note: To avoid user quota errors when processing too large basins, HydroATLAS attributes are derived from different HydroATLAS levels, given the size of a catchment. See [Configuration for HydroATLAS](#configuration-for-hydroatlas).\n",
        "\n",
        "- `AREA_MIN_THRESHOLD` (in km2): Defines the lower bound for the basin area. Smaller basins in the shapefile are ignored when deriving the attributes and forcings. If `None`, all basins are considered. Note: Due to the underlying resolution of HydroATLAS the attributes for basins that are much smaller than the HydroATLAS subpolygons will potentially become less and less representative of your catchment.\n",
        "\n",
        "- `EE_PROJECT_NAME`: Name of your personal Earth Engine project. Usually some string starting like `ee-{username}` if you followed the default project creation. Required to access your uploaded assets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7FqgYvtT3e8w"
      },
      "outputs": [],
      "source": [
        "ASSET_NAME = 'projects/ee-casadoj/assets/reservoirs_ResOpsUS'\n",
        "OUTPUT_FOLDER_NAME = 'Datasets/ResOpsUS/reservoirs'\n",
        "BASIN_ID_FIELD = 'ID'\n",
        "BASIN_PREFIX = 'GRAND'\n",
        "AREA_MAX_THRESHOLD = None\n",
        "AREA_MIN_THRESHOLD = None\n",
        "EE_PROJECT_NAME = 'resopsus'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC2x5IKi4zAJ"
      },
      "source": [
        "# Import modules\n",
        "\n",
        "Note: to be able to store data in Google Drive, you need to mount your Drive in this runtime. Follow the instructions of the pop-up window, otherwise you won't be able to store the HydroATLAS attributes or ERA5-Land data.\n",
        "\n",
        "Also note: You also need to give this runtime permission to access Earth Engine, follow the instructions in the text and paste the token from the link within the box below, then press Enter. For a more detailed description you can also check our [guide](https://github.com/kratzert/Caravan/wiki/Extending-Caravan-with-new-basins)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QyQTV9jO4ylh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b47d73-3b3f-43ed-a7e9-433ea0df674e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import ee\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Mount Google drive to colab runtime.\n",
        "drive.mount('/content/drive')\n",
        "drive_path = Path(f'/content/drive/My Drive/')\n",
        "\n",
        "# Authorize this Colab to access Earth Engine.\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=EE_PROJECT_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSdmgDYo6j5K"
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "A set of functions that are used below. Simply execute this block to make the functions available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TaN32dpz6oC_"
      },
      "outputs": [],
      "source": [
        "# @title Implementation of utility function\n",
        "\n",
        "\n",
        "def add_area_as_property(poly):\n",
        "    \"\"\"Adds polygon area (km2) as property to each feature.\n",
        "\n",
        "  Different shapefiles might have different property names and some are lacking\n",
        "  the area property all together. We use this function to have a homogeneous\n",
        "  area property we can later use for filtering catchments by the area.\n",
        "  \"\"\"\n",
        "    new_poly = ee.Feature(poly).set({\"area\": poly.area().divide(1000 * 1000)})\n",
        "    return new_poly\n",
        "\n",
        "\n",
        "def subset_properties(feat, property_name):\n",
        "    \"\"\"Returns feature collection with subset of properties.\"\"\"\n",
        "    properties = feat.propertyNames()\n",
        "    selected_properties = properties.filter(ee.Filter.eq(\"item\", property_name))\n",
        "    return feat.select(selected_properties)\n",
        "\n",
        "\n",
        "def join_features(poly):\n",
        "    \"\"\"Combine two polygons and add intersecting area as property.\"\"\"\n",
        "    primary = ee.Feature(poly.get(\"primary\"))\n",
        "    secondary = ee.Feature(poly.get(\"secondary\"))\n",
        "\n",
        "    # perform intersection (creates new object)\n",
        "    newPoly = primary.intersection(secondary)\n",
        "\n",
        "    # get area of overlap\n",
        "    area = newPoly.area().divide(1000 * 1000)\n",
        "\n",
        "    # copy properties of both initial polygons in new object\n",
        "    newPoly = newPoly.copyProperties(primary).copyProperties(secondary)\n",
        "\n",
        "    # return with the area over intersection added as property\n",
        "    return ee.Feature(newPoly).set({\"Intersect\": area}).setGeometry(None)\n",
        "\n",
        "\n",
        "def start_export_task_img_coll(img_coll, feature_coll, folder, filename):\n",
        "    \"\"\"Start Export task on Earth Engine.\n",
        "\n",
        "  This function is used to send one processing and exporting task for ERA5-Land\n",
        "  data to Earth Engine.\n",
        "  \"\"\"\n",
        "    func = lambda img: img.reduceRegions(collection=feature_coll, reducer=ee.Reducer.mean(), scale=5000)\n",
        "    results_table = img_coll.map(func)\n",
        "    new_table = results_table.flatten().map(lambda x: x.setGeometry(None))\n",
        "\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=new_table,\n",
        "        folder=folder,\n",
        "        description=f\"{filename}_era5land\",\n",
        "        fileNamePrefix=filename,\n",
        "    )\n",
        "    task.start()\n",
        "    return task\n",
        "\n",
        "\n",
        "def compute_pour_point_properties(basin_data, min_overlap_threshold, pour_point_properties):\n",
        "    \"\"\"Finds most downstream polygon to extract pour point features.\n",
        "\n",
        "  Some features are only defined for the entire upstream area. Here, we need to\n",
        "  find the most downstream HydroATLAS polygon that still has a significant\n",
        "  ( > 50%) overlap with the catchment polygon and take the attributes from there.\n",
        "  In case of non-significant overlap of the most downstream polygon, it is\n",
        "  possible that two (or more) merging rivers need to be considered that all drain\n",
        "  into that most downstream polygon. If that is the case, we take the sum of the\n",
        "  pour-point properties (as all of these properties are volumnes, counts, areas\n",
        "  etc.)\n",
        "  \"\"\"\n",
        "    # Find basin with the largest overlap (in percent).\n",
        "    percentage_overlap = [x / y for x, y in zip(basin_data[\"weights\"], basin_data[\"SUB_AREA\"])]\n",
        "    current_basin_pos = np.argmax(percentage_overlap)\n",
        "\n",
        "    # Get the HYBAS_ID of the next downstream gauge.\n",
        "    next_down_id = basin_data[\"NEXT_DOWN\"][current_basin_pos]\n",
        "\n",
        "    # Traverse the river network downstream until we hit the termination condition.\n",
        "    while True:\n",
        "\n",
        "        # Make sure that we did not reached the ocean (HYBAS_ID=0), otherwise we would\n",
        "        # later pick all intersecting polygons that also drain into HYBAS_ID = 0.\n",
        "        if next_down_id == 0:\n",
        "            break\n",
        "\n",
        "        # Check if next_down_id is in the list of HYBAS_IDs.\n",
        "        if next_down_id not in basin_data[\"HYBAS_ID\"]:\n",
        "            break\n",
        "\n",
        "        # Get position of next_down_id in the list of HYBAS_IDs\n",
        "        next_down_pos = basin_data[\"HYBAS_ID\"].index(next_down_id)\n",
        "\n",
        "        # Check that the intersection of the next downstream polygon is at least 50%\n",
        "        if percentage_overlap[next_down_pos] < 0.5:\n",
        "            break\n",
        "\n",
        "        # Continue with the next polygon.\n",
        "        next_down_id = basin_data[\"NEXT_DOWN\"][next_down_pos]\n",
        "\n",
        "    # Find all polygons that would have drained into that polygons, which can\n",
        "    # potentially be more than one in case where two (or more) rivers merge.\n",
        "    direct_upstream_polygons = []\n",
        "    for i, next_down in enumerate(basin_data[\"NEXT_DOWN\"]):\n",
        "        if (next_down == next_down_id) and ((basin_data[\"weights\"][i] > min_overlap_threshold) or\n",
        "                                            (basin_data[\"weights\"][i] / basin_data[\"SUB_AREA\"][i] > 0.5)):\n",
        "            direct_upstream_polygons.append(i)\n",
        "\n",
        "    # Finally compute metrics. As all pour_point_properties are volumes or\n",
        "    # areas, we can simply compute the sum.\n",
        "    aggregated_properties = {}\n",
        "    for prop in pour_point_properties:\n",
        "        aggregated_properties[prop] = sum([basin_data[prop][i] for i in direct_upstream_polygons])\n",
        "\n",
        "    return aggregated_properties\n",
        "\n",
        "\n",
        "def get_hydroatlas_intersections(\n",
        "    basin_fc: ee.FeatureCollection,\n",
        "    hydroatlas_fc: ee.FeatureCollection,\n",
        "    batch_size: int,\n",
        "    min_overlap_threshold: int,\n",
        "):\n",
        "\n",
        "    # Get the number of basins in the feature collection.\n",
        "    n_basins = basin_fc.size().getInfo()\n",
        "\n",
        "    # create Filterobject for the intersection\n",
        "    spatialFilter = ee.Filter.intersects(leftField=\".geo\", rightField=\".geo\", maxError=10)\n",
        "\n",
        "    # create join object\n",
        "    join = ee.Join.inner()\n",
        "\n",
        "    # list of all available basins\n",
        "    basin_ids = basin_fc.aggregate_array(BASIN_ID_FIELD).getInfo()\n",
        "\n",
        "    # dictionary to store the results\n",
        "    results = {b: defaultdict(list) for b in basin_ids}\n",
        "\n",
        "    # get number of batch requests to make\n",
        "    n_batches = n_basins // batch_size\n",
        "    if n_basins % batch_size > 0:\n",
        "        n_batches += 1\n",
        "\n",
        "    # iterate over batches\n",
        "    for batch in tqdm(range(n_batches)):\n",
        "        start_idx = batch * batch_size\n",
        "        if (batch + 1) * batch_size > n_basins:\n",
        "            process_n_basins = n_basins - start_idx\n",
        "        else:\n",
        "            process_n_basins = batch_size\n",
        "\n",
        "        # perform intersection\n",
        "        intersectJoined = join.apply(\n",
        "            ee.FeatureCollection(basin_fc.toList(process_n_basins, start_idx)),\n",
        "            hydroatlas_fc,\n",
        "            spatialFilter,\n",
        "        )\n",
        "        intersected_basins = intersectJoined.map(join_features).getInfo()\n",
        "\n",
        "        # iterate over intersected polygon and extract the information needed\n",
        "        for polygon in intersected_basins[\"features\"]:\n",
        "\n",
        "            basin_id = polygon[\"properties\"][BASIN_ID_FIELD]\n",
        "\n",
        "            # check if the intersect is larger than the minimum overlap treshold\n",
        "            if (polygon[\"properties\"][\"Intersect\"]\n",
        "                    > min_overlap_threshold) or (polygon[\"properties\"][\"Intersect\"] / polygon[\"properties\"][\"SUB_AREA\"]\n",
        "                                                 > 0.5):\n",
        "\n",
        "                # For some reason, the intersections for some basins are returned twice,\n",
        "                # leading to errors when computing the aggregates and especially the basin\n",
        "                # area. Here, we make sure that we do not add an intersect again, by\n",
        "                # filtering for unique intersect areas.\n",
        "                if (polygon[\"properties\"][\"Intersect\"] not in results[basin_id][\"weights\"]):\n",
        "\n",
        "                    for prop in USE_PROPERTIES:\n",
        "                        results[basin_id][prop].append(polygon[\"properties\"][prop])\n",
        "\n",
        "                    # extract area of intersection, used as weight\n",
        "                    results[basin_id][\"weights\"].append(polygon[\"properties\"][\"Intersect\"])\n",
        "\n",
        "            # To compute the true basin area (that is similar to the area of the basin\n",
        "            # polygon), we need the area of all intersecting HydroATLAS polygons, even\n",
        "            # the small bits that we exclude when we aggregate the attributes.\n",
        "            if (polygon[\"properties\"][\"Intersect\"] not in results[basin_id][\"area_fragments\"]):\n",
        "                results[basin_id][\"area_fragments\"].append(polygon[\"properties\"][\"Intersect\"])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def aggregate_hydroatlas_intersections(group_results: dict[str, dict], min_overlap_threshold: float):\n",
        "\n",
        "    aggregated_results = defaultdict(dict)\n",
        "    for basin, basin_data in tqdm(group_results.items()):\n",
        "\n",
        "        if basin == \"camelscl_2120001\":\n",
        "            continue\n",
        "\n",
        "        # extract weights for the weighted aggregation of the attributes\n",
        "        weights = np.array(basin_data[\"weights\"])\n",
        "\n",
        "        # compute the mask of intersections with a sufficient overlap\n",
        "        mask = weights > min_overlap_threshold\n",
        "        masked_weights = weights[mask]\n",
        "\n",
        "        # perform aggregation\n",
        "        for key, val in basin_data.items():\n",
        "            # Ignore these auxilary properties\n",
        "            if key in [\n",
        "                    \"weights\",\n",
        "                    \"UP_AREA\",\n",
        "                    \"area_fragments\",\n",
        "                    \"HYBAS_ID\",\n",
        "                    \"NEXT_DOWN\",\n",
        "                    \"SUB_AREA\",\n",
        "            ]:\n",
        "                continue\n",
        "\n",
        "            # Pour-point properties are treated separately below.\n",
        "            if key in POUR_POINT_PROPERTIES:\n",
        "                continue\n",
        "\n",
        "            val = np.array(val)\n",
        "\n",
        "            # only consider intersections with sufficient overlap\n",
        "            masked_val = val[mask]\n",
        "\n",
        "            # for wetland classes, define 'no wetland' as a new class.\n",
        "            # In the dataset this value is -999, here we set it to 13\n",
        "            if key == \"wet_cl_smj\":\n",
        "                masked_val[masked_val == -999] = 13\n",
        "\n",
        "            # if all values are -999, set value to NaN\n",
        "            if len(masked_val[masked_val == -999]) == len(masked_val):\n",
        "                aggregated_results[basin][key] = np.nan\n",
        "            else:\n",
        "                # Majority vote for the class properties\n",
        "                if key in MAJORITY_PROPERTIES:\n",
        "                    aggregated_results[basin][key] = np.bincount(\n",
        "                        masked_val[masked_val > -999],\n",
        "                        weights=masked_weights[masked_val > -999],\n",
        "                    ).argmax()\n",
        "\n",
        "                # Averaging for all remaining properties\n",
        "                else:\n",
        "                    aggregated_results[basin][key] = np.average(\n",
        "                        masked_val[masked_val > -999],\n",
        "                        weights=masked_weights[masked_val > -999],\n",
        "                    )\n",
        "\n",
        "        aggregated_pour_point_features = compute_pour_point_properties(\n",
        "            basin_data=basin_data,\n",
        "            min_overlap_threshold=min_overlap_threshold,\n",
        "            pour_point_properties=POUR_POINT_PROPERTIES,\n",
        "        )\n",
        "        for key, val in aggregated_pour_point_features.items():\n",
        "            aggregated_results[basin][key] = val\n",
        "\n",
        "        # basin area is the sum of the area of all intersecting fragments, also\n",
        "        # the smaller intersections that are ignored above.\n",
        "        aggregated_results[basin][\"area\"] = sum(basin_data[\"area_fragments\"])\n",
        "\n",
        "        # We also store the fraction of the area considered during aggregation\n",
        "        # to the total basin area.\n",
        "        aggregated_results[basin][\"area_fraction_used_for_aggregation\"] = sum(masked_weights) / sum(\n",
        "            basin_data[\"area_fragments\"])\n",
        "\n",
        "    return aggregated_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBtxX7dj-p2V"
      },
      "source": [
        "# Processing ERA5-Land\n",
        "\n",
        "This section of the code prepares and starts tasks to process hourly ERA5-Land data for each basin in the shapefile. After this section is completed, you can close this notebook since the tasks were sent to Earth Engine and Earth Engine will store the resulting files in Drive.\n",
        "\n",
        "**Note**: It might take several hours until you find the processed data in your Google Drive. The time depends on the length of ERA5-Land data that you request, the number of basins in your shape layer, the size of the basins and on the numbers of points that span your polygons. If your basin polygons have a very fine resolution, consider simplifying them to speed up this step, e.g., using the [QGIS simplify function](https://docs.qgis.org/2.8/en/docs/user_manual/processing_algs/qgis/vector_geometry_tools/simplifygeometries.html) (we used the default tolerance of 1.0 for e.g. CAMELS US and CAMELS CL)\n",
        "\n",
        "## TLDR; What do we do here?\n",
        "\n",
        "Since most streamflow records are stored in local time but ERA5-Land is provided in UTC, we use ERA5-Land in hourly resolution, even if we are only interested in daily aggregates. By using hourly data, we can shift the forcing data to local time before we compute the daily aggregates and thus make sure that the forcing data aligns with the underlying streamflow data. However, this is part of the second processing step of the meteorological forcing data that runs on your local machine (no worries, this is something that any laptop/PC can do), after we used Earth Engine to compute the spatial averages.\n",
        "\n",
        "## Output\n",
        "\n",
        "The output of this part of the code is a number (we split the time into manageable slices) of csv files. Each csv file contains one row of data per basin and timestep. The data are the spatially averaged ERA5-Land variables that we request Earth Engine to process. Note: These files can become large since this is still a lot of data. If you do not have enough free space in your Google Drive, either upgrade your disk space or process the ERA5-Land time period in chunks and download the processed data after every step.\n",
        "\n",
        "After you have downloaded all files to your local machine, use our other [notebook]() to process the data locally into time series data and into the Caravan native format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-S7hotuC96v"
      },
      "source": [
        "## Configuration for ERA5-Land\n",
        "\n",
        "- `IMGS_PER_REQUEST`: This defines the number of ERA5-Land images (one image = one hour) that we want to process in one request. Earth Engine is good in parallelizing compute, so a larger number is preferable. However, Earth Engine also has a size limit for a single user request. If you see an error when sending the requests that states that the query is aborted due to too many elements, consider reducing this number.\n",
        "\n",
        "- `START_DATE`: The first day of the period for which you request data. In Caravan, we include forcing data starting January 1st, 1981.\n",
        "\n",
        "- `END_DATE`: The last day of the period for which you request data. In Caravan, we include forcing data until December 31st, 2020. Note, that this date refers to data in UTC. To be able to include December 31st, 2020 in the local time zone you might actually need to request one additional day, depending on the location.\n",
        "\n",
        " `ERA5L_BANDS`: The list of bands for which you wish to compute the spatial averages. You can change this at will, however to be compatible with Caravan, you should probably leave it as is. The names have to match the names in [this table](https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_LAND_HOURLY#bands). **Note** The second notebook that processes the output of Earth Engine assumes that these bands are available. If you make changes here, you might need to adapt parts of the second notebook.\n",
        "\n",
        " **Note**: Later, in the second notebook, we shift the ERA5-Land data from UTC-0 to local time, before aggregating to daily resolution. Here, we only consider days with values for all 24 hours. Therefore, adapt the `START_DATE` and the `END_DATE` according to the local timezone of your data with a 1 day buffer. That is, if your basins are east of UTC-0, adapt your `START_DATE` to include e.g. the 31st December. If the basins are west of UTC-0, adapt the `END_DATE` to include 1st January. If you want to be on the safe side, simply include a 1 day buffer on both side and then crop the data in the 2nd notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fVjqXIne-Tqk"
      },
      "outputs": [],
      "source": [
        "IMGS_PER_REQUEST = 365 * 24\n",
        "START_DATE = '1974-12-31'  # Format: YYYY-MM-DD\n",
        "END_DATE = '2024-01-01'  # Format: YYYY-MM-DD\n",
        "\n",
        "# ------------------------------ Change with caution --------------------------#\n",
        "ERA5L_BANDS = [\n",
        "    'dewpoint_temperature_2m',\n",
        "    'temperature_2m',\n",
        "    'volumetric_soil_water_layer_1',\n",
        "    'volumetric_soil_water_layer_2',\n",
        "    'volumetric_soil_water_layer_3',\n",
        "    'volumetric_soil_water_layer_4',\n",
        "    'surface_net_solar_radiation',\n",
        "    'surface_net_thermal_radiation',\n",
        "    'u_component_of_wind_10m',\n",
        "    'v_component_of_wind_10m',\n",
        "    'surface_pressure',\n",
        "    'total_precipitation',\n",
        "    'snow_depth_water_equivalent',\n",
        "    'potential_evaporation',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QinE2uM2EJSv"
      },
      "source": [
        "## Initialize feature collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WeTlP-8JEFmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd640309-61d0-429c-9a34-62f104000e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of basins: 677\n",
            "Number of hourly ERA5 images: 429528\n"
          ]
        }
      ],
      "source": [
        "# initialize featurecollection from uploaded asset\n",
        "basins = ee.FeatureCollection(ASSET_NAME)\n",
        "\n",
        "# Add area as polygon property and filter using the threshold defined above\n",
        "basins = basins.map(add_area_as_property)\n",
        "if AREA_MIN_THRESHOLD is not None:\n",
        "    basins = basins.filter(ee.Filter.greaterThan(rightValue=configuration['area_greater_than'], leftField='area'))\n",
        "if AREA_MAX_THRESHOLD is not None:\n",
        "    basins = basins.filter(\n",
        "        ee.Filter.lessThanOrEquals(rightValue=configuration['area_not_greater_than'], leftField='area'))\n",
        "\n",
        "# remove all unnecessary fields from the basin polygons to reduce the export size\n",
        "basins = basins.map(lambda f: subset_properties(f, BASIN_ID_FIELD))\n",
        "\n",
        "# initialize featurecollection of ERA5-Land hourly data\n",
        "era5_collection = ee.ImageCollection('ECMWF/ERA5_LAND/HOURLY').filterDate(START_DATE, END_DATE)\n",
        "\n",
        "# subset ERA5 bands\n",
        "era5_collection = era5_collection.select(ERA5L_BANDS)\n",
        "\n",
        "# Print some meta information\n",
        "n_basins = basins.size().getInfo()\n",
        "n_images = era5_collection.size().getInfo()\n",
        "\n",
        "print(f'Number of basins: {n_basins}')\n",
        "print(f'Number of hourly ERA5 images: {n_images}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7yMLA6_F0XO"
      },
      "source": [
        "## Start processing/exporting tasks\n",
        "\n",
        "This block will create the Earth Engine jobs to process the ERA5-Land data. After this block is completed, you can close this notebook and wait for Earth Engine to store the data in your drive (might take time!). You can see the list of current tasks in the [code editor of Earth Engine](https://code.earthengine.google.com/) (right side under \"Tasks\") or use the code block below to print the current list of jobs (and their state) in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5AnW5xDgESx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd267629-3179-4c90-e7c3-d300f61d821c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 8760 images, starting from index 0\n",
            "Processing 8760 images, starting from index 8760\n",
            "Processing 8760 images, starting from index 17520\n",
            "Processing 8760 images, starting from index 26280\n",
            "Processing 8760 images, starting from index 35040\n",
            "Processing 8760 images, starting from index 43800\n",
            "Processing 8760 images, starting from index 52560\n",
            "Processing 8760 images, starting from index 61320\n",
            "Processing 8760 images, starting from index 70080\n",
            "Processing 8760 images, starting from index 78840\n",
            "Processing 8760 images, starting from index 87600\n",
            "Processing 8760 images, starting from index 96360\n",
            "Processing 8760 images, starting from index 105120\n",
            "Processing 8760 images, starting from index 113880\n",
            "Processing 8760 images, starting from index 122640\n",
            "Processing 8760 images, starting from index 131400\n",
            "Processing 8760 images, starting from index 140160\n",
            "Processing 8760 images, starting from index 148920\n",
            "Processing 8760 images, starting from index 157680\n",
            "Processing 8760 images, starting from index 166440\n",
            "Processing 8760 images, starting from index 175200\n",
            "Processing 8760 images, starting from index 183960\n",
            "Processing 8760 images, starting from index 192720\n",
            "Processing 8760 images, starting from index 201480\n",
            "Processing 8760 images, starting from index 210240\n",
            "Processing 8760 images, starting from index 219000\n",
            "Processing 8760 images, starting from index 227760\n",
            "Processing 8760 images, starting from index 236520\n",
            "Processing 8760 images, starting from index 245280\n",
            "Processing 8760 images, starting from index 254040\n",
            "Processing 8760 images, starting from index 262800\n",
            "Processing 8760 images, starting from index 271560\n",
            "Processing 8760 images, starting from index 280320\n",
            "Processing 8760 images, starting from index 289080\n",
            "Processing 8760 images, starting from index 297840\n",
            "Processing 8760 images, starting from index 306600\n",
            "Processing 8760 images, starting from index 315360\n",
            "Processing 8760 images, starting from index 324120\n",
            "Processing 8760 images, starting from index 332880\n",
            "Processing 8760 images, starting from index 341640\n",
            "Processing 8760 images, starting from index 350400\n",
            "Processing 8760 images, starting from index 359160\n",
            "Processing 8760 images, starting from index 367920\n",
            "Processing 8760 images, starting from index 376680\n",
            "Processing 8760 images, starting from index 385440\n",
            "Processing 8760 images, starting from index 394200\n",
            "Processing 8760 images, starting from index 402960\n",
            "Processing 8760 images, starting from index 411720\n",
            "Processing 8760 images, starting from index 420480\n",
            "Processing 288 images, starting from index 429240\n",
            "All jobs were sent to Earth Engine.\n"
          ]
        }
      ],
      "source": [
        "start = 0\n",
        "batch_number = 1\n",
        "# how many images to process in one task\n",
        "\n",
        "n_imgs = IMGS_PER_REQUEST\n",
        "while start < n_images:\n",
        "    # clip the number of images to process by the number of total available images\n",
        "    if (start + n_imgs) > n_images:\n",
        "        n_imgs = n_images - start\n",
        "\n",
        "    print(f\"Processing {n_imgs} images, starting from index {start}\")\n",
        "\n",
        "    # start export task for 'n_imgs' elements, starting at index 'start'\n",
        "    test_task = start_export_task_img_coll(\n",
        "        img_coll=ee.ImageCollection(era5_collection.toList(count=n_imgs, offset=start)),\n",
        "        feature_coll=basins,\n",
        "        folder=OUTPUT_FOLDER_NAME,\n",
        "        filename=f\"batch{str(batch_number).zfill(2)}\",\n",
        "    )\n",
        "    start += n_imgs\n",
        "    batch_number += 1\n",
        "\n",
        "print(\"All jobs were sent to Earth Engine.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFv3cfXqImAk"
      },
      "source": [
        "# Check current tasks on Earth Engine\n",
        "\n",
        "The next section can be used optionally to check the state of the current tasks and can also be run separately of the other code (e.g., if you want to check the state of your ERA5-Land exporting tasks after a couple of hours)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_z17GryFqT2"
      },
      "outputs": [],
      "source": [
        "task_prefix = 'batch'\n",
        "\n",
        "\n",
        "def get_task_status(_prefix=None):\n",
        "    task_status_list = [t.status() for t in ee.batch.Task.list()]\n",
        "\n",
        "    # If a prefix was specified, use it to filter the task list.\n",
        "    if _prefix:\n",
        "        task_status_list = [s for s in task_status_list if s['description'].startswith(_prefix)]\n",
        "    return task_status_list\n",
        "\n",
        "\n",
        "df = pd.DataFrame(get_task_status(task_prefix))\n",
        "\n",
        "# Convert datetime fields.\n",
        "df['creation_timestamp_ms'] = pd.to_datetime(df['creation_timestamp_ms'], unit='ms', utc=True)\n",
        "df['start_timestamp_ms'] = pd.to_datetime(df['start_timestamp_ms'], unit='ms', utc=True)\n",
        "df['update_timestamp_ms'] = pd.to_datetime(df['update_timestamp_ms'], unit='ms', utc=True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISG6-gSnW2Lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}