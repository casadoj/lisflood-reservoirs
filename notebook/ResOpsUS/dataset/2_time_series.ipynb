{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# Create dataset - time series\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado Rodr√≠guez<br>\n",
    "**Date:** 26-05-2025<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "This code creates the time series for the reservoirs in ResOpsUS. The time series include records from ResOpsUS and simulations from GloFAS.\n",
    "\n",
    "The result is a time series that combines the observed data from ResOpsUS with the simulation from GloFASv4 (when possible). For each reservoir, these time series are exported both in CSV and a NetCDF format.\n",
    "\n",
    "Records are cleaned to avoid errors:\n",
    "\n",
    "* Outliers in the **storage** time series are filtered by comparison with the a moving median (window 7 days). If the relative difference of a given storage value and the moving median exceeds a threshold, the value is removed. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_storage()`\n",
    "* Outliers in the **inflow** time series are removed using two conditions: one based in the gradient, and the other using an estimated inflow based on the water balance. When both conditions are met, the value is removed. Since inflow time series cannot contain missing values when used in the reservoir simulation, a simple linear interpolation is used to fill in gaps up to 7 days. This procedure is encapsulated in the function `lisfloodreservoirs.utils.timeseries.clean_inflow()`.\n",
    "\n",
    "**To do:**<br>\n",
    "* [ ] <font color='red'> Is the ResOpsUS raw data in the same time zone as GloFAS?</font>\n",
    "* [ ] Demand time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd31a8cb-3683-4fb1-bc9d-b9d606fe64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from lisfloodreservoirs.utils import DatasetConfig\n",
    "from lisfloodreservoirs import read_attributes\n",
    "from lisfloodreservoirs.utils.plots import plot_resops, reservoir_analysis, compare_flows\n",
    "from lisfloodreservoirs.utils.timeseries import clean_storage, clean_inflow, time_encoding\n",
    "from lisfloodreservoirs.utils.timezone import convert_to_utc, reindex_to_00utc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9102e7db-c302-4e2f-a1f1-4a3820841820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series will be saved in Z:\\nahaUsers\\casadje\\datasets\\reservoirs\\ResOpsUS\\v2.1\\time_series\n"
     ]
    }
   ],
   "source": [
    "cfg = DatasetConfig('config_ResOpsUS_v21.yml')\n",
    "\n",
    "print(f'Time series will be saved in {cfg.PATH_TS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbf891-79d3-4317-9964-b99f1df92649",
   "metadata": {},
   "source": [
    "### Attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c3724e-1740-498c-9c74-f1903f53a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677 reservoirs in the attribute tables\n"
     ]
    }
   ],
   "source": [
    "# import all tables of attributes\n",
    "attributes = read_attributes(cfg.PATH_ATTRS)\n",
    "print(f'{attributes.shape[0]} reservoirs in the attribute tables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1228fdc-7588-4e0f-ad5e-25b95df3412a",
   "metadata": {},
   "source": [
    "### Time series\n",
    "#### Oberserved: ResOpsUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3fdb0d-e45e-436d-95ee-a7464b3b013a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25753f1c0a4d4c53a48fb8213da1f10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading observed time series:   0%|          | 0/677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir 288 has no observations in the time period from 1975-01-01 00:00:00 to None\n",
      "676 reservoirs in ResOpsUS time series\n"
     ]
    }
   ],
   "source": [
    "path_plots = cfg.PATH_TS / 'plots'\n",
    "path_plots.mkdir(parents=True, exist_ok=True)\n",
    "resops_ts = {}\n",
    "for grand_id in tqdm(attributes.index, desc='Reading observed time series'): # ID refers to GRanD\n",
    "\n",
    "    # load timeseries\n",
    "    file = cfg.PATH_OBS_TS / f'ResOpsUS_{grand_id}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, index_col='date')\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "\n",
    "    # remove duplicated index\n",
    "    series = series[~series.index.duplicated(keep='first')]\n",
    "    # trim to GloFAS long run period\n",
    "    series = series.loc[cfg.START:cfg.END,:]\n",
    "    if series.empty:\n",
    "        print(f'Reservoir {grand_id} has no observations in the time period from {cfg.START} to {cfg.END}')\n",
    "        continue\n",
    "    # ensure there aren't gaps in the index\n",
    "    dates = pd.date_range(series.first_valid_index(), series.last_valid_index(), freq='D')\n",
    "    series = series.reindex(dates)\n",
    "    series.index.name = 'date'\n",
    "\n",
    "    # remove negative values\n",
    "    series[series < 0] = np.nan\n",
    "    # clean storage time series\n",
    "    series.storage = clean_storage(series.storage, w=7, error_thr=0.1)\n",
    "    # clean inflow time series\n",
    "    series.inflow = clean_inflow(\n",
    "        series.inflow, \n",
    "        storage=series.storage if attributes.loc[grand_id, 'STORAGE'] == 1 else None, \n",
    "        outlfow=series.outflow if attributes.loc[grand_id, 'OUTFLOW'] == 1 else None, \n",
    "        grad_thr=1e4, \n",
    "        balance_thr=5, \n",
    "        int_method='linear'\n",
    "    )\n",
    "\n",
    "    # convert time series to UTC (with offset)\n",
    "    series = convert_to_utc(\n",
    "        lon=attributes.loc[grand_id, 'LON'], \n",
    "        lat=attributes.loc[grand_id, 'LAT'], \n",
    "        series=series\n",
    "    )\n",
    "    # interpolate values to 00 UTC\n",
    "    series = reindex_to_00utc(series)\n",
    "    \n",
    "    # save in dictionary\n",
    "    series.index = pd.DatetimeIndex(series.index.date, name='date')\n",
    "    resops_ts[grand_id] = series\n",
    "\n",
    "    # plot observed time series\n",
    "    plot_resops(\n",
    "        series.storage,\n",
    "        series.elevation,\n",
    "        series.inflow,\n",
    "        series.outflow,\n",
    "        attributes.loc[grand_id, ['CAP_MCM', 'CAP_GLWD']].values,\n",
    "        title=grand_id,\n",
    "        save=path_plots / f'{grand_id:04}_lineplot.jpg'\n",
    "        )\n",
    "\n",
    "print(f'{len(resops_ts)} reservoirs in ResOpsUS time series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2643f396-79eb-4d7c-8b21-06bd332611a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676 reservoirs and 5 variables in the observed time series\n"
     ]
    }
   ],
   "source": [
    "# convert to xarray.Dataset\n",
    "xarray_list = []\n",
    "for key, df in resops_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords(GRAND_ID=key)\n",
    "    xarray_list.append(ds)\n",
    "obs = xr.concat(xarray_list, dim='GRAND_ID')\n",
    "\n",
    "print(f'{len(obs.GRAND_ID)} reservoirs and {len(obs)} variables in the observed time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "#### Simulated: GloFAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffc47c-489a-4b22-9d46-494a87177281",
   "metadata": {},
   "source": [
    "This snippet is a legacy. It imports the reservoir variables (inflow, storage and release) obtained from the long run simulation of GloFAS4. As GloFAS4 only did not consider all the reservoirs in the dataset, these time series are not useful any more.\n",
    "\n",
    "```Python\n",
    "# import time series\n",
    "glofas_ts = {}\n",
    "mask = ~attributes.GLOFAS_ID.isnull()\n",
    "for grand_id, glofas_id in tqdm(attributes[mask].GLOFAS_ID.items(), total=mask.sum(), desc='Reading simulated time series'):\n",
    "    file = cfg.PATH_SIM_TS / f'{glofas_id:03.0f}.csv'\n",
    "    if file.is_file():\n",
    "        series = pd.read_csv(file, parse_dates=True, dayfirst=False, index_col='date')\n",
    "        series.index -= pd.Timedelta(days=1)\n",
    "        series.storage *= attributes.loc[grand_id, 'CAP_GLWD']\n",
    "        series[series < 0] = np.nan\n",
    "        # series.columns = [f'{col.lower()}_sim' for col in series.columns]\n",
    "        glofas_ts[grand_id] = series\n",
    "    else:\n",
    "        print(f\"{file} doesn't exist\")\n",
    "        \n",
    "print(f'{len(glofas_ts)} reservoirs in GloFAS time series')\n",
    "\n",
    "# convert to xarray.Dataset\n",
    "new_dim = 'GRAND_ID'\n",
    "xarray_list = []\n",
    "for key, df in glofas_ts.items():\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    ds = ds.assign_coords({new_dim: key})\n",
    "    xarray_list.append(ds)\n",
    "sim = xr.concat(xarray_list, dim=new_dim)\n",
    "\n",
    "# rename variables in the simulated time series\n",
    "sim = sim.rename_vars({var: f'{var}_glofas' for var in list(sim)})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605ba181-7415-4969-9b06-06605081c2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677 reservoirs variables in the simulated inflow time series\n"
     ]
    }
   ],
   "source": [
    "# load time series\n",
    "var = 'dis24'\n",
    "path_inflow = cfg.PATH_RESOPS / 'ancillary' / 'ncextract' / var\n",
    "sim = xr.open_mfdataset(path_inflow.glob('*.nc'), combine='nested', concat_dim='id')\n",
    "\n",
    "# rename variables and coordinates\n",
    "sim = sim.rename({\n",
    "    'id': 'GRAND_ID', \n",
    "    'valid_time': 'date',\n",
    "    var: 'inflow_sim'\n",
    "})\n",
    "sim = sim.drop_vars(['surface', 'lat', 'latitude', 'lon', 'longitude'], errors='ignore')\n",
    "\n",
    "# correct and trim time\n",
    "sim['date'] = sim['date'] - pd.Timedelta(days=1)\n",
    "sim = sim.sel(date=slice(cfg.START, cfg.END))\n",
    "\n",
    "# compute\n",
    "sim = sim.compute()\n",
    "\n",
    "# # Create a CRS variable and set its attributes\n",
    "# crs_attrs = {\n",
    "#     'epsg_code': 'EPSG:4326',\n",
    "#     'semi_major_axis': 6378137.0,  # WGS 84\n",
    "#     'inverse_flattening': 298.257223563,  # WGS 84\n",
    "#     'grid_mapping_name': 'latitude_longitude'\n",
    "#     }\n",
    "# sim['crs'] = xr.DataArray(data=0, attrs=crs_attrs)  # CRS variable with its attributes\n",
    "\n",
    "# # define attributes\n",
    "# sim.attrs['Units'] = 'inflow: simulated discharge from GloFASv4 (m3/s)'\n",
    "# sim.time.attrs['timezone'] = 'UTC+00'\n",
    "# sim.GRAND_ID.attrs['Description'] = 'The identifier of the reservor in GRanD (Global Reservoir and Dam database)'\n",
    "# lat_attrs = {\n",
    "#     'Units': 'degrees_north',\n",
    "#     'standard_name': 'latitude',\n",
    "#     'grid_mapping': 'crs'\n",
    "# }\n",
    "# lon_attrs = {\n",
    "#     'Units': 'degrees_east',\n",
    "#     'standard_name': 'longitude',\n",
    "#     'grid_mapping': 'crs'\n",
    "# }\n",
    "# sim.latitude.attrs = lat_attrs\n",
    "# sim.longitude.attrs = lon_attrs\n",
    "\n",
    "print(f'{len(sim.GRAND_ID)} reservoirs variables in the simulated inflow time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746404c-c5d9-4e65-8c73-41f478ef4501",
   "metadata": {},
   "source": [
    "#### Meteorology: areal\n",
    "\n",
    "Time series of catchment-average meteorology generated with the LISFLOOD utility `catchstats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d0cca9-1bc5-4045-b0ea-47f0f1ee5fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 reservoirs and 3 variables in the areal meteorological time series\n"
     ]
    }
   ],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_areal = cfg.PATH_RESOPS / 'ancillary' / 'catchstats'\n",
    "rename_vars = {\n",
    "    'id': 'GRAND_ID',\n",
    "    'time': 'date',\n",
    "    'e0': 'evapo_areal',\n",
    "    'tp': 'precip_areal',\n",
    "    'ta': 'temp_areal',\n",
    "}\n",
    "variables = [x.stem for x in path_meteo_areal.iterdir() if x.is_dir() & (x.stem in rename_vars)]\n",
    "meteo_areal = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_areal}/{var}/*.nc')[f'{var}_mean'] for var in variables})\n",
    "\n",
    "# rename variables and coordinates\n",
    "meteo_areal = meteo_areal.rename(rename_vars)\n",
    "\n",
    "# correct and trim time\n",
    "meteo_areal['date'] = meteo_areal['date'] - pd.Timedelta(days=1) # WARNING!! One day lag compared with LISFLOOD\n",
    "meteo_areal = meteo_areal.sel(date=slice(cfg.START, cfg.END))\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_areal.GRAND_ID.data))\n",
    "meteo_areal = meteo_areal.sel(GRAND_ID=IDs)\n",
    "\n",
    "# compute\n",
    "meteo_areal = meteo_areal.compute()\n",
    "\n",
    "# # define attributes\n",
    "# meteo_units = 'evapo_areal: catchment-average potential evaporation from open water from ERA5 [mm/d]\\n' \\\n",
    "#     'precip_areal: catchment-average precipitation from ERA5 [mm/d]\\n' \\\n",
    "#     'temp_areal: catchment-average air temperature from ERA5 [¬∞C]\\n'\n",
    "# meteo_areal.attrs['Units'] = meteo_units\n",
    "# meteo_areal.time.attrs['timezone'] = 'UTC+00'\n",
    "# meteo_areal.GRAND_ID.attrs['Description'] = 'The identifier of the reservor in GRanD (Global Reservoir and Dam database)'\n",
    "\n",
    "print(f'{len(meteo_areal.GRAND_ID)} reservoirs and {len(meteo_areal)} variables in the areal meteorological time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf6a18-8346-458e-a73c-73744b0d4249",
   "metadata": {},
   "source": [
    "#### Meteorology: point\n",
    "\n",
    "Time series of reservoir point meteorology extracted with the LISFLOOD utilitiy `ncextract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c63708-9795-43d8-b30f-12c01bbf981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677 reservoirs and 3 variables in the areal meteorological time series\n"
     ]
    }
   ],
   "source": [
    "# load meteorological time series\n",
    "path_meteo_point = cfg.PATH_RESOPS / 'ancillary' / 'ncextract' / 'meteo'\n",
    "rename_vars = {\n",
    "    'id': 'GRAND_ID',\n",
    "    'time': 'date',\n",
    "    'e0': 'evapo_point',\n",
    "    'tp': 'precip_point',\n",
    "    'ta': 'temp_point',\n",
    "}\n",
    "variables = [x.stem for x in path_meteo_point.iterdir() if x.is_dir() & (x.stem in rename_vars)]\n",
    "meteo_point = xr.Dataset({f'{var}': xr.open_mfdataset(f'{path_meteo_point}/{var}/*.nc')[var] for var in variables})\n",
    "\n",
    "# rename variables and coordinates\n",
    "meteo_point = meteo_point.rename(rename_vars)\n",
    "meteo_point = meteo_point.drop_vars(['surface', 'lat', 'latitude', 'lon', 'longitude'], errors='ignore')\n",
    "\n",
    "# correct and trim time\n",
    "meteo_point['date'] = meteo_point['date'] - pd.Timedelta(days=1) # WARNING!! One day lag compared with LISFLOOD\n",
    "\n",
    "# keep catchments in the attributes\n",
    "IDs = list(attributes.index.intersection(meteo_point.GRAND_ID.data))\n",
    "meteo_point = meteo_point.sel(GRAND_ID=IDs)\n",
    "\n",
    "# meteo_point = meteo_point.drop_vars(['lon', 'lat'], errors='ignore')\n",
    "\n",
    "# compute\n",
    "meteo_point = meteo_point.compute()\n",
    "\n",
    "# # define attributes\n",
    "# meteo_units = 'evapo_point: potential evaporation at the reservoir location from open water from ERA5 [mm/d]\\n' \\\n",
    "#     'precip_point: precipitation at the reservoir location from ERA5 [mm/d]\\n' \\\n",
    "#     'temp_point: air temperature  at the reservoir location from ERA5 [¬∞C]\\n'\n",
    "# meteo_point.attrs['Units'] = meteo_units\n",
    "# meteo_point.time.attrs['timezone'] = 'UTC+00'\n",
    "# meteo_point.GRAND_ID.attrs['Description'] = 'The identifier of the reservor in GRanD (Global Reservoir and Dam database)'\n",
    "\n",
    "print(f'{len(meteo_point.GRAND_ID)} reservoirs and {len(meteo_point)} variables in the areal meteorological time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b39fa2-6569-41c5-bf00-8d3725e9eb2a",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "### Convert units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c817905b-edc7-4540-b237-8dc9021c7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.NORMALIZE:\n",
    "\n",
    "    # reservoir attributes used to normalize the dataset\n",
    "    area_sm = xr.DataArray.from_series(attributes.AREA_SKM) * 1e6 # m2\n",
    "    capacity_cm = xr.DataArray.from_series(attributes.CAP_MCM) * 1e6 # m3\n",
    "    catchment_sm = xr.DataArray.from_series(attributes.CATCH_SKM) * 1e6 # m2\n",
    "    \n",
    "    # Observed timeseries\n",
    "    # -------------------\n",
    "    for var, da in obs.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var in ['storage', 'evaporation']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var in ['inflow', 'outflow']:\n",
    "            obs[f'{var}_norm'] = obs[var] * 24 * 3600 / capacity_cm\n",
    "\n",
    "    # Simulated timeseries\n",
    "    # -------------------\n",
    "    for var, da in sim.items():\n",
    "        # convert variables in hm3 to fraction of reservoir capacity [-]\n",
    "        if var.split('_')[0] in ['storage']:\n",
    "            sim[f'{var}_norm'] = sim[var] * 1e6 / capacity_cm\n",
    "        # convert variables in m3/s to fraction of reservoir capacity [-]\n",
    "        elif var.split('_')[0] in ['inflow', 'outflow']:\n",
    "            sim[f'{var}_norm'] = sim[var] * 24 * 3600 / capacity_cm\n",
    "            \n",
    "    # Catchment meteorology\n",
    "    # ---------------------\n",
    "    # convert areal evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_areal[f'{var}_areal_norm'] = meteo_areal[f'{var}_areal'] * catchment_sm * 1e-3 / capacity_cm\n",
    "\n",
    "    # Point meteorology\n",
    "    # ---------------------\n",
    "    # convert point evaporation and precipitation from mm to fraction filled\n",
    "    for var in ['evapo', 'precip']:\n",
    "        meteo_point[f'{var}_point_norm'] = meteo_point[f'{var}_point'] * catchment_sm * 1e-3 / capacity_cm   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed75f2-62e4-442e-b897-52dee820581e",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef362ba-a408-490f-be6b-ed74d7cb4ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77af130d4dd4f448944081ef347e056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting time series:   0%|          | 0/677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir 288 does not have observations. Skipping to the next reservoir\n"
     ]
    }
   ],
   "source": [
    "path_csv = cfg.PATH_TS / 'csv'\n",
    "path_csv.mkdir(parents=True, exist_ok=True)\n",
    "path_nc = cfg.PATH_TS / 'netcdf'\n",
    "path_nc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for grand_id in tqdm(attributes.index, desc='Exporting time series'):    \n",
    "\n",
    "    # concatenate time series\n",
    "    if grand_id in obs.GRAND_ID.data:\n",
    "        ds = obs.sel(GRAND_ID=grand_id).drop_vars(['GRAND_ID'])\n",
    "    else:\n",
    "        print(f'Reservoir {grand_id} does not have observations. Skipping to the next reservoir')\n",
    "        continue\n",
    "    if grand_id in sim.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, sim.sel(GRAND_ID=grand_id).drop_vars(['GRAND_ID'])))\n",
    "    if grand_id in meteo_areal.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, meteo_areal.sel(GRAND_ID=grand_id).drop_vars(['GRAND_ID'])))\n",
    "    if grand_id in meteo_point.GRAND_ID.data:\n",
    "        ds = xr.merge((ds, meteo_point.sel(GRAND_ID=grand_id).drop_vars(['GRAND_ID'])))\n",
    "        \n",
    "    # delete empty variables\n",
    "    for var in list(ds.data_vars):\n",
    "        if (ds[var].isnull().all()):\n",
    "            del ds[var]\n",
    "\n",
    "    # trim time series to the observed period\n",
    "    start, end = attributes.loc[grand_id, ['TIME_SERIES_START', 'TIME_SERIES_END']].values\n",
    "    ds = ds.sel(date=slice(start, end))\n",
    "\n",
    "    # create time series of temporal attributes\n",
    "    ds['year'] = ds.date.dt.year\n",
    "    ds['month'] = ds.date.dt.month\n",
    "    ds['month_sin'], ds['month_cos'] = time_encoding(ds['month'], period=12)\n",
    "    ds['weekofyear'] = ds.date.dt.isocalendar().week\n",
    "    ds['woy_sin'], ds['woy_cos'] = time_encoding(ds['weekofyear'], period=52)\n",
    "    ds['dayofyear'] = ds.date.dt.dayofyear\n",
    "    ds['doy_sin'], ds['doy_cos'] = time_encoding(ds['dayofyear'], period=365)\n",
    "    ds['dayofweek'] = ds.date.dt.dayofweek\n",
    "    ds['dow_sin'], ds['dow_cos'] = time_encoding(ds['dayofweek'], period=6)\n",
    "        \n",
    "    # export CSV\n",
    "    # ..........\n",
    "    ds.to_pandas().to_csv(path_csv / f'{grand_id}.csv')\n",
    "\n",
    "    # export NetCDF\n",
    "    # .............\n",
    "    ds.to_netcdf(path_nc / f'{grand_id}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
