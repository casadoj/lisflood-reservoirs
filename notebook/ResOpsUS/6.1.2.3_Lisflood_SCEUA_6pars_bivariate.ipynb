{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4516f5e0-f0dd-4bdb-af93-3cf34cfb94c6",
   "metadata": {},
   "source": [
    "# Bivariate calibration with SCE-UA\n",
    "***\n",
    "\n",
    "**Autor:** Chus Casado<br>\n",
    "**Date:** 11-06-2024<br>\n",
    "\n",
    "**Introduction:**<br>\n",
    "In this code I calibrate the LISFLOOD routine in 94 reservoirs in the US for which we have both the GloFASv4 simulation and the records from ResOpsUS. \n",
    "\n",
    "The calibration looks for the optimal value of 6 parameters that control the normal state of the reservoir ($FF_n$, $\\alpha$, $QQ_n$), the flood state ($FF_f$, $QQ_f$), and the factor that limits the outflow depending on the inflow ($\\beta$). $FF$ stands for fracion filled, i.e., the proportion of the reservoir that is filled. $QQ$ stands for outflow quantile. The algorithm used to seach for the optimal parameters is SCE-UA (Shuffle Complex Evolution University of Arizona) [(Duan et al., 1994)](https://www.sciencedirect.com/science/article/abs/pii/0022169494900574), and the target variable is the observed outflow.\n",
    "\n",
    "**To do:**<br>\n",
    "* [ ] Clean observed time series\n",
    "\n",
    "**Questions:**<br>\n",
    "* [ ] Why only some of the iterations are saved in the CSV file?\n",
    "* [ ] What's the relation between likelihood and KGE?\n",
    "* [ ] Fit the $Q_{min}$ based on the inflow data. Hot to fit a GEV to minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a76b996-1c4e-4893-82a5-b4a36627ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import spotpy\n",
    "from spotpy.objectivefunctions import kge\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "# import cartopy.crs as ccrs\n",
    "# import cartopy.feature as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592a1524-2d5b-4c03-b9e5-56597d902050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lisfloodreservoirs import calibration\n",
    "\n",
    "# optimizer = calibration.get_optimizer(model='lisflood', n_targets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957c16b-8d22-4b48-9d28-5908595dba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lisfloodreservoirs.calibration.bivariate_lisflood import bivariate_6pars_1of\n",
    "from lisfloodreservoirs.reservoirs.lisflood import Lisflood\n",
    "from lisfloodreservoirs.utils.metrics import KGEmod, pareto_front\n",
    "from lisfloodreservoirs.utils.plots import plot_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16a79-1c1c-42d0-8b03-11e6ece2dfb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897dd88-61c4-4740-88a4-55b5c896d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml', 'r', encoding='utf8') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "### Paths\n",
    "\n",
    "PATH_DATASET = Path(cfg['paths']['dataset'])\n",
    "\n",
    "### Reservoir model\n",
    "\n",
    "MODEL = cfg['model'].lower()\n",
    "\n",
    "### Calibration\n",
    "\n",
    "# # sequential mode\n",
    "# parallel = \"seq\"  \n",
    "\n",
    "# calibration parameters\n",
    "ALGORITHM = cfg['calibration']['algorithm'].lower()\n",
    "TARGET = cfg['calibration']['target']\n",
    "MAX_ITER = cfg['calibration'].get('max_iter', 1000)\n",
    "COMPLEXES = cfg['calibration'].get('COMPLEXES', 4)\n",
    "TRAIN_SIZE = cfg['calibration'].get('TRAIN_SIZE', 0.7)\n",
    "\n",
    "# results will be saved in this path\n",
    "PATH_OUT = Path(cfg['calibration']['path_out'])\n",
    "PATH_OUT = PATH_OUT / MODEL / ALGORITHM\n",
    "if len(TARGET) == 1:\n",
    "    PATH_OUT /= 'univariate'\n",
    "elif len(TARGET) == 2:\n",
    "    PATH_OUT /= 'bivariate'\n",
    "else:\n",
    "    print('ERROR. Only univariate or bivariate calibrations are supported')\n",
    "    exit\n",
    "PATH_OUT.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351856-cee6-4d5e-b561-724f4ea6ebb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c98ca-b485-44de-aa1a-3ffd79dad967",
   "metadata": {},
   "source": [
    "### GloFAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0d0d9-7665-4ffb-8088-ae15c2191fc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705471d-435b-4b9f-a3a7-e6f684318037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load shapefile of GloFAS reservoirs\n",
    "reservoirs = gpd.read_file('../../GIS/reservoirs_analysis_US.shp')\n",
    "reservoirs.set_index('ResID', drop=True, inplace=True)\n",
    "\n",
    "print(f'{reservoirs.shape[0]} reservoirs in the shape file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae70c9c-e57e-440a-84fc-65863d1287cc",
   "metadata": {},
   "source": [
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d6e3e-7285-45b3-9a92-d01b657d5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read GloFAS time series\n",
    "path = Path('../../data/reservoirs/GloFAS/long_run')\n",
    "glofas_ts = {}\n",
    "for file in tqdm(glob.glob(f'{path}/*.csv')):\n",
    "    id = int(file.split('\\\\')[-1].split('.')[0].lstrip('0'))\n",
    "    if id not in reservoirs.index:\n",
    "        continue\n",
    "    glofas_ts[id] = pd.read_csv(file, parse_dates=True, dayfirst=False, index_col='date')\n",
    "    \n",
    "print(f'{len(glofas_ts)} reservoirs in the GloFAS time series')\n",
    "\n",
    "# convert storage time series into volume\n",
    "for id, df in glofas_ts.items():\n",
    "    df.storage *= reservoirs.loc[id, 'CAP'] * 1e6\n",
    "\n",
    "# period of GloFAS simulation\n",
    "start, end = glofas_ts[id].first_valid_index(), glofas_ts[id].last_valid_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9fe99-43ab-4974-bd42-175abd6a8508",
   "metadata": {},
   "source": [
    "### ResOpsUS\n",
    "#### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bae636-3b4e-4024-8295-eba053461b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "resops_ts = {}\n",
    "for glofas_id in tqdm(reservoirs.index):\n",
    "    # load timeseries\n",
    "    grand_id = reservoirs.loc[glofas_id, 'GRAND_ID']\n",
    "    series_id = pd.read_csv(PATH_DATASET / 'time_series_all' / f'ResOpsUS_{grand_id}.csv', parse_dates=True, index_col='date')\n",
    "    # remove empty time series\n",
    "    series_id = series_id.loc[start:end]#.dropna(axis=1, how='all')\n",
    "    # remove duplicated index\n",
    "    series_id = series_id[~series_id.index.duplicated(keep='first')]\n",
    "    # save in dictionary\n",
    "    resops_ts[glofas_id] = series_id\n",
    "\n",
    "print(f'{len(resops_ts)} reservoirs in the ResOpsUS time series')\n",
    "    \n",
    "# convert storage from hm3 to m3\n",
    "for id, df in resops_ts.items():\n",
    "    df.storage *= 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0859e7-364c-4b79-ad44-7ed2fe6b8adb",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055a152-1a97-4175-b92a-734ba6256ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ResID in tqdm([275]):#reservoirs.index):\n",
    "    \n",
    "    # file where the calibration results will be saved\n",
    "    dbname = f'{PATH_OUT}/{ResID:03}_samples'\n",
    "    if os.path.isfile(dbname + '.csv'):\n",
    "        print(f'The file {dbname}.csv already exists.')\n",
    "        continue\n",
    "        \n",
    "    ## TIME SERIES\n",
    "    try:\n",
    "        # observed time series\n",
    "        obs = resops_ts[ResID][['storage', 'inflow', 'outflow']].copy()\n",
    "        obs[obs < 0] = np.nan\n",
    "\n",
    "        # define calibration period\n",
    "        if obs.outflow.isnull().all():\n",
    "            print(f'Reservoir {ResID} is missing outflow records')\n",
    "            continue\n",
    "        elif obs.storage.isnull().all():\n",
    "            print(f'Reservoir {ResID} is missing storage records')\n",
    "            continue\n",
    "        else:\n",
    "            start_obs = max([obs[var].first_valid_index() for var in ['storage', 'outflow']])\n",
    "            end_obs = min([obs[var].last_valid_index() for var in ['storage', 'outflow']])\n",
    "            cal_days = timedelta(days=np.floor((end_obs - start_obs).days * TRAIN_SIZE))\n",
    "            start_cal = end_obs - cal_days\n",
    "\n",
    "        # define train and test time series\n",
    "        x_train = glofas_ts[ResID].inflow[start_cal:end_obs]\n",
    "        y_train = obs.loc[start_cal:end_obs, ['storage', 'outflow']]\n",
    "        x_test = glofas_ts[ResID].inflow[start:start_cal]\n",
    "        y_test = obs.loc[start_obs:start_cal, ['storage', 'outflow']]\n",
    "        \n",
    "    except:\n",
    "        print(f'ERROR. The time series of reservoir {ResID} could not be set up')\n",
    "        continue\n",
    "        \n",
    "    ## SET UP SPOTPY\n",
    "    try:\n",
    "        # extract GloFAS reservoir parameters\n",
    "        Vc, Vtot, Qmin = reservoirs.loc[ResID, ['clim', 'CAP', 'minq']]\n",
    "        Vtot *= 1e6\n",
    "        Vc *= Vtot\n",
    "\n",
    "        # initialize the calibration setup of the LISFLOOD reservoir routine\n",
    "        setup = bivariate_6pars_1of(x_train, y_train.storage, y_train.outflow,\n",
    "                                    Vc, Vtot, Qmin,\n",
    "                                    KGEmod)\n",
    "\n",
    "        # define the sampling method\n",
    "        if ALGORITHM == 'sceua':\n",
    "            sampler = spotpy.algorithms.sceua(setup, dbname=dbname, dbformat='csv', save_sim=False)\n",
    "        elif ALGORITHM == 'mc':\n",
    "            sampler = spotpy.algorithms.mc(setup, parallel=parallel, dbname=dbname, dbformat='csv', save_sim=False)\n",
    "        else:\n",
    "            print(F'ERROR. Algorithm {ALGORITHM} is not supported. Select either \"SCEUA\" or \"MC\"')\n",
    "            break\n",
    "    except:\n",
    "        print(f'ERROR. The SpotPY set up of reservoir {ResID} could not be done')\n",
    "        \n",
    "    ## LAUNCH SAMPLING\n",
    "    try:\n",
    "        # start the sampler\n",
    "        if ALGORITHM == 'sceua':\n",
    "            sampler.sample(MAX_ITER, ngs=COMPLEXES, kstop=3, pcento=0.01, peps=0.1)\n",
    "        elif ALGORITHM == 'mc':\n",
    "            sampler.sample(MAX_ITER)\n",
    "    except:\n",
    "        print(f'ERROR. While sampling the reservoir {ResID}')\n",
    "        continue\n",
    "\n",
    "    ### ANALYSE RESULTS\n",
    "    try:\n",
    "        # read CSV of results\n",
    "        results = pd.read_csv(f'{dbname}.csv')\n",
    "        results.index.name = 'iteration'\n",
    "        parcols = [col for col in results.columns if col.startswith('par')]\n",
    "    except:\n",
    "        print(f'ERROR while reading results form reservoir {ResID}')\n",
    "        continue\n",
    "\n",
    "        # # compute calibration KGE of each simulation\n",
    "        # simulation = results.filter(regex='^simulation_').transpose()\n",
    "        # simulation.index = pd.date_range(start_cal, end_obs, freq='D')\n",
    "        # results['KGEcal'] = [KGEmod(y_train.outflow, simulation[i])[0] for i in simulation.columns]\n",
    "\n",
    "    try:\n",
    "        # compute validation KGE of each simulation and overwrite CSV file\n",
    "        Vo = y_test.storage[0]\n",
    "        #results['like_val'] = [1 - KGEmod(y_test.outflow, setup.simulation(results.loc[i, parcols], x_test, Vo))[0] for i in tqdm(results.index)]\n",
    "        results['like_val'] = np.nan\n",
    "        for i in tqdm(results.index):\n",
    "            Q_sim, V_sim = setup.simulation(results.loc[i, parcols], x_test, Vo)\n",
    "            results.loc[i, 'like_val'] = np.sqrt(np.sum([(1 - KGEmod(y_test.outflow, Q_sim)[0])**2, (1 - KGEmod(y_test.storage, V_sim)[0])**2]))\n",
    "        results.to_csv(f'{dbname}.csv', index=False, float_format='%.8f')\n",
    "    except:\n",
    "        print(f'ERROR while computing KGE for the validation period in reservoir {ResID}')\n",
    "\n",
    "    try:\n",
    "        # select optimal parameters\n",
    "        best_iter = results.like1.idxmin()\n",
    "        parvalues = {col[3:]: float(results.loc[best_iter, col]) for col in parcols}\n",
    "\n",
    "        # export optimal parameters\n",
    "        with open(f'{PATH_OUT}/{ResID:03}_optimal_parameters.yml', 'w') as file:\n",
    "            yaml.dump(parvalues, file)\n",
    "            \n",
    "        # plot pairplot of the likelihood\n",
    "        sns.pairplot(results, vars=parcols, corner=True, hue='like1', palette='Spectral_r', plot_kws={'s': 12})\n",
    "        plt.savefig(PATH_OUT / f'{ResID:03}_pairplot.jpg', dpi=300, bbox_inches='tight')\n",
    "    except:\n",
    "        print(f'ERROR while searching for optimal parameters in reservoir {ResID}')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "\n",
    "        # # declare the reservoir with the optimal parameters\n",
    "        # Vn, Vf = [parvalues[var] * Vtot for var in ['FFn', 'FFf']]\n",
    "        # Vn_adj = Vn + parvalues['alpha'] * (Vf - Vn)\n",
    "        # Qn, Qf = [setup.inflow.quantile(parvalues[var]) for var in ['QQn', 'QQf']]\n",
    "        # k = parvalues['k']\n",
    "        # res = Lisflood(Vc, Vn, Vn_adj, Vf, Vtot, Qmin, Qn, Qf)\n",
    "        \n",
    "        # declare the reservoir with the optimal parameters\n",
    "        Vf = parvalues['FFf'] * Vtot\n",
    "        Vn = Vc + parvalues['alpha'] * (Vf - Vc)\n",
    "        Vn_adj = Vn + parvalues['beta'] * (Vf - Vn)\n",
    "        Qf = setup.inflow.quantile(parvalues['QQf'])\n",
    "        Qn = parvalues['gamma'] * Qf\n",
    "        k = parvalues['k']\n",
    "        res = Lisflood(Vc, Vn, Vn_adj, Vf, Vtot, Qmin, Qn, Qf)\n",
    "\n",
    "        # simulate the whole period and analyse\n",
    "        sim = res.simulate(glofas_ts[ResID].inflow[start_obs:end_obs], obs.storage[start_obs], k=k)\n",
    "        res.scatter(sim, obs, norm=False, title=ResID, save=PATH_OUT / f'{ResID:03}_scatter.jpg')\n",
    "        res.lineplot({'GloFAS': glofas_ts[ResID], 'cal': sim}, obs, save=PATH_OUT / f'{ResID:03}_lineplot.jpg')\n",
    "\n",
    "    except:\n",
    "        print(f'ERROR while simulating with optimal parameters in reservoir {ResID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd670a-22b1-4fea-9bee-3cb84643a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup.simulation(results.loc[0, parcols], x_test, Vo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f25620-952c-4498-8c25-48f769f31cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd1735-c7e8-49ef-9904-d5b255415ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(results.index):\n",
    "    Q_sim, V_sim = setup.simulation(results.loc[i, parcols], x_test, Vo)\n",
    "    np.sqrt(np.sum([(1 - KGEmod(y_test.outflow, Q_sim)[0])**2, (1 - KGEmod(y_test.storage, V_sim)[0])**2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291c47d-ae17-4da4-8f89-f237be8f3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859e3ce-873a-4d0f-9cdf-970d5108718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    [1 - KGEmod(y_test.outflow, setup.simulation(results.loc[i, parcols], x_test, Vo))[0] for i in tqdm(results.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3818d2f-4ca4-42fb-9f03-f30814332985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665c2f2-652e-402e-98a0-d92862846c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = results.sort_values('like1', ascending=False)\n",
    "x = np.arange(aux.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(x, 1 - aux.like1, label='cal')\n",
    "ax.plot(x, 1 - aux.like_val, label='val')\n",
    "ax.axvline(aux.index.get_loc(best_iter), c='k', lw=.5, ls=':')\n",
    "ax.set(xlabel='iterations',\n",
    "       xlim=(0, results.shape[0]),\n",
    "       ylabel='KGE',\n",
    "       ylim=(None, 1.02))\n",
    "ax.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e074758-baaf-4a3f-aba9-7b8c8410ded1",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3f575-703f-4714-8a22-425609922fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pars = {}\n",
    "for ResID in tqdm(reservoirs.index):\n",
    "    try:\n",
    "        file = PATH_OUT / f'{ResID:03}_optimal_parameters.yml'\n",
    "        with open(file, 'r') as f:\n",
    "            pars[ResID] = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    except:\n",
    "        print(f'The file {file} was not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e53a6-aebd-45b2-b6fe-126085d37a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e8563-b378-4742-b6f3-9b61e210513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in PATH_OUT.glob('*_optimal_parameters.yml'):\n",
    "    print(file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e6ada-c7a2-481f-bfc3-17664376137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(file, 'r') as f:\n",
    "        data = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43abc8b-eb32-491e-ac97-43c3f13854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "    data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4e2f8-2b3c-48c0-97da-d527bba10e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "    str(file).split('\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6def9c-61ed-4d9f-b2a9-65bdefa89414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
